[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Capital Modeling and Portfolio Management",
    "section": "",
    "text": "Preface\nWriting a monograph on capital modeling is a humbling experience. Capital Modeling, also known as Dynamic Financial Analysis (DFA), Enterprise Risk Analysis, or Internal Risk Modeling is at least 120 years old. It began in 1903, when Swedish actuary Filip Lundberg introduced the stochastic compound-Poisson risk model to describe how an insurer’s surplus evolves over time. The literature is vast, thousands of pages across hundreds of books and articles. How can one condense that all down to a comprehensive how-to manual? The answer is, one cannot, and we do not try. Instead, we attempt to sketch out what needs to happen, and provide sufficient references to the existing literature so that the reader can obtain whatever detail is needed.\nWhere we have introduced some innovation is at the very end of the process—capital allocation, or, more properly, the allocation of cost of capital. Here, we explore in detail the (unfortunate) consequences of assuming a fixed cost of capital rate and offer a TVaR-based generalization in its place: Spectral Risk Measures, also known as Distortion Risk Measures, and their Natural Allocation. Yet we cannot claim originality here, either, as we are leaning heavily on another decades-long vein of actuarial research.\nWith allocation, we deliberately adopt a single-period perspective. We do not address the time required for claims to develop to ultimate, nor do we treat the insurer as a going concern. The first omission aligns with most regulatory capital models, which focus on one-year changes in reserves and new accident year loss bookings. They account for reserve risk, but not the full run-off risk. The second reflects our desire to avoid getting bogged down in what David Babbel (discussion in Reitano (1997)) calls the “quagmire of equity valuation”—a topic one of us has written about already and the other chooses to avoid.\nIt is perhaps a fitting irony that the quantification of risk is riddled with uncertainty. Capital modeling is part science, part dark art—and its real value often isn’t the final number, but the insight gained along the way. While some may chase the best estimate, models are most useful when they show a range of outcomes and the assumptions behind them. Some inputs are squishy, like risk appetite, and it’s often easier to rule out wrong answers than pin down the right one. Think of capital modeling less as a calculator and more as a compass: it won’t give you the answer, but it helps you head in the right direction.\nWe draw heavily from our book Mildenhall and Major (2022). You do not need the book to implement the mechanics presented in this monograph; we wrote it to stand alone. However, it is a good source for the whys and wherefores, and advanced topics.\nFinally, capital modeling is a participation sport not a spectator sport. We recommend strongly that you open your favorite spreadsheet or software tool and reproduce the simple examples. Generally, as the models become more realistic they scale up in terms of number of rows and columns but not in terms of conceptual complexity. Both authors have used the basic methods outlined on 10,000+ simulated events with 100+ units and obtained satisfactory results—though this should not be attempted in a spreadsheet unless you are exceedingly patient. We apologize that straightforward spreadsheet concepts, such as the sum product of two columns or a weighted average, become intimidating mathematical formulas, but since we want to be explicit, we need to present the formulas and there’s no sidestepping this. The reader will find it helpful to see through the formulas to their simple spreadsheet form, best done hands-on. Lastly, this paragraph should not be taken as an endorsement of spreadsheets. You should learn to program!\n\n\nAbout the Authors\nStephen J. Mildenhall has extensive general insurance experience, having worked in primary and reinsurance pricing, broking, and education since 1992. He is a Fellow of the Casualty Actuarial Society, an Associate of the Society of Actuaries, and holds a PhD degree in mathematics from the University of Chicago.\nJohn A. Major has served as a research leader and data scientist in diverse insurance contexts, contributing to the state of the art in areas such as claim fraud detection, insurance-linked securities, terrorism risk, and catastrophe modeling. Since 2004, much of his attention has focused on the shareholder value of risk transformation. His publications in over a dozen books and journals have been cited in hundreds of scholarly articles. He is an Associate of the Society of Actuaries and holds a Master’s degree in mathematics from Harvard University.\n\n\n\n\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.\n\n\nReitano, Robert R. 1997. “Two paradigms for the market value of liabilities.” North American Actuarial Journal 1 (4): 104–22. https://doi.org/10.1080/10920277.1997.10595657.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "dedication.html",
    "href": "dedication.html",
    "title": "Dedication",
    "section": "",
    "text": "Dedicated to Donald Mango (1963-2022)\nDon was a friend, colleague, and mentor to us both. His vision, guidance, and deep understanding shaped not only this monograph but our development as actuaries. A passionate educator and gifted communicator, he elevated the profession and all who knew him. We are profoundly grateful.",
    "crumbs": [
      "Dedication"
    ]
  },
  {
    "objectID": "010_Introduction.html",
    "href": "010_Introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What are you getting yourself into?\nCongratulations! You have been tasked with developing a capital modeling system! But what does this mean? Maybe you were lucky enough to be handed a detailed specification of what this capital model is supposed to do and who is going to make use of its outputs and how those outputs will affect business operations. Or perhaps not.\nBut what is a capital modeling system? What is it for? And how do you build one?\nActuarial standards define capital as “The funds…over and above those funds backing the liabilities,” where “funds backing the liabilities” are amounts booked to support incurred liabilities (loss reserves) or future liabilities (unearned premiums), Actuarial Standards Board (2011). Capital is then a cushion in excess of those amounts. This definition highlights that capital is an accounting concept whose value will vary with the accounting standard. Since the distinction between capital and reserves is blurry, it is often more productive to focus on all available assets. Pricing then splits assets between policyholder-provided premium and reserves and owner-provided capital.\nThe modeling standard (Actuarial Standards Board 2019) defines a model as “A simplified representation of relationships among real-world variables, entities, or events using statistical, financial, economic, mathematical, non-quantitative, or scientific concepts and equations.” A key word here is simplified; as Gabaix and Laibson (2008) points out, “[a] perfect replica of the earth would reproduce every contour, but such a representation would be impractical.” Capital modelers should take to heart the admonition attributed to Einstein to make things “as simple as possible, but no simpler.” Any increment of complexity should materially improve the utility of your model. Modelers face many pressures to build granular, complex models but receive no countervailing rewards for accuracy and practicality.\nToday, computers are ubiquitous in modeling, and this monograph will focus exclusively on quantitative models embedded in computer systems.\nA capital modeling system may be expected to support:\nWe will assume in this monograph that your goal is, ultimately, a comprehensive system that can support all of the above activities. Of course, as you build it, you will need to prioritize interim goals and use cases, but directionally this is where we assume you are heading. For modelers in the US, questions of pricing are more important than questions of capital adequacy, as the latter constraints tend to be externally imposed. In the EU, however, an internal model may be called on to determine capital adequacy. There, regulators issue extensive guidance on model building that obviously trump any conflicting advice we offer.\nThe system’s core modeling function is to mirror and tie into the plan. It is first a system to support and enhance the existing planning process and second to quantify risk around the plan. Support for the other listed tasks stems from this capability to represent risk.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "010_Introduction.html#sec-What-Into",
    "href": "010_Introduction.html#sec-What-Into",
    "title": "1  Introduction",
    "section": "",
    "text": "risk identification and analysis,\nproduct pricing (ratemaking),\nrating and underwriting,\nreinsurance decisions,\nbusiness unit performance assessment,\nportfolio optimization,\nvaluation of a potential spinoff or acquisition,\nregulatory compliance,\nrating agency discussions, and\nstrategic planning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "010_Introduction.html#sec-Structure-Monograph",
    "href": "010_Introduction.html#sec-Structure-Monograph",
    "title": "1  Introduction",
    "section": "1.2 Structure of the monograph",
    "text": "1.2 Structure of the monograph\nThe remainder of the Introduction helps you with Getting started, addressing high-level goals and operational issues to get you off the ground in Section 1.3, making some Clarifying remarks in Section 1.4, providing a System overview in Section 1.5 that outlines a generic capital modeling system and which forms the basis for subsequent discussions of functionality, and in Section 1.6, Introduction to InsCo, presenting a highly simplified example which is used throughout for illustration.\nThe rest of the monograph is structured as follows:\n\nChapter 1 Introduction is this chapter.\nChapter 2 Business operations discusses the largest module in the capital modeling system. This is where the financial fortunes of the firm are simulated. Building or acquiring and implementing this module will absorb most of your resources. This function is also where decades of training, experience, and literature can be brought to bear. In this chapter we will provide further pointers into that literature.\nChapter 3 Capital adequacy and sources of risk discusses the second module, which is smaller and easier to implement. It depends on the outputs from Business Operations and reflects well-understood principles of risk measurement. Basic risk measurement as well as regulatory aspects are discussed in this chapter. We also discuss risk analysis applications. The simple example continues. At the end of the chapter, algorithms for computing expected value, quantiles (VaR), and TVaR on discrete distributions are presented.\nChapter 4 Pricing and allocation is where we introduce new ways of thinking, specifically Spectral Risk Measures (SRMs) and the Natural Allocation thereof. In this chapter, InsCo is treated with a complete pricing and allocation exercise. Along the way, we discuss the industry standard approach to capital allocation and why it is problematic. At the end of the chapter, algorithms for computing SRMs and the Natural Allocation on discrete distributions are presented.\nChapter 5 Applications of pricing and allocation use the outputs of the Capital Adequacy and Pricing and Allocation modules in performance assessment, new business pricing, reinsurance decision-making, portfolio optimization, and mergers and acquisitions.\nChapter 6 Selecting and calibrating an SRM  discusses how choice of an SRM impacts the various portfolio components and presents an approach to finding an acceptable solution. It also discusses how to leverage the range of consistent outcomes and understand their implications on risk appetite.\nChapter 7 Evaluating models discusses how to evaluate your own model, third-party models, and modeling platforms.\nChapter 8 Advanced topics deals with uncertainty and allocating capital (if you must) and a clever technique to implement correlation.\nChapter 9  Calculations with Aggregate shows how to use the open-source aggregate package to work the examples in this monograph.\nChapter 10 The Appendices include a table of symbols, list of relevant Actuarial Sandards of Practice, and a glossary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "010_Introduction.html#sec-Getting-Started",
    "href": "010_Introduction.html#sec-Getting-Started",
    "title": "1  Introduction",
    "section": "1.3 Getting started",
    "text": "1.3 Getting started\n\n1.3.1 The charter\nThe maxim, “If you build it, they will use it” does not apply to capital models. Successful models are built in response to business needs, with high-level direction set by a senior business leader. A charter can be a valuable tool in crystallizing that direction. You should have some more-or-less formal document answering design questions such as:\n\nWhy are we building a capital modeling system?\n\nIs this driven by regulatory concerns, dissatisfaction with the current planning process, or something else?\n\nWhat types of decisions will it support?\n\nSee examples in Section 1.1.\n\nWho is going to use this model?\n\nWho is going to run it, hands on?\nHow often will it run (daily, weekly, monthly, etc.)?\nWho is going to be in a position to set inputs or pose queries?\nWho is going to receive reports?\nWhat is the users’ level of sophistication, in terms of what they are likely to ask and what sort of output or report they are willing and able to absorb?\nIs the model intended for senior staff to be reviewing outputs or junior staff to be integrating outputs with their own workflow? This may differ across functional units.\n\nWhen will the outputs be used?\n\nDuring the planning process, presumably, but probably other times as well.\n\nWhose business processes will be affected by the system?\nHow will outputs be integrated into planning and underwriting?\nHow will we know if we are successful?\n\nDefine metrics such as use and impact.\nWhat stakeholders need to be in agreement to make the system successful?\n\n\nIdeally, this document will bear a cover memo endorsed by some sufficiently high-ranking executive. At the very least, it must be widely exposed to senior management and key stakeholders (typically Lines of Business) via road shows.\nHow will you know if you are successful? Mainly, by seeing your model get used, i.e., “fully embedded into the risk strategy and operational processes of the insurer” (IAIS 2008). This means, among other things:\n\nManagement has an overall understanding of what your model does, in terms of input versus output.\nThe model is used in the planning cycle; results are regularly discussed and reviewed.\nDecisions that affect the firm’s risk profile usually refer to model results.\nDecision-makers ask when the next set of outputs will be ready.\n\nThe European Insurance and Occupational Pension Authority (EIOPA (2009), title I, chapter VI, section 4, article 120; EIOPA (2015) title I, chapter 6) elaborates on these points in the context of the use test.\n\n\n1.3.2 Organizational factors\nYour project plan needs to address organizational issues. What resources will be available for initial development, ongoing maintenance, and operations? What actuarial and programming staff will need to be involved? What data are required? What funds are needed to purchase software? What hardware is needed?\nDon Mango addresses organizational and human factors related to implementing an internal risk model (IRM) in Brehm et al. (2007), sec 3.1. He covers the following areas:\n\nStartup staffing: This step includes considering the organizational chart, business functions represented, resource commitments, roles, and responsibilities. He recommends that system implementation is “the establishment of a new competency” and calls for the transfer of full-time employees or even adding new hires. Control of inputs and outputs of the model needs to be taken as seriously as general ledger or reserving systems.\nPurpose and scope: He suggests initially to cover only the prospective underwriting year and to quantify variation around that plan; later, expand the scope to include reserves, assets, and operational risks with more formal probability distributions. \nParameter development: Data quality issues will require expert judgment from underwriting, claims, planning, and actuarial staff. “Develop a systematic way to capture expert risk opinion.” Correlation is as important as it is problematic. \nSoftware: Evaluate available software solutions, and decide whether to build or buy. This decision depends on the strength and skills of the modeling team.\nValidation testing: This will occur over time and requires significant planning. Reach out to affected parties and make sure they understand what validation does and how it works.\nRollout: Early in the process, secure top management backing for prioritization, communications and training, and pilot testing.\nIntegration and maintenance: Of course, he recommends mandatory inclusion of model reports as part of decision support in the planning cycle. Make major updates no more frequently than semiannually. Maintain centralized control of inputs and outputs.\n\nEmma et al. (2000) Chapter 6 discusses dangers and pitfalls. We can recast that advice as things to do:\n\nMake sure to get sufficient senior management support.\n\nGet a charter!\nMake sure the effort is going to support a real business use.\n\nAttend to communication between the team and key stakeholders.\n\nThe model design may be quite technical. Make sure stakeholders understand enough about the model to be informed on what it can and cannot do, as well as what is required to develop and maintain it. \nUse the same language as stakeholders. This is especially important when talking to professionals like accountants.\nAllocate enough time to communication.\n\nMake sure model results can be integrated into planning.\n\nResults must reconcile to plan.\nResults must emerge in time to be used in the planning process.\n\n\nPlanning the development of a capital modeling system is very much like planning any other significant software system. Treat it with the same level of care.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "010_Introduction.html#sec-clarifying-remarks",
    "href": "010_Introduction.html#sec-clarifying-remarks",
    "title": "1  Introduction",
    "section": "1.4 Clarifying remarks",
    "text": "1.4 Clarifying remarks\n\n1.4.1 Pricing, ratemaking, and costing\nThroughout the monograph we talk about pricing, using phrases like the “required price” to mean a model indicated price. For actuaries working outside highly regulated lines, we believe our terminology is standard and immediately comprehensible. However, actuaries working in regulated lines may be sensitive to the distinction between costing, ratemaking, and pricing. Cost estimates consider only loss-related differences between insureds. Ratemaking allows behavioral differences to creep into premiums, considering price elasticity and propensity to shop in determining a profit maximizing rate, i.e., allowing price optimization. Finally, price is usually reserved for a market determined sold rate. Acceptable prices may include corporate strategic objectives aside from considerations of the insured’s risk. We do not explicitly consider price optimization or other strategic pricing objectives and are therefore really engaged in costing, but we find the word “pricing” preferable and trust that no confusion will arise.\n\n\n1.4.2 Margins and returns\nMuch actuarial ink has been spilled on the distinction between margins and returns (McClenahan 1999). The situation is straightforward. Insureds care about premiums and hence margins to premium. Investors care about capital and hence the return on capital. Insurers and regulators care about both. Capital is a cost for an insurer, hence they speak of the cost of capital. Investors purchase the securities (equity, bonds) insurers use to finance capital, and they are interested in the return on their investment. Cost of capital is almost always ex ante and expected; return on investment can be expected or ex post and actual. The rate regulator focuses on premium margins and the embedded cost of capital, whereas the solvency regulator monitors actual returns and the risks taken to achieve them.\nPremium leverage forms the bridge between premium and capital. The comparison in Table 1.1 helps illustrate this bridge. It contrasts the typical economics of an attritional, low-risk unit with those of a high-risk, catastrophe exposed one. The value column is particularly important: it clarifies a common point of confusion. Attritional insurance is typically inexpensive in the sense it is written at a high loss ratio. It is also written at high leverage, often producing a high equity-like return on capital. Conversely, catastrophe risk is expensive and written at a low loss ratio. It is difficult to pool efficiently and as a consequence is written at low leverage, resulting in low debt-like returns, as seen in the catastrophe bond market. This can seem paradoxical: business that is more risky from an insurance perspective yields a lower return on capital, and vice versa. The paradox is resolved by recognizing the difference between risk to the insurer and risk to the investor. Capital supporting attritional risk faces a roughly 50% chance of short-term loss, whereas capital supporting pure catastrophe risk has very a low probability of loss. The loss-given-default characteristics also differ substantially. Last, it might seem we should use margin in this table to measure expense rather than loss ratio. We don’t because margin is negatively correlated with loss ratio, so using margin would break the clean high/high/high vs. low/low/low symmetry of the table. \n\n\n\n\nTable 1.1: Comparison of loss ratio, leverage and return for attritional and catastrophe type units.\n\n\n\n\n\n\n\n\n\nUnit Type\nMeasure\nValue\nQualitative\nQuantitative\n\n\n\n\nAttritional\nLoss ratio (%)\nHigh\nCheap insurance\n90s\n\n\n\nLeverage\nHigh\nEfficient pooling\n2:1\n\n\n\nReturn (%)\nHigh\nHigh returns\nTeens to 20s\n\n\n\n\n\n\n\n\n\nCatastrophe\nLoss ratio (%)\nLow\nExpensive insurance\n30s\n\n\n\nLeverage\nLow\nInefficient pooling\n1:5\n\n\n\nReturn (%)\nLow\nLow returns\nSingle digit\n\n\n\n\n\n\n\n\n\n\n\n1.4.3 Expected, plan, actual, and other flavors\nWhen we talk about pricing, terms like “loss” and “margin” usually mean expected loss or margin. It is helpful to distinguish several different pricing viewpoints (Robbin 1992). A premium or rate can be:\n\nFiled: incorporated into filed rates in a regulated line.\nRequired or Target: premium required to hit a (corporate) pricing target.\nPlan: reflects adjusted targets actually embedded in a plan.\nIndicated: produced by an actuarial pricing model or process.\nPriced: produced by an actuarial-underwriting process.\nQuoted: actually released into the market as a bindable rate.\nSold: the premium actually sold in a transaction.\nMarket: an estimate of sold rates.\n\nThe same terms can be applied to margins. These terms are almost always meant prospectively and are ex ante. In addition, for margins there is the important\n\nActual: achieved margin reflecting developed ultimate loss experience. \n\nActual is obviously after the fact or ex post.\nOur pricing discussions primarily focus on setting some flavor of required premium in the context of a corporate or business unit plan. We often use the term “required” as a synonym for “target” or “indicated,” i.e., a price computed by a model in order to meet some specified objective.\n\n\n1.4.4 Parts of a model\nBefore describing what should be in a model, some distinctions should be kept in mind. There are various levels to modeling, starting from the computer infrastructure and extending all the way up to the application being run by the end-user. Table 1.2 lays out these levels and associated concepts.\n\n\n\nTable 1.2: Model components from hardware to output.\n\n\n\n\n\n\n\n\n\nConcept\nExamples\n\n\n\n\nLevels include\n\n\n\nComputer hardware\nMac, PC, server, workstation, cloud (AWS, Databricks)\n\n\n\n\n\n\nOperating system and\nmacOS, Windows, Linux\n\n\nsystem utilities\n\n\n\n\n\n\n\nProgramming language\nPython, R, Java, Visual BASIC, C++\n\n\n\n\n\n\nModeling platform\nExcel, ReMetrica, IGLOO, MetaRisk, Oasis,\n\n\n\nyour own custom “empty” spreadsheet or function library\n\n\n\n\n\n\nModel\nSpreadsheet\n\n\n\nRating agency capital models, Solvency II standard calculation\n\n\n\nSpecific peril cat model, e.g., vendor US Hurricane model\n\n\n\n\n\n\nModels include\n\n\n\nModeling techniques\nSimulation, calculation\n\n\n\n\n\n\nStructural parameters\nProtected cells, e.g., unit loss volatility and correlations\n\n\n\nCat model damage curve\n\n\n\n\n\n\nBusiness parameters\nUser input cells, e.g, unit plan premium and loss ratios\n\n\n\nCat model property portfolio\n\n\n\n\n\n\nRaw outputs\nCat model event or yearly loss tables, event timeline database\n\n\n\n\n\n\nOutput exhibits\nProbable maximal loss table, summary statistics, pro forma financials\n\n\n\n\n\n\nStructural parameters are changed infrequently. Business parameters are changed frequently, especially during planning when it is standard to run many different iterations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "010_Introduction.html#sec-System-Overview",
    "href": "010_Introduction.html#sec-System-Overview",
    "title": "1  Introduction",
    "section": "1.5 System overview",
    "text": "1.5 System overview\nThere is considerable variation in how a capital modeling system could be designed and implemented. In this section we provide a conceptual template to follow.\n\n\n\n\n\n\nFigure 1.1: Three modules of a capital modeling system.\n\n\n\nWe can mentally decompose a capital modeling system into three components shown in Figure 1.1. By far, the biggest and most complex module represents the stochastic business of an insurer. This is labeled Business Operations in the diagram. A capital modeling system must represent some or all of the firm’s portfolio of insurance liabilities.\nThere is a wide range of sophistication possible to this representation, depending on the particulars of the purpose for which the system is to serve. The core concept is the loss which the portfolio might incur. This loss, being a priori uncertain, is a random variable. It might be represented by a distribution of losses or by a set of loss scenarios, which we will call events. The output could include just the losses, or it could include a complete set of financial statements—balance sheet, income statement, and cash flows—all of which ultimately depend on the losses. If the latter, there are numerous accounting standards that could be followed. These possibilities are discussed in Chapter 2.\nOutput from the Business Operations module feeds into the much simpler Capital Adequacy and Pricing & Allocation modules. This is signified by the solid arrows. Dotted arrows signify that these two modules, in turn, affect business operations in the real world and hence, indirectly, the Business Operations module. Indeed, the real-world effect of these modules is the raison d’être of the entire capital modeling system.\nFigure 1.2 expands the Business Operations module.\n\n\n\n\n\n\nFigure 1.2: Internal structure of Business Operations module.\n\n\n\nStarting at the left of the diagram is the Units module, with inputs labeled U. We use the term “unit” to refer to a component of the portfolio. A unit can be a single policy or a group of policies defined by line of business, geography, branch, business unit, or other characteristics. A unit can also represent the segmentation between ceded and assumed reinsurance.\nThe output of Units is a representation of experience, which means paid loss, at the very least, but could also include a time series of loss developments, reserves, etc. Input parameters are things like exposure and possibly shape parameters for loss distributions.\nThese loss outputs are likely to be correlated (not independent) across the units either by common economic or social factors or enforced correlation via copulas, Iman-Conover sampling (Section 8.2), etc.\nIf investment experience is included as one (or more than one) unit, then input from the Economic Scenario Generator (labeled ESG) will be important. This is an optional module which represents external economic factors such as interest rates, returns on various asset classes, rates of monetary inflation, unemployment, etc.\nThe Reinsurance module, with inputs labeled R, modifies the gross experience of the units. This includes all outward (ceded) reinsurance. Any inward (assumed) reinsurance would be represented among the units. The output, net experience, includes the effect of any retrocession.\nAccounting translates experience realizations into pro formas, i.e., financial statements. This output from the Accounting module is a core element of the system. Multiple occurrences of pro formas, generated on the fly or possibly stored in a database, are used in stochastic simulation, whereas single occurrences are used for analyzing specific events. The level of detail here must match the specification of the units.\nThe system is likely to include the input labeled Capital Structure. This specifies, at the very least, the total amount of assets available to pay claims. It may also specify details of noninsurance liabilities, such as bonds, preferred stock, and equity issued by the firm. The Economic Scenario Generator may have some influence on the outward cash flows—which reduce assets—from preferred and common stock dividends. The Accounting module would take into account the fact that losses in excess of total assets lead to insolvency and haircuts in claim payments. If there is no capital structure information at all, asset limitations are ignored and assets are in effect assumed to be infinite.\n\n\n\n\n\n\nFigure 1.3: Full detail of the capital modeling system.\n\n\n\nFigure 1.3 expands the picture by bringing in the remaining modules.\nThe Capital Adequacy module, with inputs labeled C, operates on the pro formas and determines the minimal amount of assets to satisfy requirements from regulators, rating agencies, and the risk committee of the firm’s board of directors. Such determinations could include a simple Value at Risk (VaR) calculation or more complex Best’s Capital Adequacy Ratio (BCAR) calculation. The module also includes risk reports—analysis of what is driving the need for assets. The input parameters can be regarded as the firm’s risk tolerance, specifying what level of risk is acceptable for a given amount of capital or vice versa. Outputs feed back into the real world and implicitly affect the capital structure, hence the dotted line. This is discussed in more detail in Chapter 3.\nThe Pricing module has two separately identified inputs. First, the Profit Target specifies the desired overall profitability of the portfolio as a whole. This could be posed as an aggregate premium, margin, loss ratio, or return on capital; these are equivalent specifications discussed in Chapter 4. The second input, labeled P, specifies more esoteric properties such as the shape of the distortion function underlying an SRM—also discussed in Chapter 4. The two inputs can be interpreted as specifying the firm’s risk appetite.\nAllocation takes the required portfolio premium as given and allocates the premium (equivalently margin, loss ratio) to the individual units. This information, presumably, has real-world impacts on the units via performance management, risk management, underwriting actions, etc. Impact is represented by the dotted arrows to Units.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "010_Introduction.html#sec-Intro-Insco",
    "href": "010_Introduction.html#sec-Intro-Insco",
    "title": "1  Introduction",
    "section": "1.6 Introduction to InsCo",
    "text": "1.6 Introduction to InsCo\nInsCo is our one-period simplified model of insurance company operations.\nWe will use an extremely simple toy example to illustrate the operation of the system presented in Section 1.5. It deliberately abstracts away from much of the complexity, detail, and nuance of the real world. Readers should be able to reproduce the example easily in a spreadsheet.\nA more complex and realistic example can be found in Section 9.5, and more in Mildenhall and Major (2022). Those, as well as this simple example and even more complex portfolios, can all be analyzed with aggregate, an open-source Python library Mildenhall (2024) that “builds approximations to compound (aggregate) probability distributions quickly and accurately.” Because aggregate uses Fast Fourier Transforms, it “delivers the speed and accuracy of parametric distributions to situations that usually require simulation” and is therefore much faster than simulation. All the analytical techniques discussed in this monograph can be implemented in aggregate.\nInsCo is a limited liability company that intermediates between insureds and investors. InsCo’s customers are insureds (policyholders) who are subject to risks they wish to insure. Insureds who use insurance for risk transfer or financing are sensitive to insurer quality and possible default because it correlates with their own misfortune.\nInsurance legal entities serve two principal purposes. First, they provide statutory insurance such as mandatory automobile liability. Second, they allow insureds to pool together and benefit from diversification without requiring onerous bilateral contracts. They do this through insolvency rules, which provide the framework under which unrelated insureds interact in the unlikely event of an insolvency.\nInsCo comes into existence at time \\(t=0\\) and lasts for one period. InsCo has no initial liabilities. At \\(t=0\\), it writes one or more single-period insurance contracts and collects premiums from its insureds.\nWhen InsCo writes a policy, it collects premium at \\(t=0\\) and earns it over the period. We assume all other transactions occur at the end of the period. Therefore all the premium is earned and available to pay claims at \\(t=1\\). If InsCo’s ending assets \\(a\\) are insufficient to pay the claims, then it defaults.\nInsCo has promised to pay policyholders claims under various contingencies, with the aggregate promise represented by the random variable \\(X\\ge 0\\). If \\(X&gt;a\\), then only \\(a\\) gets paid out, i.e., the actual payments are the minimum of \\(X\\) and \\(a\\), which we write as \\(X\\wedge a\\). We assume the probability distribution of \\(X\\) is known.\nInsCo is owned by investors who provide risk-bearing capital. Investors are also risk averse. The market structure is displayed in Figure 1.4.\n\n\n\n\n\n\nFigure 1.4: InsCo, insureds, regulators, and investors: roles and cash flows.\n\n\n\nAt time \\(t=0\\), while collecting premiums, InsCo also raises capital from investors by selling them its uncertain \\(t=1\\) residual value. That is, at time \\(t=1\\), InsCo pays any claims due in the amount of \\(X \\wedge a\\) and pays any residual value \\((X-a)^+\\), if it exists, to its investors as return of capital plus a dividend or investment return. If InsCo’s ending assets are insufficient to pay the claims, \\(X&gt;a\\), then it defaults. Investors have limited liability: they may lose their original investment but owe nothing more.\nPremiums cover expected losses and loss adjustment expenses, as well as the cost of capital including frictional capital costs. All other expenses are outside our model.\nSymbolically, at time \\(t=0\\), InsCo collects premiums \\(P\\) from policyholders and capital \\(Q\\) from investors. These are the only sources of funds and comprise the total assets via the funding equation: \\[\na = P+Q.\n\\tag{1.1}\\] Two important questions arise from InsCo’s promises to pay.\n\nAre there sufficient assets to honor those promises?\nAre investors being adequately compensated for taking on those risks?\n\nCrucially, we need to talk about not one but two different risk measures to answer these questions.\nQuestion 1 concerns risk tolerance and is answered by the Capital Adequacy module. It determines the assets necessary to back an existing or hypothetical portfolio at a given level of risk. This exercise can also be reverse engineered: given existing or hypothetical assets, what constraints on business does the risk tolerance entail? Alternatively, given business and capital, what is the implied risk tolerance?\nAssets \\(a\\) and liabilities \\(X\\) are related by some rule driven by a combination of regulatory authorities, rating agencies, and InsCo’s own internal risk management policies, representing a risk tolerance. Such a rule we call a capital risk measure, and we may write \\(a\\) as a functional \\(a(X)\\). Value at Risk (VaR) or Tail Value at Risk (TVaR) at some high confidence level, such as 99.5% or 1 in 200 years, are both popular, but other possible measures exist, see Chapter 3. As a first approximation, we may take it that \\(a\\) is sufficient to avoid insolvency altogether, i.e., in all events, all claims are paid.\nQuestion 2, answered by the Pricing module, concerns how that asset amount \\(a\\) is to be split between premium \\(P\\) and capital \\(Q\\) (Equation 1.1); this is quite different from determining \\(a\\). It is about risk pricing or risk appetite. We must determine the expected margin insureds need to pay in total to make it worthwhile for investors to bear the portfolio’s risk. Such a rule we call a pricing risk measure, and we may write premium as a functional \\(P = \\rho(X)\\).\n\n\n\n\n\n\nActuarial Standards Board. 2011. “Treatment of Profit and Contingency Provisions and the Cost of Capital in Property / Casualty Insurance Ratemaking.” Actuarial Standard of Practice No. 30.\n\n\nActuarial Standards Board. 2019. “Modeling.” Actuarial Standard of Practice No. 56, no. 56.\n\n\nBrehm, Paul J, Spencer M Gluck, Kreps Rodney E, et al. 2007. “Enterprise Risk Analysis for Property & Liability Insurance Companies.” Guy Carpenter & Company, LLC.\n\n\nEIOPA. 2009. Directive 138/2009/EC (Solvency II Directive).\n\n\nEIOPA. 2015. Delegated Regulation (EU) 2015/35.\n\n\nEmma, Charles C et al. 2000. Dynamic Financial Models of Property-Casualty Insurers. Casualty Actuarial Society. https://www.casact.org/sites/default/files/database/forum_00wforum_00wf317.pdf.\n\n\nGabaix, Xavier, and David I Laibson. 2008. The Seven Properties of Good Models.\n\n\nIAIS. 2008. Standard on the Use of Internal Models for Regulatory Capital Purposes. no. July.\n\n\nMcClenahan, Charles L. 1999. “Insurance Profitability.” Actuarial Considerations Regarding Risk and Return, 113–24.\n\n\nMildenhall, Stephen J. 2024. “Aggregate: fast, accurate, and flexible approximation of compound probability distributions.” Annals of Actuarial Science, 1–40. https://doi.org/10.1017/S1748499524000216.\n\n\nMildenhall, Stephen J. n.d. Aggregate Software Documentation.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.\n\n\nRobbin, Ira. 1992. The Underwriting Profit Provision.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html",
    "href": "020_BusinessOperations.html",
    "title": "2  Business operations",
    "section": "",
    "text": "2.1 General considerations\nRemember that all models are wrong; the practical question is how wrong do they have to be to not be useful.\n—George E. P. Box\nModeling the insurance business operations—in terms of cash flows and other accounting concepts—is the most significant part of capital modeling work. It would be impractical to try to explain this in detail here; multiple book volumes would be required. Indeed, much of the actuarial syllabus is devoted to knowledge that could be applied to this effort. Instead, this chapter offers an overview using a simple example and adds pointers into the existing literature.\nAfter you have addressed the broad questions posed in Section 1.3, you will have to drill down into more specific design issues such as:\nSee Emma et al. (2000) chapter 4 for more some considerations on these dimensions of functionality; chapter 7 goes into more detail about outputs and reports. Chapter 5 presents a classification of risks:\nHow deeply each is to be modeled is part of the requirements to be determined. Emma et al. (2000) discusses the first three at length, but not the last.\nSubsequent sections in this chapter elaborate on some of these issues.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html#sec-BO-General-Considerations",
    "href": "020_BusinessOperations.html#sec-BO-General-Considerations",
    "title": "2  Business operations",
    "section": "",
    "text": "Will the model be deterministic or stochastic?\nWhat is the planning time horizon: one year versus an ongoing concern?\nWhat is the calculation time horizon: one year versus to ultimate?\nWhat is the time granularity: annual, quarterly, monthly, etc.?\nWill it deal with the runoff of current business only, or will it model an ongoing concern?\nHow will it treat reserve risk?\nHow will it treat asset risk?\nHow will it treat investment income (risk-free rate, crediting, capital charge, simplifications from discounting)?\nHow will multiple related legal entities be reflected in the model?\nWhat is the balance of variables between user-selected inputs (e.g., plan premium) and modeler-selected inputs (e.g., correlation or severity)?\nWhat accounting and regulatory frameworks and constraints are supported (US Statutory, Solvency II, US GAAP, IFRS 17, rating agencies)?\nHow will the model relate to other corporate systems?\n\n\n\nAsset risk (default, market value, liquidity)\nObligation risk (reserve, premium, loss projection, catastrophe, reinsurance, and expense)\nInterest rate risk (asset, obligation, and net cash flows)\nMismanagement risk",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html#sec-BO-Units",
    "href": "020_BusinessOperations.html#sec-BO-Units",
    "title": "2  Business operations",
    "section": "2.2 Units",
    "text": "2.2 Units\nBusiness operations start with the individual business units. These are the sources of premiums, losses, and some of the expenses. Expect to devote considerable effort to modeling losses; your loss model implementations will live in the Units module.\nThe design issues at this stage include:\n\nGranularity of represented components, i.e., breakdowns by geography, line of business, etc.\nGranularity of frequency and severity assumptions.\nInclusion of parameter uncertainty, see Section 8.1.\nInteractions among variables.\nFeedback loops, i.e., embedded automatic conditional decisions. For example, future rate changes might be calculated based on emerging loss ratios.\n\nThe existence of mismanagement risk (Section 2.1) suggests that building in feedback loops—“management in a box”—might be too optimistic. We strongly recommend against trying to do this. Adjusting rates to follow inflation is reasonable because this is very likely to happen in reality. Adjusting rates on the basis of a previous random good or bad year is not reasonable because it is highly speculative as to what will happen in reality. Instead, have the model in effect state “if nothing is changed, this is what would likely happen next….”\nDaykin et al. (1993) includes the following chapters relevant to the Units submodule:\n\n2 - Frequency\n3 - Severity\n4 - Compound distributions\n5 - Simulation\n9 - Extended time horizons, including reserves\n10 - Premiums and markets\n11 - Expenses\n12 - Cycles and growth\n\nBrehm et al. (2007) includes:\n\n5.1 - Frequency and severity distributions\n5.2, 5.3 - Reserves\n5.5 - Underwriting cycles\n\nNearly the entirety of Klugman et al. (2019) is devoted to selecting, fitting, and applying mathematical models of losses.\nIf you are modeling reserves, you must accurately represent your own firm’s methodology. The literature on setting reserves is enormous, with Friedland (2010) a good place to start. Seventeen text references on advanced reserving are listed at Casualty Actuarial Society (2022). Wuthrich and Merz (2015) is a rich source of information on stochastic reserve models. Szkoda et al. (1995) has a list of about 200 reserving considerations. Actuarial Standards Board (2007) is also relevant.\nModeling the correlation between units’ loss experience can be particularly tricky. Reshuffling simulated loss outcomes to obtain a desired correlation matrix can be done by the Iman-Conover methods, see Section 8.2.\nAas et al. (2009), Embrechts (2010), and Brehm et al. (2007) section 3.3 discuss copulas, which are mechanisms for introducing dependency that goes beyond simple correlation. For example, the routine loss bodies of two loss distributions can be uncorrelated while the high-loss tails become correlated.\nAnother technique for inducing dependence is the use of common causes. If there are underlying phenomena that influence losses in different units, then modeling those first, and the conditional distribution of unit losses second, can induce the desired dependence. This is most commonly seen in catastrophe modeling but can also appear in a technique called timeline simulation (Brehm et al. 2007, sec. 3.4), where specific events are modeled as occurring at specific points of time.\nWe now illustrate the workings of the Business Operations module with our InsCo example.\nIn a real engagement, we would start with historical data on exposures, premiums, and losses. They would be on-leveled to represent the current (or anticipated next accounting year) situation. Probabilistic models would be fit, and a simulated sample of future possible outcomes generated. This work is covered early in the actuarial exam syllabus, and we imagine it has already been done.\nInsCo has units \\(i=1,\\dots,m=3\\), called A, B, and C. We represent their random experience \\(X^i\\) by a table consisting of 10 outcomes with equal probabilities \\(p_j=1/10\\), \\(j=1,\\dots,n=10\\) with components \\(X_j^i\\) and portfolio totals \\(X_j\\). The exact assumptions are detailed in Table 2.1.\n\n\n\n\n\nTable 2.1: InsCo loss probability mass function by unit with expected loss, CV loss, and plan premium.\n\n\n\n\n\n\n\n\n\nEvent \\(j\\)\nUnit A \\(X^1_j\\)\nUnit B \\(X^2_j\\)\nUnit C \\(X^3_j\\)\nTotal \\(X_j\\)\n\n\n\n\n1\n15\n7\n0\n22\n\n\n2\n15\n13\n0\n28\n\n\n3\n5\n20\n11\n36\n\n\n4\n7\n33\n0\n40\n\n\n5\n13\n20\n7\n40\n\n\n6\n5\n27\n8\n40\n\n\n7\n15\n16\n9\n40\n\n\n8\n26\n19\n10\n55\n\n\n9\n17\n8\n40\n65\n\n\n10\n16\n20\n64\n100\n\n\nEX\n13.4\n18.3\n14.9\n46.6\n\n\nCV\n0.453\n0.412\n1.324\n0.455\n\n\nPlan Prem\n13.9\n18.7\n19.6\n52.2\n\n\n\n\n\n\n\n\n\nThere is some correlation among the simulated experience of the units. This correlation might have been imposed by a sampling technique involving copulas, but here we simply present the results. Table 2.1 represents the loss portion of the Unit module. Statisticians would call this table a discrete joint distribution, sample, or empirical distribution. Cat modelers might call it a probabilistic database. It shows expected loss for the portfolio is 46.6, plan premium is 52.2, and hence plan profit, or margin, is 5.6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html#sec-BO-Reinsurance",
    "href": "020_BusinessOperations.html#sec-BO-Reinsurance",
    "title": "2  Business operations",
    "section": "2.3 Reinsurance",
    "text": "2.3 Reinsurance\nAssumed reinsurance, i.e., selling reinsurance or retrocessional cover to other insurance companies, should be treated as another unit, like homeowners or auto. Ceded reinsurance can be treated in one of two ways.\nThe simplest way is to treat reinsurance as a negative unit in parallel with the underlying unit. The premiums it receives are booked as negative (because they are really paid), and the losses it pays are booked as negative (because they are really received). For example, say there is the homeowners line of business being protected with catastrophe reinsurance. Then, the homeowners gross premiums and losses are posted to one unit and the reinsurance ceded premiums and ceded losses are posted as negatives to a parallel ceded homeowners unit. The algebraic sum of premiums (respectively, losses) constitute the net homeowners position which is not explicitly represented.\nThe more complicated way, and that implied by the design presented in Section 1.5, is to have the operation of reinsurance modeled downstream of the units. This is recommended because there are many situations, such as corporate catastrophe covers, which impact multiple units.\nIn the end, it is simply a computational design choice about where calculations occur. Mathematically, the same gross-to-net transformation is being modeled either way.\nThe capital modeler may encounter two different types of reinsurance. The first is that purchased by business units. It is often reasonable for the corporate modeler to work with distributions net of this reinsurance, at least initially—business units typically frown on corporate modelers criticizing their reinsurance decisions! The second is reinsurance purchased at the corporate level that covers a number of different business units. Catastrophe reinsurance is typically of this form. Optimizing the purchase of catastrophe reinsurance (and allocating its cost to business units) is a wonderful application of capital models, and corporate purchases should usually be modeled explicitly to facilitate it.\nCounterparty credit risk should be handled by representing the probabilistic nature of ceded losses. What is supposed to be received as a reinsurance payout may or may not actually occur. Brehm et al. (2007) chapter 6.1 is devoted to reinsurance receivables as a risk class.\nOur InsCo example includes a 35 excess of 65 portfolio stop-loss contract with ceded losses of 35 occurring only in event 10. There is no counterparty risk. This example is discussed in Section 5.2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html#sec-BO-ESG",
    "href": "020_BusinessOperations.html#sec-BO-ESG",
    "title": "2  Business operations",
    "section": "2.4 Asset risk and Economic Scenario Generator",
    "text": "2.4 Asset risk and Economic Scenario Generator\nWhile the primary focus of the capital model is on how insurance liabilities impact the profitability and solvency of the firm, it may be desirable to represent contingencies on the asset side. First, to model assets as another source of volatility in the firm’s financial position, and, second, to model how macroeconomic phenomena impact the insurance liabilities themselves. The key to modeling asset risk is a module that simulates interest rates and returns on various asset classes and possibly unemployment, inflation, and other socioeconomic indices. Such a module is generally referred to as an Economic Scenario Generator.\nBrehm et al. (2007) chapter 6.2 is devoted to investments. Daykin et al. (1993) chapter 7 deals with inflation and chapter 8 is devoted to investments and asset/liability considerations such as the dynamics of reinvestment. Conning (2020) provides a comprehensive “basic guide to Economic Scenario Generators, with an emphasis on applications for the property/casualty insurance industry.” Also note that the American Academy of Actuaries and the Society of Actuaries (SOA) have joined resources to manage the Economic Scenario Generators used in regulatory reserve and capital calculations (Society of Actuaries 2024). We are not using an Economic Scenario Generator in the InsCo example.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html#sec-BO-Capital-Structure",
    "href": "020_BusinessOperations.html#sec-BO-Capital-Structure",
    "title": "2  Business operations",
    "section": "2.5 Capital structure",
    "text": "2.5 Capital structure\nCapital structure, in corporate finance, refers to the division of liabilities corresponding to the sources of funds used to finance the firm. The basic division consists of shareholders’ equity, (possibly) preferred stock, various types of debt, and possibly other liabilities. Reinsurance can also be considered part of the capital structure. See Mildenhall and Major (2022) for more on this perspective.\nAn important aspect of capital structure is its hierarchical nature insofar as there exists a pecking order in which the claimants providing these funds must be repaid in the course of liquidation. Insureds, providing premiums, come first. Common stockholders come last.\nFor an insurance entity, capital refers to the excess of assets over policyholder liabilities, whereas equity refers to the excess of assets over all other liabilities. Thus, debt can count as capital but not equity. For example, InsCo’s assets consist of cash in the amount \\(a=100\\). The corresponding liabilities—the capital structure—consist of unearned premiums, a bond that InsCo had issued in the past and upon which it must pay a 3% annual coupon, and shareholder equity. See Table 2.2.\nIn the analysis presented in this monograph, it is usually sufficient to distinguish premiums \\(P\\) from capital \\(Q\\) (but adhere to Equation 1.1). However, it may be desirable to distinguish components of \\(Q\\) in reporting the financial fortunes of the firm. The hierarchy of claimants induces thresholds or benchmarks to distinguish degrees of financial distress or insolvency. This is elaborated upon in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html#sec-BO-Accounting",
    "href": "020_BusinessOperations.html#sec-BO-Accounting",
    "title": "2  Business operations",
    "section": "2.6 Accounting",
    "text": "2.6 Accounting\nAccounting is the language of business that defines the measurement of success or failure. It has its own vocabulary and grammar. You must have someone well versed in insurance accounting on the model development team. Whatever underlying representation for business cash flows the model uses must be translated into one or more standard accounting frameworks. This is material covered in the Casualty Actuarial Society’s Exam 6. A good place to start is the detailed reading by Odomirok et al. (2020) or the earlier short introduction by Blanchard III (2008).\nAccounting is important for several reasons.\nFirst, model output must be readily integrated with the firm’s business plan. The plan is the key document orienting the goals of the company. The capital model is intended to express and quantify the possible future variations from the plan that may emerge. In order to be useful and readily communicated to a variety of business actors in the firm, the model output needs to be expressed in the same terms as, and in a comparable format to, the plan. Companies often use their own management accounting conventions, usually slight variations on their reporting or regulatory standards. Therefore, the outputs must hew to the same accounting standards as the plan.\nSecond, accounting has real-world consequences. Insurers are subject to various types of valuation accounting standards, including:\n\nStatutory or regulatory standards, such as US NAIC, EU Solvency II, APRA.\nFinancial reporting standards, such as GAAP and IFRS.\nRating agency standards, such as Standard and Poor’s and AM Best’s.\n\nFailure to maintain an adequate financial position under any one of these standards could prove ruinous to the firm. This is elaborated on in Mildenhall and Major (2022), section 8.3.\nEmma et al. (2000) chapter 7 opines that models should simultaneously represent at least cash (economic), statutory, reporting (GAAP, IFRS17), and tax accounting because “[t]his is the only way to reflect the details of the interrelationships among constraints.” The model must model cash flows, which form the basis for the other accrual accounting models. In addition to representing external reporting standards, it is advisable to support your entity’s management accounting, which is usually a slight variant of its reporting standard.\nFigure 2.1 illustrates the relationship between three accounting views. For example, reserves are undiscounted under US GAAP and the market view of loss reserves may differ from management’s.\n\n\n\n\n\n\nFigure 2.1: Capital, surplus, and equity under market value, GAAP, and statutory accounting views. From Mildenhall and Major (2022), used by permission.\n\n\n\nAccounting produces income and balance sheet statements. The InsCo beginning period \\(t=0\\) balance sheet is shown in Table 2.2. Recall that the total plan premium is 52.2, received at the beginning of the policy period; expenses are excluded from this model and are assumed to be zero.\n\n\n\nTable 2.2: InsCo’s starting balance sheet.\n\n\n\n\n\nAssets\n\nLiabilities\n\n\n\n\n\nCash\n100.0\nUnearned premium reserve\n52.2\n\n\n\n\nDebt (bond)\n20.0\n\n\n\n\nShareholder equity\n27.8",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "020_BusinessOperations.html#sec-BO-Pro-Forma",
    "href": "020_BusinessOperations.html#sec-BO-Pro-Forma",
    "title": "2  Business operations",
    "section": "2.7 Pro formas",
    "text": "2.7 Pro formas\nPro forma financial statements are hypothetical financial statements about future states of the business. Whatever internal representation the model uses for outcomes, they must be translated into standard fiancial reports. We focus here on InsCo’s balance sheet and income statements. Others are possible, including:\n\nCash flow\nSources and uses of funds\nChange in equity\nComprehensive income\nVarious statutory blanks, NAIC annual statement\n\nFor our one-period InsCo, the ending period shareholder value, if positive, is released to investors at time \\(t=1\\). For event 4 in Table 2.1, the income statement and balance sheet prior to paying losses and dividends are shown in Table 2.3 and Table 2.4. The balance sheet after payments is obviously \\(0=0\\) because InsCo is a one-period entity.\n\n\n\nTable 2.3: InsCo income statement, event 4.\n\n\n\n\n\nIncome\n\nExpense\n\n\n\n\n\nPremiums\n52.2\nLoss & LAE\n40.0\n\n\n\n\nBond coupon\n0.6\n\n\n\n\nShareholder dividends\n11.6\n\n\n\n\n\n\n\n\n\nTable 2.4: InsCo ending balance sheet, event 4 prior to paying losses.\n\n\n\n\n\nAssets\n\nLiabilities\n\n\n\n\n\nCash\n100.0\nUnearned premium reserve\n0.0\n\n\n\n\nLoss reserves\n40.0\n\n\n\n\nDebt (bond)\n20.0\n\n\n\n\nAccrued interest\n0.6\n\n\n\n\nShareholder equity\n39.4\n\n\n\n\n\n\nThe Accounting module would produce 10 such pairs of financial statements.\nStatutory reporting, done to satisfy regulatory authorities, is a deep and complex subject. Actuaries operating in the US should be familiar with the NAIC procedures manuals that can be found in Appendix V of Koca et al. (2023).\n\n\n\n\n\n\nAas, Kjersti, Claudia Czado, Arnoldo Frigessi, and Henrik Bakken. 2009. “Pair-copula constructions of multiple dependence.” Insurance: Mathematics and Economics 44 (2): 182–98. https://doi.org/10.1016/j.insmatheco.2007.02.001.\n\n\nActuarial Standards Board. 2007. “Property/Casualty Unpaid Claim Estimates.” Actuarial Standard of Practice No. 43.\n\n\nBlanchard III, Ralph S. 2008. Basic Insurance Accounting – Selected Topics. July.\n\n\nBrehm, Paul J, Spencer M Gluck, Kreps Rodney E, et al. 2007. “Enterprise Risk Analysis for Property & Liability Insurance Companies.” Guy Carpenter & Company, LLC.\n\n\nCasualty Actuarial Society. 2022. Complete Text References for Exam 7, Syllabus of Basic Education. https://www.casact.org/sites/default/files/2021-03/7_individual_textref.pdf.\n\n\nConning. 2020. “A User’s Guide to Economic Scenario Generation in Property/Casualty Insurance.” CAS Research Papers. https://www.casact.org/sites/default/files/2021-02/economic-scenario-generation-conning1020.pdf.\n\n\nDaykin, Chris D, Teivo Pentikainen, and Martti Pesonen. 1993. Practical Risk Theory for Actuaries. Chapman; Hall/CRC.\n\n\nEmbrechts, Paul. 2010. “Risk Aggregation presentation.” Copula Theory and Its Applications, 111–26. https://doi.org/10.1007/978-3-642-12465-5.\n\n\nEmma, Charles C et al. 2000. Dynamic Financial Models of Property-Casualty Insurers. Casualty Actuarial Society. https://www.casact.org/sites/default/files/database/forum_00wforum_00wf317.pdf.\n\n\nFriedland, Jacqueline. 2010. “Estimating Unpaid Claims Using Basic Techniques.” https://www.casact.org/sites/default/files/database/studynotes_friedland_estimating.pdf.\n\n\nKlugman, Stuart A, Harry H Panjer, and Gordon E Willmot. 2019. Loss Models: From Data to Decisions. Vol. 715. John Wiley & Sons.\n\n\nKoca, Stephen et al. 2023. Statements of Actuarial Opinion on Property and Casualty Loss Reserves. American Academy of Actuaries.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.\n\n\nOdomirok, Kathleen C, Liam M Mcfarlane, Gareth L Kennedy, and Justin J Brenden. 2020. Casualty Actuarial Society Financial Reporting through the Lens of a Property/Casualty Actuary.\n\n\nSociety of Actuaries. 2024. Economic Scenario Generators|SOA. https://www.soa.org/resources/tables-calcs-tools/research-scenario/.\n\n\nSzkoda, Susan T et al. 1995. CAS Dynamic Financial Analysis Property/Casualty Insurance Companies Handbook. Casualty Actuarial Society. https://www.casact.org/sites/default/files/database/forum_96wforum_96wf001.pdf.\n\n\nWuthrich, Mario V., and Michael Merz. 2015. Stochastic Claims Reserving Manual: Advances in Dynamic Modeling.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Business operations</span>"
    ]
  },
  {
    "objectID": "030_CapitalAdequacy.html",
    "href": "030_CapitalAdequacy.html",
    "title": "3  Capital adequacy and sources of risk",
    "section": "",
    "text": "3.1 Capital adequacy\nToo much of everything is just enough.\n—Grateful Dead, “I Need a Miracle”\nWhat quantity of assets is needed to back the liabilities the firm is taking on? The answer depends on who you ask. Insureds want more protection, meaning lower probability of insolvency and therefore higher assets. Regulators, representing the public, are aligned with the insureds. Investors want to minimize their own risk, so they want to invest less capital (for a given expected profit). Rating agencies are concerned with the counterparty (bond) creditworthiness and claims-paying ability of the firm, so they want to see an adequate level of lower-priority (equity, reinsurance) capital. The firm’s own management must adjudicate these competing demands in their own minds: on the one hand, they don’t want the firm to go under, but on the other hand, they need to deliver an acceptable rate of return to investors. They will have their own view of the ideal level of assets.\nEven 100% collateralization of liabilities only theoretically guarantees 100% probability of solvency. In practice, insolvency can be caused by nonmodeled or inadequately modeled operational risks, such as fraud, rogue actors within the organization, management missteps, or bad-faith claim judgments in excess of policy limits. Capital adequacy is all about pegging an amount of assets deemed to be safe enough.\nIn practice it works like this:\nWhile insurance regulations and laws in the US are created and enforced by the individual states, the National Association of Insurance Commissioners (NAIC) develops model laws, regulations, and standards which most states follow. The NAIC created and maintains the Risk-Based Capital (RBC) framework, which is the primary method regulators use to assess capital adequacy. It specifies minimal capital requirements lest an insurer be put under watch or, worse, taken over by the regulators. NAIC (2024) provides background on RBC and links to related content.\nInsurers in the European Union are bound by the Solvency II Directive (EIOPA 2009). This specifies a Solvency Capital Requirement to be calculated either by a standard model or by a firm’s internal model. See IAA Risk Margin Working Group (2009), Floreani (2011), and Meyers (2019).\nRating agencies include S&P Global Ratings, AM Best, Moody’s, and Fitch. They are primarily concerned with credit risk, i.e., assigning bonds a letter grade, but they also concern themselves with insurer claims paying ability, so there are two distinct types of rating. Rating agencies have their own models, often factor-based models similar to NAIC’s RBC. Karakuyu et al. (2023) describe “S&P Global Ratings’ methodology and assumptions for analyzing the risk-based capital (RBC) adequacy of insurers and reinsurers.” Jakobsen et al. (2020) explain AM Best’s Universal BCAR (Best’s Capital Adequacy Ratio) model. Chan et al. (2024) “specifies Fitch Ratings’ criteria for assigning new and monitoring existing international scale insurer Financial Strength (IFS) ratings.” Readers should consult the agencies’ websites for the most recent models, as these frameworks are periodically revised.\nLarger insurance firms, especially publicly traded firms, tend to be organized into multiple legal entities, groups, and subsidiaries. Szkoda et al. (1995) state: “To properly analyze the financial condition of a company with subsidiaries, each subsidiary should be analyzed separately.” Also, “understand debt obligations of company and parent company, look through on debt structure to determine if subsidiaries, etc., have sufficient cash flows to meet parent’s obligations.” Emma et al. (2000) say that either a consolidated model with internal cash flows between entities or entirely separate models (feeding each other) can be developed. Bear in mind that in stressed situations capital can become trapped.\nBrehm et al. (2007) chapter 2.3 compares regulatory and rating agency capital adequacy models. Eling and Holzmüller (2008) provide an overview and comparison of RBC requirements from the US, the EU, Switzerland, and New Zealand, although it is somewhat dated.\nThe firm’s management will likely attend to a hierarchy of event possibilities (not necessarily distinct or in order):\nAll these are highly correlated with the size of loss experienced in an accounting period, and all but missed earnings and market effects can be ameliorated by holding more capital.\nConsider the InsCo example described in Table 3.1. If losses exceed 52.2 (premiums), then InsCo’s investors will not receive any gain on their investment. This event has a probability of 30%. If losses are 80 or more (\\(52.2+27.8\\) capital), then shareholders will be wiped out (lose all their investment); the probability is 10%. Sixty cents more and bondholders will not receive their promised coupons, and the company defaults and is put to the debt holders. If losses reach 100 (also a 10% chance), then bondholders will be wiped out as well, suffering a 100% loss-given-default. Regulatory intervention thresholds might be defined by intermediate levels of loss and corresponding probabilities. Any of these might be used as VaR levels to define required assets. This is elaborated on in the next section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capital adequacy and sources of risk</span>"
    ]
  },
  {
    "objectID": "030_CapitalAdequacy.html#capital-adequacy",
    "href": "030_CapitalAdequacy.html#capital-adequacy",
    "title": "3  Capital adequacy and sources of risk",
    "section": "",
    "text": "Local regulatory authorities prescribe a minimum amount of assets and set reserving and valuation standards, which together imply a minimum capital requirement.\nRating agencies specify amounts that correspond to different rating levels (effectively, grades) given to the firm. Their models are generally based on statutory valuations but with adjustments, for example, for discount in loss reserves. The implied capital levels are usually higher than the regulatory minimums.\nManagement decides a target rating level (or in the EU, a ratio of capital to Solvency II–required capital), balancing revenue and profit needs with what they believe their customers will tolerate. For example, personal lines markets are relatively insensitive to ratings, whereas large commercial markets require more highly rated insurers.\nManagement—with the help of the capital model—will look at the probabilities of adverse events causing the end-of-period assets to breach the above thresholds. They will then likely consider changing reinsurance, raising capital, or reducing exposure to keep these probabilities in control. In a startup situation, they might require an extra capital buffer, but a going concern rarely raises capital except for specific growth opportunities.\n\n\n\n\n\n\n\n\nMissed earnings\nSpreads widen on bonds; stock price goes down\nMarket value less than book value\nBond ratings outlook change (watch)\nBond ratings downgrade\nClaims paying ratings downgrade\nBeing placed on regulatory watch\nRegulatory supervision, conservatorship or liquidation\nLoss of significant shareholder equity\nLoss of all shareholder equity\nFailure to pay bond interest\nFailure to fully redeem a bond\nBankruptcy and reorganization\n\n\n\n\n\n\nTable 3.1: InsCo critical loss levels.\n\n\n\n\n\nEvent\nLoss Level\nProbability\n\n\n\n\nZero profit\n52.2\n30%\n\n\nShareholders wiped out\n80.0\n10%\n\n\nFailure to pay coupons\n80.6\n10%\n\n\nFailure to redeem bonds\n100.0\n10%\n\n\n99% average loss (TVaR)\n100.0\n10%\n\n\n\n\n\n\n\n3.1.1 Value at Risk and Tail Value at Risk\nVarious different risk measures can be used to calculate required capital. For a thorough discussion, see section 3.6.3 of Mildenhall and Major (2022). Typically, the goal is to avoid a particular event with specified probability. An exceedance probability measure tells you the probability a firm becomes insolvent at a particular future time given an initial capital level. Inverting that relation determines the capital needed to attain the desired probability of safety. Thus there is a correspondence between risk measures and adequacy measures (Cherny and Madan 2009). Exceedance probability measures are common and lead to one or more Value at Risk (VaR) criteria: given a threshold probability, e.g., 99.5%, what size of loss will not be exceeded at that probability level?\nVaR has a bit of a problem, however. It is not subadditive, which means there are situations where the VaR of the sum of two risks is greater than the sum of the two VaRs. Subadditivity is a good property for a risk measure because it means you manage risk from the bottom up: by managing the risk of each unit to be below its own threshold, you have managed the total risk to be below the sum of the thresholds. There are obvious implications for risk and capacity management. Theoretically, because VaR is not subadditive, it is not a good choice to manage risk at the unit level on up. However, this failure is more an academic quibble than a practical shortcoming. In many real-world situations VaR is subadditive, and it has the added benefits of being easy to understand, estimate, and back-test, and of always existing. In practice VaR is the risk measure of choice for insurance regulators around the world—with the exception of the Swiss.\nAcademics, the Swiss, and bankers in Basel 3 (also in Switzerland), posit a better alternative is Tail Value at Risk (TVaR). TVaR asks, for a given probability level, what is the average loss in the worst events representing that accumulated probability? Defined in this way, TVaR is subadditive, a plus. On the downside, TVaR does not exist for thick-tailed distributions with no mean. It is also much harder to elicit and back-test than VaR. See Mildenhall and Major (2022) for an elaboration on this.\nAnother issue with TVaR is its precise definition. Historically, TVaR was called Expected Shortfall (ES) and Conditional Value at Risk (CVaR) by some authors. In addition, there are two subtly different measures called Tail Conditional Expectation (TCE) and Worst Conditional Expectation (WCE). In many cases these are all the same, but in many other realistic (discrete) cases they are not. See chapter 4 of Mildenhall and Major (2022) for a thorough analysis of VaR and TVaR.\nIn many older texts, TVaR is defined as the average of losses above a specified VaR loss value. This coincides with the TCE definition for continuous random variables but not for discrete or mixed variables, and, in those cases, the resulting measure is not subadditive! More recent literature, led by McNeil et al. (2005), uses the definition we give. Formally, \\[\n\\mathsf{TVaR}_p(X):=\\dfrac{1}{1-p}\\int_{p}^1 \\mathsf{VaR}_s(X)\\,ds=\n\\dfrac{1}{1-p}\\int_{p}^1 F^-(s)\\,ds\n\\] where \\(F^-(p)\\) is the least inverse of the cumulative distribution function. In particular \\(\\mathsf{TVaR}_0(X)=\\mathsf E[X]\\) and \\(\\mathsf{TVaR}_1(X)\\) is defined to be \\(\\sup(X)\\), the largest possible outcome.\nHere are two simple examples to help demystify subadditivity.\n\nDiscrete example. Consider a probability space with three outcomes, with probability 0.9, 0.05, and 0.05. Unit 1 has outcomes 0, 0, and 10, and unit 2 has 0, 10, and 0. The total outcomes are 0, 10, and 10. The \\(p=0.9\\) VaRs are 0 for each unit, but it is 10 in total. The corresponding TVaRs are 5 for each unit and 10 in total.\nContinuous example. It is a curious fact that the sum of two standard exponential distributions fails to be subadditive at \\(p=0.7\\) (and neighboring \\(p\\)), so the failure can also apply for continuous variables. The unit VaRs are \\(-\\log(0.3)=1.204\\) and the sum (a gamma with shape 2) has VaR equal to \\(2.439\\), which is greater than \\(2 \\times 1.204 = 2.408\\).\n\nMirroring the broader VaR-TVaR debate, your authors split in favor of VaR and TVaR. Your capital model must, however, employ some risk measure criteria to determine the amount of assets necessary to protect the firm against adverse loss experience. It can, of course, also cheat and select a (T)VaR threshold equating to actual capital held.\nOur InsCo example uses assets \\(a=100\\), which can be characterized in various ways. One way is to say there is a \\(\\mathsf{TVaR}_{0.99}\\) standard for the amount of assets needed. This is input “C” in Section 1.5. The other way says it is \\(\\mathsf{VaR}_1\\). Both result in \\(a=100\\), the maximum loss among the events, which is the asset value assumed previously. Notice two points. First, in practice the order of operations flows from accountants determining the amount of capital to modelers describing its adequacy (99% TVaR). Second, that \\(a=100\\) corresponds to full collateralization, and so it means there is no possibility of default. Default is not driving any results in this example! Nor should it in the preponderance of realistic models for well-rated companies. We might have chosen any VaR or TVaR level beyond 90% and obtained the same arithmetical results. We use TVaR to illustrate the algebra involved with a nontrivial asset risk measure.\nIn addition to specifying the model capital amount, it would be helpful to have some analytical capabilities to explain what drives the results. This is discussed in the next section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capital adequacy and sources of risk</span>"
    ]
  },
  {
    "objectID": "030_CapitalAdequacy.html#sec-Risk-Analysis",
    "href": "030_CapitalAdequacy.html#sec-Risk-Analysis",
    "title": "3  Capital adequacy and sources of risk",
    "section": "3.2 Risk analysis",
    "text": "3.2 Risk analysis\nA main output of the Capital Adequacy module is an assessment of how often there is sufficient capital to avoid various undesirable outcomes. Natural follow-up questions revolve around explaining those results. Having drill-down and diagnostic capabilities is helpful, especially for complex models. These may include:\n\nStatistics of selected variables\nIdentification of extreme events\nExplaining what variables drive outputs\nEvaluation of decision rules (e.g., reinsurance)\nRisk/reward analysis\nA battery of what-if questions\n\nA systematic battery of what-if questions—what if trend is 3% higher than plan? What if hurricane frequency is actually 20% higher than the cat model assumptions?—can reveal where assumptions are particularly sensitive.\nEven within the plan, there is an opportunity for sophisticated analysis. For example, machine learning could construct a decision tree to characterize simulation events with high losses.\nAt this point, there is a fork in the analytic road with two divergent paths that surprisingly rejoin to produce the same conclusion. The paths lead along a dynamic marginal impact analysis and a more static risk-adjusted probability analysis. Early work of Venter et al. (2006) saw the two paths, and more technical work of Delbaen (2000) actually knew the paths rejoined. Our work, Mildenhall and Major (2022) connected the dots more explicitly in an actuarial context.\nThe first form of risk analysis looks at the marginal impact on required assets of changes in the underlying portfolio. While unrealistic, a linear scaling of the unit loss distributions is often used, i.e., assets \\(a(X)\\) become \\(a((1+\\epsilon)X)\\). This is unrealistic because, e.g., doubling one’s auto exposures homogeneously will not change \\(X\\) into \\(2X\\). While it may approximately double the expected loss, the shape of the distribution will change: the standard deviation will less than double (Mildenhall 2017). Nonetheless, linear scaling is useful as a directional indicator. We will return to the issue of nonlinear scaling in Section 5.5.\nSpecifically, the marginal approach looks at the gradient (slope) of the risk measure: the change resulting from a small change in exposure. If \\(X=\\sum_i X^i\\) and \\(\\phi(X)\\) is a risk measure, then define \\[\n\\nabla^i\\phi(X) = \\frac{\\partial}{\\partial\\epsilon}\\phi(X+\\epsilon X^i)=\\lim_{\\epsilon \\downarrow 0}\\frac{\\phi(X+\\epsilon X^i)-\\phi(X)}{\\epsilon}.\n\\] If \\(\\phi\\) is continuous and homogeneous, that is, \\(\\phi(tX)=t\\phi(X)\\) for \\(t&gt;0\\), then Euler’s homogeneous function theorem tells us that \\[\n\\sum_i \\nabla^i\\phi(X) = \\phi(X),\n\\] i.e., the marginals consist of an allocation of the original measure (Mildenhall 2004).\nIn our example, the risk measure is \\(\\phi(X) = \\mathsf{TVaR}_{0.99}(X)\\). The gradient of this is \\[\n\\nabla^i(\\phi(X)) = \\dfrac{1}{1-p}\\int_{p}^1 \\mathsf E[X^i \\mid X=\\mathsf{VaR}_s(X)]\\,ds,\n\\] often called the coTVaR. Those measures on the three units in our example refer again to the worst of the 10 losses, so the answers are 16, 20, and 64, respectively, see the last event in Table 3.1.\nSo far, the discussion refers to required assets, not capital. To calculate capital, we have to subtract premiums (Equation 1.1). This is detailed in Table 3.2. Ideally, required capital is calculated based on required premiums, but at this stage in our calculations we do not have those numbers.\n\n\n\nTable 3.2: InsCo marginal analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\nRequired assets coTVaR, \\(a\\)\n16.0\n20.0\n64.0\n100.0\n\n\nExpected loss, \\(L\\)\n13.4\n18.3\n14.9\n46.6\n\n\nPlan premium\n13.9\n18.7\n19.6\n52.2\n\n\nRequired capital (plan)\n2.1\n1.3\n44.4\n47.8\n\n\n\n\n\n\n\nNote that while TVaR submits to marginal analysis fairly well, VaR needs special handling. Say we were concerned with \\(\\mathsf{VaR}_{0.85}=65\\). This occurs solidly in event 9 (Table 2.1). A straightforward marginal analysis breaks this down to:\n\n\n\nTable 3.3: InsCo marginal analysis of \\(\\mathsf{VaR}_{0.85}\\).\n\n\n\n\n\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\n17.0\n8.0\n40.0\n65.0\n\n\n\n\n\n\nYet, for both the lower event 8 and higher event 10, line B’s contribution is higher. Imagine a more detailed simulation where each event is only worth 1/10,000 probability. Looking at just that specific event, defining the VaR is inviting a good deal of instability. Another run with different random numbers will likely produce materially different results.\nThere are two ways out of this problem. One is to estimate the conditional expected loss values described in Section 4.3. The other is to use a more robust replacement for VaR known as Range Value at Risk (RVaR) (Cont et al. 2010). This is the average loss occurring between two probability levels. For example, instead of using \\(\\mathsf{VaR}_{0.99}\\), you might use \\(\\mathsf{RVaR}_{0.985,0.995}\\).\nA sign of problems of the marginal approach is the need to stipulate that epsilon  decreases to zero. When there are ties in the outcome distribution, the risk measure is not actually differentiable! It has a cusp, and depending on whether you increase or decrease volume in a given line, you get different answers. The risk measure is analogous to the absolute value function at zero, where the left and right derivatives are different. (We will see this haunt us in Section 5.5.) To ensure the risk measure is differentiable, and a marginal approach is really valid, it is enough to require that all outcomes of the total distribution be distinct. In that case, for a small enough change in any unit, there is no reordering of the total outcomes and the risk measure is differentiable. We will implement that requirement in the next chapter.\nThe second form of risk analysis, again following Venter et al. (2006), looks for a set of risk-adjusted probabilities that reproduce the results of the risk measure. With these adjusted probabilities in hand, it is natural to use the risk-adjusted expected value of each unit as an allocation of risk. This method is the basis of the Natural Allocation described in Mildenhall and Major (2022) and picked up in Chapter 4 to allocate premium.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capital adequacy and sources of risk</span>"
    ]
  },
  {
    "objectID": "030_CapitalAdequacy.html#algorithms",
    "href": "030_CapitalAdequacy.html#algorithms",
    "title": "3  Capital adequacy and sources of risk",
    "section": "3.3 Algorithms",
    "text": "3.3 Algorithms\n\n3.3.1 Algorithm to evaluate expected loss for discrete random variables\nThe algorithm in this subsection is very basic. We present it to establish an approach to working with discrete random variables that we generalize in subsequent chapters.\nWe present an algorithm to compute \\(\\mathsf E[X]\\) in two ways, based on \\[\\begin{equation}\\label{eq:ex-two-means}\n\\mathsf E[X]=\\int_0^\\infty xdF(x) = \\int_0^\\infty S(x)dx.\n\\end{equation}\\]\nAlgorithm Input: \\(X\\) is a discrete random variable, taking finitely many values \\(X_j\\ge 0\\), and \\(p_j=\\mathsf P(X=x_j)\\). \nFollow these steps to evaluate .\nAlgorithm Steps\n\nPad the input by adding a zero outcome \\(X=0\\) with probability 0.\nGroup by \\(X_j\\) and sum the corresponding \\(p_j\\).\nSort events by outcome \\(X_j\\) into ascending order. Relabel events \\(X_0&lt; X_1 &lt; \\dots &lt; X_m\\) and probabilities \\(p_0,\\dots, p_m\\).\nDecumulate probabilities to compute the survival function \\(S_j:=S(X_j)\\) using \\(S_0=1-p_0\\) and \\(S_j=S_{j-1}-p_j\\), \\(j &gt; 0\\).\nDifference the outcomes to compute \\(\\Delta X_j=X_{j+1} - X_j\\), \\(j=0,\\dots, m-1\\).\nOutcome-probability sum-product: \\[\n\\mathsf E[X] =\\int_0^\\infty xdF(x)= \\sum_{j=1}^m X_j p_j.\n\\tag{3.1}\\]\nSurvival function sum-product: \\[\n\\mathsf E[X] = \\int_0^\\infty S(x)dx = \\sum_{j=0}^{m-1} S_{j}\\Delta X_j.\n\\tag{3.2}\\]\n\nComments\n\nStep (1) treats 0 as special because the second integral in starts at \\(X=0\\). Step (1) allows us to systematically deal with any discrete data. It adds a new outcome row only when the smallest observation is \\(&gt;0\\).\nAfter Step (3), the \\(X_j\\) are distinct; they are in ascending order, \\(X_0=0\\), and \\(p_j=\\mathsf P(X=X_j)\\).\nIn Step (4), \\(S_m=\\mathsf P(X&gt;X_m)=0\\) since \\(X_m\\) is the maximum value of \\(X\\).\nThe forward difference \\(\\Delta X\\) computed in Step (5) replaces \\(dx\\) in various formulas. Since it is a forward difference \\(\\Delta X_m\\) is undefined. It is also unneeded.\nIn step (6), the sum starts at \\(j=1\\) because \\(X_0=0\\). Notice that \\(\\mathsf P(X=X_j)=S_{j-1}-S_j\\) is the negative backward difference of \\(S\\). \nNote the index shift between Equation 3.1 and Equation 3.2.\nBoth Equation 3.1 and Equation 3.2 are exact evaluations. The approximation occurs when the underlying distribution being modeled is replaced with the discrete sample given by \\(X\\).\n\n\n\n3.3.2 Algorithm to evaluate VaR for a discrete distribution\n\nAlgorithm input: \\(X\\) is a discrete random variable, taking \\(N\\) equally likely values \\(X_j \\ge 0\\), \\(j = 0, \\dots, N-1\\). Probability level \\(p\\).\nFollow these steps to determine \\(\\mathsf{VaR}_p(X)\\).\nAlgorithm steps\n\nSort outcomes into ascending order \\(X_0 &lt; \\dots &lt; X_{N-1}\\).\nCalculate \\(n=\\lfloor pN\\rfloor\\), the greatest integer less than or equal to \\(pN\\).\nReturn \\(\\mathsf{VaR}_p(X) := X_{n}\\) (since the indexing starts at \\(0\\)).\n\nThese steps compute the least quantile at level \\(p\\). A \\(p\\) quantile is any value \\(x\\) so that \\(\\mathsf P(X &lt; x) \\le p \\le \\mathsf P(X\\le x)\\) and \\(\\mathsf{VaR}_p\\) is the smallest such \\(x\\). If you draw a graph of the distribution function and allow the vertical lines joining jumps to be part of the graph, then this algorithm is just inverting the distribution function by finding the leftmost \\(x\\) at which the graph reaches height \\(p\\), see chapter 4 of Mildenhall and Major (2022).\n\n\n3.3.3 Algorithm to evaluate TVaR for a discrete distribution\nAlgorithm input: \\(X\\) is a discrete random variable, taking \\(N\\) equally likely values \\(X_j\\ge 0\\), \\(j=0,\\dots, N-1\\). Probability level \\(p\\).\nFollow these steps to determine \\(\\mathsf{TVaR}_p(X)\\).\nAlgorithm steps\n\nSort outcomes into ascending order \\(X_0 &lt; \\dots &lt; X_{N-1}\\).\nFind \\(n\\) so that \\(n \\le pN &lt; (n+1)\\).\nIf \\(n+1=N\\), then \\(\\mathsf{TVaR}_p(X) := X_{N-1}\\) is the largest observation, exit;\nElse \\(n &lt; N-1\\) and continue.\nCompute \\(T_1 := X_{n+1} + \\cdots + X_{N-1}\\).\nCompute \\(T_2 := ((n+1)-pN)x_n\\).\nCompute \\(\\mathsf{TVaR}_p(X) := (1-p)^{-1}(T_1+T_2)/N\\).\n\nThese steps compute the average of the largest \\(N(1-p)\\) observations. Step (6) adds a pro-rata portion of the \\(\\lfloor N(1-p)\\rfloor\\) largest observation when \\(N(1-p)\\) is not an integer.\n\n\n\n\n\n\nBrehm, Paul J, Spencer M Gluck, Kreps Rodney E, et al. 2007. “Enterprise Risk Analysis for Property & Liability Insurance Companies.” Guy Carpenter & Company, LLC.\n\n\nChan, Cynthia et al. 2024. Insurance Rating Criteria. Fitch Ratings.\n\n\nCherny, Alexander, and Dilip Madan. 2009. “New measures for performance evaluation.” Review of Financial Studies 22 (7): 2571–606. https://doi.org/10.1093/rfs/hhn081.\n\n\nCont, Rama, Romain Deguest, and Giacomo Scandolo. 2010. “Robustness and sensitivity analysis of risk measurement procedures.” Quantitative Finance 10 (6): 593–606. https://doi.org/10.1080/14697681003685597.\n\n\nDelbaen, Freddy. 2000. “Coherent Risk Measusres.” Blätter Der DGVFM 10 (24(4)): 733–39. https://doi.org/10.1007/BF02808276.\n\n\nEIOPA. 2009. Directive 138/2009/EC (Solvency II Directive).\n\n\nEling, Martin, and David Holzmüller. 2008. “An Overview and Comparison of Risk-Based Capital Standards.” Journal of Insurance Regulation 26 (4): 31–60.\n\n\nEmma, Charles C et al. 2000. Dynamic Financial Models of Property-Casualty Insurers. Casualty Actuarial Society. https://www.casact.org/sites/default/files/database/forum_00wforum_00wf317.pdf.\n\n\nFloreani, Alberto. 2011. “Risk margin estimation through the cost of capital approach: Some conceptual issues.” Geneva Papers on Risk and Insurance: Issues and Practice 36 (2): 226–53. https://doi.org/10.1057/gpp.2011.2.\n\n\nIAA Risk Margin Working Group. 2009. Measurement of Liabilities for Insurance Contracts: Current Estimates and Risk Margins. International Actuarial Assocation.\n\n\nJakobsen, Mathilde, Timothy Prince, Brad Herman, and Mahesh Mistry. 2020. Understanding Universal BCAR.\n\n\nKarakuyu, Ali, Mark Button, et al. 2023. Criteria | Insurance | General: Insurer Risk-Based Capital Adequacy–Methodology and Assumptions.\n\n\nMcNeil, Alexander J., Paul Embrechts, and Rudiger Frey. 2005. Quantitative Risk Management: Concepts, Techniques, and Tools. Princeton University Press. https://doi.org/10.1198/jasa.2006.s156.\n\n\nMeyers, Glenn G. 2019. Stochastic Loss Reserving Using Bayesian MCMC Models. CAS Monograph Series.\n\n\nMildenhall, Stephen J. 2004. “A Note on the Myers and Read Capital Allocation Formula.” North American Actuarial Journal 8 (2): 32–44. http://library.soa.org/library-pdf/naaj0402_3.pdf.\n\n\nMildenhall, Stephen J. 2017. “In Praise of Value at Risk.” Actuarial Review 44(6): 48–50. https://ar.casact.org/in-praise-of-value-at-risk/.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.\n\n\nNAIC. 2024. Risk-Based Capital.\n\n\nSzkoda, Susan T et al. 1995. CAS Dynamic Financial Analysis Property/Casualty Insurance Companies Handbook. Casualty Actuarial Society. https://www.casact.org/sites/default/files/database/forum_96wforum_96wf001.pdf.\n\n\nVenter, Gary G., John A. Major, and Rodney E. Kreps. 2006. “Marginal Decomposition of Risk Measures.” ASTIN Bulletin 36 (2): 375–413. https://doi.org/10.2143/AST.36.2.2017927.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capital adequacy and sources of risk</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html",
    "href": "040_PricingAndAllocation.html",
    "title": "4  Pricing and allocation",
    "section": "",
    "text": "4.1 Pricing the whole\nThis chapter is about the basic theory of allocating premium and capital based on the Natural Allocation of Spectral Risk Measures (SRMs). Here, we present the logic in an informal fashion. Think of it as the CliffsNotes or Schaum’s Outlines version. Topics include:\nAllocating capital per se is addressed in Section 8.3.\nThe theory presented here revolves around a one-period model. In reality, insurance claims often evolve over time with multiple payments spanning more than one accounting period. The output of the Business Operations module will undoubtedly reflect this. In order to apply the theory, we must collapse the multiperiod view into a single-period equivalent. There are two ways to do this. One, we can represent paid claims in a unit and carry a one-year change in reserves as a separate but parallel unit. Two, we can operate entirely in ultimate claims, discounting them back to time zero. To do this, we lean on the principles espoused in (wuthrich2010market?):\nUsing risk-free rates to discount losses is somewhat controversial. Fairley (1979) argued for a risk-free rate because “policyholders do not share in [the insurer’s] investment risks…” Cummins (1990) argued for using the market return on the company’s actual portfolio because adding investment risk increases the probability of default for policyholders and therefore changes their expected loss recoveries, i.e., they do share in the investment risk. However, note that the insolvency priority order of payments puts policyholders first in line to be paid and shareholders last (i.e., first in line for losing money). Investment risk is not shared equally—if we want to follow Cummins’s line of reasoning, the correct discount rate for losses is nearer risk-free than it is the full investment yield. The risk-free rate is probably not a bad approximation.\nAn alternative theory is embedded in IFRS17 rules (Caramagno et al. 2021). There, a discount rate for future liability cash flows is the sum of a risk-free rate and a liquidity premium. Since insurance contracts cannot be traded continuously in a liquid market, investors must therefore demand a liquidity premium. Typically this is around 50 basis points. The IFRS17 rate proxies an entity independent market rate.\nFor more complex situations where cash flows depend more directly on financial outcomes (as is the case in much of life insurance), refer to (wuthrich2010market?)—but not the parts about risk loads; their approach is not the same as the SRM approach presented here!\nPricing applications of capital models typically proceed from the top down (Bühlmann 1985) and start by determining the premium needed for the whole portfolio to meet corporate return objectives. Total needed return depends on the amount of capital and the required return on that capital. Following the market structure laid out for InsCo (Figure 1.4), the amount is determined by regulators or rating agencies using a capital risk measure. The return is estimated using a separate analysis which will be used to calibrate the pricing risk measure.\nA stock insurer uses a peer analysis of valuation against relevant financial metrics to estimate what it believes its investors require as a return on equity to support the current or desired share price. This return is combined with the capital structure (known debt costs) to determine a weighted average cost of capital across debt and equity, producing an implied needed total income. Income is then adjusted for non-underwriting income and taxes to determine needed income from insurance operations. It is this amount that is handed to the capital modelers to be allocated to individual units (Section 4.2). The analysis below ignores non-underwriting income and taxes and proceeds directly from the implied weighted average cost of capital. Mutual companies substitute their own analysis, reflecting their founding charter and mission, and their lack of access to equity financing, to determine an analogous cost of capital.\nSay the analysis determines that investors require a 15% return on capital for their investment in InsCo. The portfolio’s premium, margin, capital, assets, and expected losses have a simple relationship with rate of return. The time value of money is ignored, i.e., the risk-free rate is zero. Here we analyze only the gross losses, leaving the analysis of reinsurance for later.\nDefine the expected loss \\(L=\\mathsf E[X\\wedge a]\\). \\(L\\) is slightly less than the promised loss \\(\\mathsf E[X]\\) because of the possibility of default; in practice the adjustment is so small you can often ignore it. Premium can be decomposed into expected loss plus margin: \\[\nP=L+M.\n\\tag{4.1}\\] Equation 1.1, the funding equation, says \\(a=P+Q\\) giving the split of assets between policyholder premium and investor capital. Policyholders in aggregate have been promised \\(X\\) and investors the balance of funds \\((a-X)^+\\). If \\(X&gt;a\\) policyholders are paid \\(a\\) and investors nothing. No matter the loss outcome, all ending assets are paid in losses or returned to investors: \\[\na \\equiv X\\wedge a + (a-X)^+.\n\\] Investors expect to receive \\[\n\\mathsf E[(a - X)^+] = a - L = (P + Q) - L = (L+M) + Q - L = M + Q\n\\] and therefore their expected return on investment is \\(\\iota=M/Q\\).\nIn Chapter 10.5 of Mildenhall and Major (2022), we introduce the insurance pentagon, Figure 4.1, which displays key insurance pricing variables and the relationships between them. There are five monetary variables: premium, loss, margin, assets, and capital, and three ratios describing leverage, loss ratio, and return.\nTable 4.1 displays a number of relationships between the eight key insurance variables which we use repeatedly: you should become familiar with them. The table uses Greek letters for the ratios and Roman for monetary amounts.\nTable 4.1: The eight pentagon variables and relationships between them and with other insurance statistics.\n\n\n\n\n\n\n\n\n\nRef\nVariable or relationship\nInterpretation\n\n\n\n\n\nMonetary amounts\n\n\n\n1\n\\(L\\)\nExpected loss\n\n\n2\n\\(P\\)\nPremium\n\n\n3\n\\(M\\)\nMargin\n\n\n4\n\\(a\\)\nTotal assets\n\n\n5\n\\(Q\\)\nCapital\n\n\n\n\n\n\n\n\nRelated amounts\n\n\n\n\n\\(a-L\\)\nThe unfunded liability above expected loss, funded by margin and capital\n\n\n\n\n\n\n\n\nMonetary identities\n\n\n\nPrem\n\\(P=L+M\\)\nPremium is expected loss plus margin\n\n\nFund\n\\(a = P + Q\\)\nFunding equation: premium and capital only source of \\(t=0\\) assets\n\n\n\n\n\n\n\n\nRatios\n\n\n\n6\n\\(\\lambda = L/P\\)\nLoss ratio\n\n\n7\n\\(\\gamma = P/a\\)\nPremium to asset leverage (gamma=g for leveraGe)\n\n\n8\n\\(\\iota = M/Q\\)\nExpected return on capital (investor) or cost of capital (insured)\n\n\n\n\n\n\n\n\nRelated ratios\n\n\n\n\n\\(\\nu = 1/(1+\\iota)\\)\nRisk discount factor, analog of \\(v=1/(1+i)\\)\n\n\n\n\\(\\delta = \\iota /(1+\\iota)\\)\nRisk discount rate, analog of paying interest at \\(t=0\\)\n\n\n7a\n\\(P/Q= \\gamma/(1-\\gamma)\\)\nPremium to capital leverage ratio, divide top/btm by \\(a\\), used Fund\n\n\n\n\n\n\n\n\nRatio identities\n\n\n\n\n\\(\\delta = \\iota \\nu\\)\nAnalog of \\(d=iv\\) in theory of interest\n\n\nDisc\n\\(1 = \\nu + \\delta\\)\nAnalog to \\(1=v+d\\)\n\n\n\n\n\n\n\n\nCapital identities\n\n\n\nQ\n\\(Q=\\nu(a-L)\\)\n\\(Q\\) is \\(t=0\\) price paid for asset with expected \\(t=1\\) return \\(a-L\\)\n\n\n8a\n\\(\\iota = (P-L) / (a-P)\\)\nSubstitute Prem and Fund into (8)\n\n\n\n\\(\\delta = (P-L) / (a-L)\\)\nSubstitute Prem and Fund into (8)\n\n\n\n\n\n\n\n\nPremium identities\n\n\n\nP1\n\\(P=\\nu L + \\delta a\\)\nPremium \\(=\\) weighted average of expected loss and maximum loss, \\(a\\)\n\n\nP2\n\\(P=a - \\nu(a-L)\\)\nPremium \\(=\\) assets not funded by investor capital\n\n\nP3\n\\(P=L + \\delta (a-L)\\)\nPremium \\(=\\) expected loss plus share of unfunded liability\n\n\nP3a\n\\(P=L + \\iota Q\\)\nAs P3 but expressed in terms of cost of capital\nPremium (like discount) is received at \\(t=0\\), hence the relevance of \\(\\delta\\). Income (like interest) is earned and paid at \\(t=1\\) and measured with \\(\\iota\\). Discount is easier to work with as a measure of return because it is always between \\(0\\) and \\(1\\): \\(\\delta=1\\) corresponds to an infinite return.\nPricing is the process of splitting the unfunded liability \\(a-L\\) into premium margin and capital parts funded by the policyholder and investor.  The risk discount factors tell you how to accomplish this.\nThe relationship between premium and loss can be expressed using the loss ratio or its reciprocal, the premium multiplier. The latter is sometimes used in catastrophe pricing, when loss ratios seem embarrassingly low. Likewise, leverage could be expressed as premium to capital rather than as premium to assets. The point is there is a measure of leverage, a measure of margin to volume, and a measure of return-cost to capital.\nGiven required expected return from a top-down approach analysis, the standard pentagon equations in Table 4.1 show that knowing \\(\\iota\\), \\(L\\), and \\(a\\) determines total premium. It can be expressed in multiple ways, the most helpful of which are \\[\nP =  L + \\delta (a-L) = L + \\iota Q = \\nu L +\\delta a.\n\\tag{4.2}\\] The first two expressions make it clear that premium equals expected loss plus cost of capital. The third will reappear again and again in this monograph.\nExample: TVaR Capital Risk Measure. Before considering allocation, let’s look at total pricing with a TVaR capital measure, \\(a=\\mathsf{TVaR}_p(X)\\). The quantity \\(a-L=\\mathsf{TVaR}_p(X)-\\mathsf E[X]\\) is usually referred to as \\(\\mathsf{XTVaR}_p(X)\\), for excess TVaR above the mean. Total premium, from Equation 4.2 becomes \\[\n\\begin{aligned}\nP\n&= L + \\delta (a-L)  \\\\\n%&= L + \\delta (\\mathsf{TVaR}_p(X) -L)  \\\\\n&= L + \\delta \\mathsf{XTVaR}_p(X)  \\\\\n&= L + \\iota Q.\n\\end{aligned}\n\\]\nExample: InsCo. Recall from Table 3.2 that the portfolio plan premium is 52.2. When \\(\\iota=0.15\\), we have \\(\\nu=0.86956\\) and \\(\\delta=0.13043\\), resulting in a required portfolio premium of \\(46.6+0.13043\\cdot(100-46.6)=53.565\\). The premium required to achieve a 15% return is 53.6, or 1.4 higher than plan. The required margin is 7.0, also 1.4 higher than the plan margin. This difference between plan and required profitability (negative here) is sometimes called Economic Value Added or EVA. It represents the amount of profit over and above what is required to satisfy investors. In Section 5.5 we see if we can improve the situation. In the rest of this chapter, we address how to allocate the required portfolio premium and margin to the units.\nThe pentagon contains eight variables and five relationships (funding, premium, loss ratio, leverage, and return) leaving three free variables, i.e., most of the time knowing three of the eight variables is enough to determine the rest. This sometimes fails, e.g., knowing the three ratios does not determine volume.\nYou should become familiar with the underlying algebra of Table 4.1, not memorize it, but be able to figure it out quickly. Here are some examples.\nMake sure you are happy with P3 and P3a. These are used repeatedly below.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-Pricing",
    "href": "040_PricingAndAllocation.html#sec-Pricing",
    "title": "4  Pricing and allocation",
    "section": "",
    "text": "Figure 4.1: The insurance pentagon.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(\\nu+\\delta=1\\) and starting from \\(P=\\nu L +\\delta a\\) gives \\(P=(1-\\delta)L + \\delta a = L + \\delta (a-L)\\). Since we know \\(P=L+M\\), this implies \\(M=\\delta (a-L)\\).\nAlternatively, \\(P=\\nu L  + (1-\\nu) a = a - \\nu(a-L)\\). Now, since we know \\(a=P+Q\\) so \\(P = a-Q\\), this implies \\(Q=\\nu (a-L)\\).\nHence return \\(\\iota = M / Q = \\delta / \\nu\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-Allocation",
    "href": "040_PricingAndAllocation.html#sec-Allocation",
    "title": "4  Pricing and allocation",
    "section": "4.2 Allocation to units: Overview",
    "text": "4.2 Allocation to units: Overview\nIntroducing the Natural Allocation. The Natural Allocation (NA) is a way to allocate total premium or margin down to individual units. The method is natural because it relies on so few assumptions, is easy to compute, and gives a reasonable range of interpretable answers. It also agrees with the marginal and risk-adjusted probability methods when they are well defined—a huge conceptual advantage. The NA works by taking the problem of pricing the whole risk and dividing it into a series of simpler problems involving pricing individual layers of assets and then solving those simpler problems. Layers are easier to price because they are binary, involving no loss or a full loss, and are completely characterized by their probability of a loss. The NA is described in the next six sections.\n\nSection 4.3 describes four adjustments that must be made to raw simulated output before starting. These are technical, computational adjustments.\nSection 4.4 describes how the method works in total. It concludes by showing that the NA depends on the selection of a distortion function to determine the return to each layer of assets.\nSection 4.5 explains how to move from a total price for each layer to a price by layer by unit. Summing these allocations up across layers gives the unit NA.\nTo this point, we have not introduced any distortion functions, so Section 4.6 introduces five families. These essentially span the full range of risk appetites. We compute the NA for the Wang distortion, which corresponds to a middle-of-the-road risk appetite.\nSection 4.7 describes how the NA works for the constant cost of capital distortion (CCoC) , which assigns the same return to each layer. The CCoC corresponds to a very extreme tail-risk averse risk appetite. Unfortunately, while the CCoC is an unrealistic assumption, it is the basis for what we call the industry standard approach. It is the root cause of many problems encountered in practical allocation work. We explain why and how it came to be accepted.\nFinally, Section 4.8 tries to head off objections to the NA caused by interpreting scenarios defined by outcome loss amount as events. If this idea has not occurred to you before already, we recommend skipping it! However, someone invariably brings it up, and you need to be able to explain why it is not a problem, so you do need to read it eventually.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-Complications",
    "href": "040_PricingAndAllocation.html#sec-Complications",
    "title": "4  Pricing and allocation",
    "section": "4.3 Four technical adjustments",
    "text": "4.3 Four technical adjustments\nIn order to make the algebra work out in subsequent sections, we need to adjust our events so that four simplifying conditions hold. These alter the form of the simulation data, but not the underlying meaning. We adjust our simulated loss outcomes so that\n\nLoss outcomes are sorted by total loss, from lowest to highest.\nThe first loss is zero.\nLosses are capped at the available assets.\nPortfolio losses are unique by event.\n\nZero Loss. If, in our table of loss probabilities, the first, lowest, portfolio loss \\(X_1\\) is greater than zero (and this is true in Table 2.1), then it is computationally simpler to introduce another row with \\(X_0=0\\) and \\(p_0=0\\). This does not change the results because \\(p_0=0\\). If \\(X_1=0\\) already, then no such adjustment is necessary. However, in that case, we do renumber so that the first event is called \\(k=0\\). The reason for this will become apparent in the next section; layer size and exceedance probability must be properly accounted for.\nCapped Losses. When there is zero probability of insolvency, allocating the portfolio’s expected loss \\(\\mathsf E[X]\\) to unit \\(i\\) is easy—it’s just the unit’s expected loss \\(\\mathsf E[X^i]\\). When assets are lower and there is a nonzero probability of default \\(X&gt;a\\), we need to consider what actually happens to claim payouts. The most common rule, and the one we assume, is equal priority. Payments to unit \\(i\\) are given a common haircut: \\[\n\\begin{aligned}\nX^i(a) :&= X^i \\frac{X\\wedge a}{X} \\\\\n%        &= \\frac{X^i}{X} \\, (X\\wedge a)\\\\\n&= \\begin{cases}\n      X^i  & X \\le a \\\\\n      X^i\\dfrac{a}{X} & X &gt; a.\n  \\end{cases}\n\\end{aligned}\n\\] This expresses payments to unit \\(i\\) as a constant pro rata (haircut) proportion \\((X\\wedge a)/X\\) of the contractually promised payment \\(X^i\\). The factor is the same for all units, hence equal priority. We assume the appropriate substitutions have been made \\[\n\\begin{aligned}\nX^i &\\leftarrow X^i(a)   \\\\\nX   &\\leftarrow X \\wedge a\n\\end{aligned}\n\\] in our loss table. It is important to realize that once this adjustment has been made we can ignore default. Default has not disappeared; by replacing promised loss payments \\(X^i\\) with actual \\(X^i(a)\\), we have “baked in” default. Going forward we can therefore ignore default as a separate phenomenon.\nUnique Losses. Consider a particular outcome \\(X=X_k\\). If there is only one event with \\(X=X_k\\), then there is only one set of unit losses \\(\\{X_k^1,\\dots,X_k^m\\}\\) whose sum is \\(X_k\\). If there are multiple events with \\(X=X_k\\) but the same set of \\(\\{X_k^i\\}\\) then we can, with no loss of information, collapse them all (summing the probabilities) and consider the result to be one event.\nHowever, if there are two or more distinctly different events resulting in \\(X=X_k\\), then the above logic cannot be applied. This is true in Table 2.1. In this case marginal calculations do not make sense and, when, in Section 4.4, we go to distort probabilities, there will be an ambiguity in the ordering of the events which will be material to the calculations. We need to resolve that ambiguity, and we can do it by using the conditional expected loss of the units. Define \\[\n\\kappa^i(x) := \\mathsf E[X^i \\mid X=x].\n\\] Then collapse the events sharing \\(X=X_k\\), replacing the particular \\(X^i\\) values with their conditional expectation \\(\\kappa^i(X_k)\\) and summing the probabilities into the newly defined \\(p_k\\). We appeal to a symmetry argument: within the set of events resulting in \\(X=X_k\\), beyond their relative probabilities, there is no reason to prioritize one over another. Henceforth, we will assume the substitutions \\[\nX_k^i \\leftarrow \\kappa^i(X_k)\n\\tag{4.3}\\] have been made in our table of outcomes. Notice that Equation 4.3 always applies when losses exceed assets. The substitution Equation 4.3 relies on equal priority in the sense that the recovery to the collapsed event equals the conditional expectation of the recoveries to its components: \\[\n\\begin{aligned}\n\\mathsf E\\left[X^i \\frac{X\\wedge a}{X} \\mid X=x\\right]\n&= \\mathsf E\\left[X^i  \\mid X=x\\right]\\frac{X\\wedge a}{X} \\\\\n&= \\kappa^i(X) \\frac{X\\wedge a}{X}.\n\\end{aligned}\n\\] Thus, equal priority is essential to make the approach work. Luckily, equal priority is universal in insurance regulation, even though other approaches are possible in theory (Mildenhall and Major (2022), chapter 12.3).\nIn our example, notice in Table 2.1 that the events \\(4\\), \\(5\\), \\(6\\), and \\(7\\) all have \\(X=40\\). Taking the \\(p\\)-weighted averages of each unit’s losses across those events, collapsing the events down to one event, renumbering, and inserting the zero row, we get Table 4.2. The table has also added a row \\(0\\) with \\(0\\) loss and probability. There is no need for a default adjustment because \\(a=100\\), sufficient to pay the maximum loss.\n\n\n\n\nTable 4.2: InsCo loss probability distribution by unit with duplicate \\(X\\) averaged out and zero row inserted. Index now called \\(k\\) rather than \\(j\\).\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\nUnit A \\(X^1_k\\)\nUnit B \\(X^2_k\\)\nUnit C \\(X^3_k\\)\nPortfolio \\(X_k\\)\n\n\n\n\n0\n0\n0\n0\n0\n0\n\n\n1\n0.1\n15\n7\n0\n22\n\n\n2\n0.1\n15\n13\n0\n28\n\n\n3\n0.1\n5\n20\n11\n36\n\n\n4\n0.4\n10\n24\n6\n40\n\n\n5\n0.1\n26\n19\n10\n55\n\n\n6\n0.1\n17\n8\n40\n65\n\n\n7\n0.1\n16\n20\n64\n100\n\n\n\\(\\mathsf E[X]\\)\n\n13.4\n18.3\n14.9\n46.6\n\n\nPlan Prem\n\n13.9\n18.7\n19.6\n52.2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-Pricing-layers",
    "href": "040_PricingAndAllocation.html#sec-Pricing-layers",
    "title": "4  Pricing and allocation",
    "section": "4.4 Pricing layers in total",
    "text": "4.4 Pricing layers in total\nLayer pricing assumes the required return associated with each individual dollar of assets is given by a function of its risk of being consumed for claim payments.\nThe NA is based on the principle of layer pricing, introduced formally by Wang (1996) and articulated by Venter (1991), Bodoff (2007), and others.\nSuppose the total assets \\(a\\) are funded by premium and the sale of the residual value of separate layers or tranches. These layers are analogous to collateralized aggregate reinsurance, as we shall now explain. Reinsurance in our one-period model is a transaction where ceded premium is paid to the reinsurer at \\(t=0\\) and ceded losses are recovered from the reinsurer at \\(t=1\\). Let’s turn that around to make the role of reinsurance as capital more transparent. InsCo (or its regulator) demands collateralized reinsurance. Consider a layer of width \\(\\Delta X_k\\). The layer has its own funding equation and needs to be supported by assets of \\(\\Delta X_k\\) in total. At \\(t=0\\) the counterparty remits \\(\\nu (\\Delta X_k-L_k)\\) and the insurer \\(L_k+\\delta(\\Delta X_k-L_k)\\) into the collateral account, where \\(L_k\\) is expected loss in the layer. Since \\(\\nu+\\delta=1\\), the layer is fully collateralized. At \\(t=1\\) the layer loss becomes known and is paid in full from collateral. Any amounts remaining are remitted back to the counterparty. This is reinsurance, but with the transactions reversed to reveal the capital financing function.\nTo be concrete, say we understand the distribution of portfolio losses \\(X\\) through a simulation. Say the annual aggregate results \\[\nX \\in \\{X_0 &lt; X_1 &lt; \\dots &lt; X_n=a\\}\n\\] and each outcome has its probability \\(\\{p_0, p_1, \\dots, p_n\\}\\).\nWe tranche total assets \\(a\\) into a stack of fully collateralized aggregate reinsurance layers. Each layer is a half-open interval, \\(k=0,\\dots,n-1\\), covering losses \\(X \\in (X_k,X_{k+1}]\\), i.e., for a loss of \\(X\\) it pays \\[\n\\begin{cases}\n0           & X \\le X_k \\\\\nX - X_k     & X_k &lt; X \\le X_{k+1} \\\\\n\\Delta X_k  & X_{k+1} &lt; X\n\\end{cases}.\n\\] However, there are no partial payments because \\(X_k\\) are the only possible outcomes, so the cover reduces to one paying nothing when \\(X \\le X_k\\) and \\(\\Delta X_k\\) as soon as \\(X &gt; X_k\\).\n\nFor our InsCo numerical example, \\(X \\in \\{0, 22, 28, 36, 40, 55, 65, 100\\}\\) and \\(n=7\\). So, we have seven layers \\((X_k, X_{k+1}]\\) that we imagine are funded separately, each by a combination of investor capital (security market purchase price of the returned collateral) and insured’s premiums. The probability that losses exactly equal \\(X_k\\) is \\(p_k\\). The probability that layer \\(k\\) is used for loss payments (rather than handed back to investors) is the exceedance probability or survival function (hence notation \\(S\\)): \\[\nS_k = S(X_k) := \\Pr\\{X&gt;X_k\\} = \\sum_{j=k+1}^n p_j.\n\\] The numbers for our example are laid out in Table 4.3.\n\n\n\n\nTable 4.3: InsCo layer loss details and survival probabilities.\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\nUnit A \\(X^1_k\\)\nUnit B \\(X^2_k\\)\nUnit C \\(X^3_k\\)\nPortfolio \\(X_k\\)\nLayer size \\(\\Delta X_k\\)\nExceedance Pr \\(S_k\\)\n\n\n\n\n0\n0\n0\n0\n0\n0\n22\n1\n\n\n1\n0.1\n15\n7\n0\n22\n6\n0.9\n\n\n2\n0.1\n15\n13\n0\n28\n8\n0.8\n\n\n3\n0.1\n5\n20\n11\n36\n4\n0.7\n\n\n4\n0.4\n10\n24\n6\n40\n15\n0.3\n\n\n5\n0.1\n26\n19\n10\n55\n10\n0.2\n\n\n6\n0.1\n17\n8\n40\n65\n35\n0.1\n\n\n7\n0.1\n16\n20\n64\n100\n0\n0\n\n\n\n\n\n\n\n\n\nHow is layer \\(k\\) to be funded? Analogous to our portfolio funding Equation 1.1, we have the layer funding equation: \\[\n\\Delta X_k = P_k+Q_k.\n\\tag{4.4}\\]\nWhile the portfolio funding equation is self-evident, is it necessary to constrain a layer’s capital to \\(Q_k=\\Delta X_k-P_k\\), or can it be something else? It is easy to see it cannot provided \\(P = \\sum_k P_k\\). Let’s assume that (we’ll justify it later) and see why. First, note that \\(Q_k\\ge \\Delta X_k - P_k\\) for each \\(k\\), otherwise there could be a shortfall in loss payments on the layer. This is just the statement that each layer must be fully collateralized. Now, if the inequality is strict for any \\(k\\), we get \\(Q=\\sum_k Q_k &gt; \\sum_k \\Delta X_k - P_k = a - P = Q\\), which is a contradiction. Hence \\(Q_k = \\Delta X_k - P_k\\).\nWe still need to justify \\(P = \\sum_k P_k\\). You might think that the aggregate premium could be less than the sum of the premiums by layer because of potential diversification opportunities between the layers. However, in a sense there are no diversification opportunities between the layers because they are comonotonic: they are all increasing functions of the total loss \\(X\\). Some, but not all, pricing functionals are comonotonic additive, meaning that the price of the sum of comonotonic risks equals the sum of the prices. To proceed, we need to use a comonotonic additive pricing functional. This is not a problem, as the functionals we are proposing to use are comonotonic additive. Thus, we may assume the layer funding Equation 4.4 holds.\nWe have reduced the pricing problem to pricing a fully collateralized layer. Here again we can apply our general rules: if we know the expected loss and return by layer we know the premium. \\(S_k\\) is the probability that layer \\(k\\) is needed to pay claims and so \\(L_k=S_k\\Delta X_k\\) is the expected value of layer \\(k\\)’s contribution to the total portfolio expected claim payments \\(L\\). For the return, we need to made one last assumption: \\(S_k\\) is the only thing that investors deem relevant to pricing layer \\(k\\). In layer \\(k\\), the investors get \\(\\Delta X_k\\) back with probability \\(1-S_k\\) and nothing with probability \\(S_k\\). Layer \\(k\\)’s payoff as a fraction of its limit \\(\\Delta X_k\\) is a Bernoulli random variable with one parameter: \\(1-S_k\\). We assume that its probability distribution, determined by \\(S_k\\), is all that matters. This restriction is known as law invariance.\nA risk measure \\(\\rho(X)\\) is law invariant if and only if it is a function of the distribution \\(F\\) of \\(X\\). That is, if \\(X\\) and \\(Y\\) have the same distribution function, then \\(\\rho(X)=\\rho(Y)\\). Law invariant risk measures do not depend on the cause of loss or the particulars of the event generating the loss. VaR, TVaR, and standard deviation are examples of law invariant risk measures. A measure like \\(\\rho(X)=\\mathsf E[X] + \\lambda \\mathsf{cov}(X, Y)\\), which depends on a state variable \\(Y\\), is not.\nLaw invariance may seem a strange assumption for a pricing risk measure, especially in the light of theories like the capital asset pricing model where systematic or nondiversifiable risk is considered (these are not law invariant). The rationale is that insurance risk is diversifiable and has no analog of market risk. The motivation for law invariance in the capital risk measure stems from a regulator’s desire for an objective default probability measure. An entity’s risk of insolvency depends only on the distribution of its future change in surplus—the cause of loss is irrelevant. Law invariance enables risk to be estimated statistically, from an empirical sample or a model fit to \\(X\\). Law invariance is sometimes called objectivity for these reasons. It allows regulators to calibrate their models using only losses. While the justification for law invariance in the pricing measure is admittedly weaker, we find it eminently useful.\nSuppose we know (or, being actuaries, assume) a function \\(g(s)\\) that maps attachment probability \\(S_k\\) to rate on line (premium to limit ratio) allowing us to write \\[\nP_k = g(S_k)\\Delta X_k.\n\\] The function \\(g(s)\\) is known as a distortion function.\nBefore moving on, here is a summary of the key assumptions that we have made and how they are used:\n\nEqual priority in default – collapse equal outcomes to one.\nComonotonic additive pricing risk measure – funding equation by layer.\nLaw invariant pricing risk measure – price layer using a distortion function.\nExistence of a rate on line distortion function \\(g\\) for pricing Bernoulli layers.\n\nDistortion functions must have certain properties if the layer prices are to be rational. In particular, \\(g(0)=0\\), \\(g(1)=1\\), and \\(g\\) must be increasing and concave (decreasing slope). Violations of these lead to irrational pricing behavior, as defined and explained in Mildenhall and Major (2022), chapter 10.\nThe logic for pricing layer \\(k\\) is exactly the same as what was used to derive Equation 4.2 and gives: \\[\nP_k = g(S)\\Delta X_k = \\nu_k S_k \\Delta X_k +\\delta_k \\Delta X_k\n\\tag{4.5}\\] where the return \\(\\iota_k\\) and risk discount factors \\(\\nu_k\\) and \\(\\delta_k\\), are functions of \\(S\\) (hence the subscripts). So, if we knew the required return \\(\\iota_k\\) associated with layer \\(k\\), we could determine its premium and capital. Summing across all layers we would then know both total premium and total capital, and they would sum to total assets \\(a\\). Computing the total premium \\[\nP = \\sum_j g(S_j)\\Delta X_j\n\\] in this way is known as a Spectral Risk Measure (SRM). SRMs are coherent risk measures in the sense of Artzner et al. (1999). They are also law invariant (of course) and comonotonic additive, and in fact are the only risk measures that have all these properties.\n\nLet’s look more closely at Equation 4.5. From our setup, the largest possible loss and \\(\\sum \\Delta X_k=a\\). Using that, recall the two ways of computing the mean (integration by parts): \\[\n\\mathsf E[X]=\\sum_{k=0}^{n}  X_k p_k = \\sum_{k=0}^{n-1} \\Delta X_k S_k.\n\\] The right-hand term is a sum over expected losses in each layer \\(k\\). This formula is illustrated in Figure 4.2 by the blue area and explained in the context of the Lee diagram in chapter 3 of Mildenhall and Major (2022). Now, if each layer of capital costs \\(g(S_j)\\), we can use integration by parts to get \\[\n\\begin{aligned}\nP\n&=  \\sum_{k=0}^{n-1} g(S_k) \\Delta X_k \\\\\n&= \\sum_{k=0}^{n-1} g(S(X_k))(X_{k+1} - X_k) \\\\\n&= \\sum_{k=0}^{n} (g(S_{k-1}) - g(S_{k})))X_k \\\\\n&= \\sum_{k=0}^{n} q_k X_k\n\\end{aligned}\n\\tag{4.6}\\] where \\(g(S_{-1}):=1\\). Refer to Figure 4.2. The size of each vertical step is \\(\\Delta X_k\\), and the left-to-right distance from each blue (respectively, orange) vertical step to the \\(p=1\\) axis is \\(S_k\\) (respectively, \\(g(S_k)\\)). The area under the blue line is the expected loss. The area under the orange line is the premium, corresponding to either expression in Equation 4.6. The difference is the margin, shown in orange shading in the second panel. The complement of premium is capital, above the orange line, shaded in light blue. This view of the relationship between capital, margin, and loss is dramatically different from the conventional view in the third panel.\nIn Equation 4.6, the last two formulas can be interpreted as the expected value outcomes \\(X_k\\) with respect to adjusted or distorted probabilities \\(q_k\\), analogous to \\(p_k = S_{k-1} - S_k\\). (This symbol \\(q\\) for a probability measure is not to be confused with capital amount \\(Q\\) symbol.) Notice that \\(\\sum_{j=0}^{n} q_j = 1\\). Since all \\(q_j\\) are nonnegative, collectively they satisfy the properties of a probability distribution.\n\n\n\n\n\n\n\n\nFigure 4.2: Computing loss, margin, and premium for the simple example, CCoC distortion.\n\n\n\n\n\nExercise. For \\(\\mathsf E[X]\\), prove that \\(\\sum_j S_j \\Delta X_j = \\sum_j p_j X_j\\).\nSolution. \\[\n\\begin{aligned}\n\\sum_{j=0}^n p_j X_j &= \\sum_{j=1}^n (S_{j-1}-S_j) X_j \\\\\n&= \\sum_{j=0}^{n-1} S_j X_{j+1} - \\sum_{j=1}^n S_j X_j \\\\\n&= \\sum_{j=0}^{n-1} S_j \\Delta X_{j}\n\\end{aligned}\n\\] because \\(S_0 X_0 = S_n X_n = 0\\). \nExercise. For \\(\\rho(X)\\), prove that \\(\\sum_j g(S_j) \\Delta X_j = \\sum_j q_j X_j\\).\nSolution. This is essentially the same derivation as for \\(\\mathsf E[X]\\) but with \\(g(S)\\) substituted for \\(S\\) and \\(q\\) substituted for \\(p\\). \nWe can go one step further and write \\[\nP = \\sum_{k=0}^{n} q_kX_k = \\sum_{k=0}^{n} X_k Z_k p_k.\n\\tag{4.7}\\] The likelihood ratios \\(Z_j=q_j/p_j\\) individually can be less than, equal to, or greater than one, but their expected value equals one.\nRecall that for our example \\(\\iota=0.15\\), \\(\\nu=0.86956\\), and \\(\\delta=0.13043\\). Table 4.4 tabulates the portfolio expected loss and premium based on the CCoC distortion which we define in Section 4.6. We can see, comparing to Table 4.11, that the same portfolio premium is obtained.\n\n\n\n\nTable 4.4: Pricing the portfolio with CCoC distorted \\(g(S)\\).\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\nPortfolio \\(X_k\\)\nLayer size \\(\\Delta X_k\\)\nExceedance Pr \\(S_k\\)\nDistorted \\(g(S)\\)\n\\(S\\Delta X\\)\n\\(g(S)\\Delta X\\)\n\n\n\n\n0\n0\n0\n22\n1\n1\n22\n22\n\n\n1\n0.1\n22\n6\n0.9\n0.913\n5.4\n5.478\n\n\n2\n0.1\n28\n8\n0.8\n0.826\n6.4\n6.609\n\n\n3\n0.1\n36\n4\n0.7\n0.739\n2.800\n2.957\n\n\n4\n0.4\n40\n15\n0.3\n0.391\n4.500\n5.870\n\n\n5\n0.1\n55\n10\n0.2\n0.304\n2\n3.043\n\n\n6\n0.1\n65\n35\n0.1\n0.217\n3.5\n7.609\n\n\n7\n0.1\n100\n0\n0\n0\n0\n0\n\n\nSum\n1\n\n100\n\n\n46.6\n53.565",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-pricing-units",
    "href": "040_PricingAndAllocation.html#sec-pricing-units",
    "title": "4  Pricing and allocation",
    "section": "4.5 Pricing units via pricing layers",
    "text": "4.5 Pricing units via pricing layers\nA portfolio’s premium, computed by an SRM, has an NA to units.\nWe are now in a position to consider the pricing of portfolio units. Let the portfolio aggregate loss be the sum of \\(m\\) unit losses: \\[\nX = \\sum_{i=1}^m X^i.\n\\] The various \\(X^i\\) may or may not be correlated. We assume they are nonnegative.\nIt is natural to look at Equation 4.6 and suppose that the price of \\(X^i\\) should be \\[\nP^i = \\sum_{j=0}^a q_j X_j^i.\n\\tag{4.8}\\] Note the resemblance of this to the expected value of \\(X^i\\): \\[\nL^i = \\sum_{j=0}^a p_j X_j^i.\n\\tag{4.9}\\] Finally, of course, unit margin (analogous to Equation 4.1) is simply the difference: \\[\nM^i = P^i-L^i.\n\\tag{4.10}\\]\nThis decomposition of \\(P\\) into \\(\\sum_i P^i\\) is known as the NA of an SRM—so-called because it relies on no additional assumptions. It is the distorted expectation of \\(X^i\\). Other ways of writing it are \\[\nP^i =\\ E[Z\\cdot X^i] = \\mathsf E[X^i]\\mathsf E[Z] + \\mathsf{cov}(Z,X^i) = \\mathsf E[X^i] + \\mathsf{cov}(Z,X^i)\n\\tag{4.11}\\] because \\(\\mathsf E[Z]=1\\). The first expression resembles a pricing kernel in modern finance theory or a co-measure (Venter et al. 2006). The last expression resembles the Capital Asset Pricing Model.\nExercise. Verify Equation 4.11.\nSolution. \\(\\mathsf{cov}(Z,X)=\\mathsf E[ZX] - \\mathsf E[Z]\\mathsf E[X]=\\mathsf E[ZX] -\\mathsf E[X]\\) because \\(\\mathsf E[Z]=1\\). \nWhy do we require unique outcome values for \\(X\\)? To enforce a unique ordering of outcomes. The values \\(q_k\\) depend on the order of events. If there are multiple \\(k\\) with equal \\(X_k\\), that ambiguity doesn’t matter for computing total premium \\(P\\). But when we consider allocations to units it does, because it allows some unwanted flexibility in \\(q_k\\). When the outcomes of \\(X\\) are distinct we are also guaranteed that the NA equals the marginal cost of increasing business in unit \\(i\\). Since the outcomes are distinct, the order of outcomes of \\(X\\) and of \\(X+\\epsilon X^i\\) is the same provided \\(\\epsilon\\) is small enough (positive or negative). As noted above, \\(q\\) reproduces stand-alone pricing for all risks comonotonic with (same order as) \\(X\\). Thus \\[\n\\begin{aligned}\n\\nabla^i  \\rho_X\n&= \\lim_{\\epsilon\\to 0}\\frac{\\rho(X + \\epsilon X^i) - \\rho(X)}{\\epsilon} \\\\\n&= \\lim_{\\epsilon\\to 0}\\frac{\\mathsf E_q[X + \\epsilon X^i] - \\mathsf E_q[X]}{\\epsilon} \\\\\n&= \\lim_{\\epsilon\\to 0}\\frac{\\mathsf E_q[X] + \\mathsf E_q[\\epsilon X^i] - \\mathsf E_q[X]}{\\epsilon} \\\\\n&= \\lim_{\\epsilon\\to 0}\\frac{\\epsilon \\mathsf E_q[X^i]}{\\epsilon} \\\\\n&= \\mathsf E_q[X^i].\n\\end{aligned}\n\\] This very important result was proved by Delbaen around 2000. It shows that the NA is the natural extension of Tasche (1999).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-5-distortions",
    "href": "040_PricingAndAllocation.html#sec-5-distortions",
    "title": "4  Pricing and allocation",
    "section": "4.6 Five representative distortion functions",
    "text": "4.6 Five representative distortion functions\nWe now introduce five carefully selected families of distortion functions: constant cost of capital (CCoC), proportional hazards (PH), Wang transform, dual moment transform, and TVaR. Table 4.5 shows the formulas for each and parameters selected to achieve InsCo’s portfolio pricing consistent with an expected return of \\(\\iota=0.15\\). For the Wang distortion, introduced in Wang (2000), \\(\\Phi\\) is the standard Gaussian cumulative distribution function. For the PH (respectively, other four) a lower (respectively, higher) parameter indicates a more risk averse distortion resulting in higher prices for a given risk.\n\n\n\n\nTable 4.5: Distortion functions calibrated to portfolio price \\(\\rho(X)=53.565\\).\n\n\n\n\n\n\n\n\n\nDistortion\nFormula\nParameter\n\n\n\n\nCCoC\n\\(\\nu s+\\delta\\)\n\\(\\iota=0.1500\\)\n\n\nPH\n\\(s^\\alpha\\)\n\\(\\alpha=0.3427\\)\n\n\nWang\n\\(\\Phi(\\Phi^{-1}(s)+\\lambda)\\)\n\\(\\lambda=0.7205\\)\n\n\nDual\n\\(1-(1-s)^m\\)\n\\(m=1.5951\\)\n\n\nTVaR\n\\(1\\wedge s/(1-p)\\)\n\\(p=0.2713\\)\n\n\n\n\n\n\n\n\n\nFigure 4.3 plots the five distortion functions and distorted probabilities. The blue star above \\(s=0\\) on the right reminds us of the probability mass for the CCoC . The thin black line shows the reference \\(Z=1\\) line, where risk-adjusted and objective probabilities are equal. The plots explain the order of the distortions: CCoC gives the greatest weight to the worst \\(s=0\\) loss (remember the \\(*\\) above the worst event) and TVaR the least. Conversely, TVaR gives the greatest weight to moderate losses between the 25th and 75th percentiles. Table 4.6 shows the \\(g(s)\\) values corresponding to each of the five distortions for \\(S\\) values in our example, Table 4.7 shows the distorted probabilities (\\(q\\)) associated with each distortion function, and Table 4.8 shows the \\(Z=q/p\\) factor for each distortion, showing the proportionate increase or decrease over objective probabilities \\(p\\).\n\n\n\n\n\n\nFigure 4.3: Distorted exceedance (\\(g(s)\\), left) and distorted probabilities (\\(q\\), right) by distortion type.\n\n\n\n\n\n\n\nTable 4.6: Distorted exceedance (\\(g(s)\\)) by distortion type.\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(S\\)\nCCoC\nPH\nWang\nDual\nTVaR\n\n\n\n\n0\n1\n1\n1\n1\n1\n1\n\n\n1\n0.9\n0.9130\n0.9269\n0.9478\n0.9746\n1\n\n\n2\n0.8\n0.8261\n0.8515\n0.8819\n0.9233\n1\n\n\n3\n0.7\n0.7391\n0.7734\n0.8071\n0.8535\n0.9606\n\n\n4\n0.3\n0.3913\n0.4200\n0.4279\n0.4339\n0.4117\n\n\n5\n0.2\n0.3043\n0.3136\n0.3089\n0.2995\n0.2745\n\n\n6\n0.1\n0.2174\n0.1903\n0.1739\n0.1547\n0.1372\n\n\n7\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.7: Distorted probabilities (\\(q\\)) by distortion type.\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\nCCoC\nPH\nWang\nDual\nTVaR\n\n\n\n\n1\n0.1\n0.0870\n0.0731\n0.0522\n0.0254\n0\n\n\n2\n0.1\n0.0870\n0.0754\n0.0660\n0.0513\n0\n\n\n3\n0.1\n0.0870\n0.0781\n0.0748\n0.0698\n0.0394\n\n\n4\n0.4\n0.3478\n0.3534\n0.3791\n0.4196\n0.5489\n\n\n5\n0.1\n0.0870\n0.1064\n0.1190\n0.1344\n0.1372\n\n\n6\n0.1\n0.0870\n0.1233\n0.1350\n0.1448\n0.1372\n\n\n7\n0.1\n0.2174\n0.1903\n0.1739\n0.1547\n0.1372\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.8: Likelihood ratios (\\(Z=q/p\\)) by distortion type.\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\nCCoC\nPH\nWang\nDual\nTVaR\n\n\n\n\n1\n1\n0.8696\n0.7310\n0.5216\n0.2540\n0\n\n\n2\n1\n0.8696\n0.7541\n0.6598\n0.5134\n0\n\n\n3\n1\n0.8696\n0.7810\n0.7480\n0.6979\n0.3941\n\n\n4\n1\n0.8696\n0.8834\n0.9479\n1.0490\n1.3723\n\n\n5\n1\n0.8696\n1.0640\n1.1899\n1.3439\n1.3723\n\n\n6\n1\n0.8696\n1.2329\n1.3502\n1.4479\n1.3723\n\n\n7\n1\n2.1739\n1.9033\n1.7391\n1.5470\n1.3723\n\n\n\n\n\n\n\n\n\nAll of the distortions underweight events 1 through 3. Event 4 has the median loss. TVaR, while assigning zero weight to the lowest losses, is still the most body focused because it overweights events 4 and above—equally—and among all distortions applies the smallest weight to the largest loss. CCoC, however, applies the largest weight to event 7 (the largest loss) and only that event. The other distortions apply weights to event 7 in the order of presentation in the table. The ordering for event 4, representing body focus, is the opposite of the ordering for event 7, representing tail focus.\nThe results obtained by using the Wang distortion are shown in Table 4.9.\n\n\n\n\nTable 4.9: Pricing the portfolio with Wang transformed probabilities \\(q\\).\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\nUnit A \\(X^1_k\\)\nUnit B \\(X^2_k\\)\nUnit C \\(X^3_k\\)\nPortfolio \\(X_k\\)\n\\(S\\)\n\\(g(S)\\)\n\\(q\\)\n\\(Z\\)\n\n\n\n\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n1\n0.1\n15\n7\n0\n22\n0.9\n0.948\n0.052\n0.522\n\n\n2\n0.1\n15\n13\n0\n28\n0.8\n0.882\n0.066\n0.660\n\n\n3\n0.1\n5\n20\n11\n36\n0.7\n0.807\n0.075\n0.748\n\n\n4\n0.4\n10\n24\n6\n40\n0.3\n0.428\n0.379\n0.948\n\n\n5\n0.1\n26\n19\n10\n55\n0.2\n0.309\n0.119\n1.190\n\n\n6\n0.1\n17\n8\n40\n65\n0.1\n0.174\n0.135\n1.350\n\n\n7\n0.1\n16\n20\n64\n100\n0\n0\n0.174\n1.739\n\n\n\\(L=\\mathsf E_p\\)\n\n13.4\n18.3\n14.9\n46.6\n\n\n\n\n\n\n\\(P=\\mathsf E_q\\)\n\n14.109\n18.637\n20.818\n53.565\n\n\n\n\n\n\n\\(M=P-L\\)\n\n0.709\n0.337\n5.918\n6.965\n\n\n\n\n\n\nProp of tot \\(M\\)\n\n0.102\n0.048\n0.850\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far, we have seen how to allocate premium \\(P\\) (and therefore margin \\(M\\), via Equation 4.1) to portfolio units via the NA of an SRM, once a distortion function \\(g(s)\\) is available. Chapter 8 discusses how to choose a distortion function. A premium allocation allows us to compute economic value added by unit, the actual premium in excess of required premium, and to assess static portfolio performance by unit—motivations for performing capital allocation in the first place (Chapter 5). In many ways it is also a good place to stop.\nHowever, if desired, the ideas behind the natural premium allocation can be extended to give a capital allocation consistent with SRM premium allocation. This is discussed in Section 8.3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-ISA",
    "href": "040_PricingAndAllocation.html#sec-ISA",
    "title": "4  Pricing and allocation",
    "section": "4.7 The Industry Standard Approach and its problems",
    "text": "4.7 The Industry Standard Approach and its problems\nIn this section we step back to the 1990s, before the layer approach was formally introduced, and describe how what we call the Industry Standard Approach (ISA) evolved and the problems it generates. At that time, actuaries determined prices on a stand-alone basis, because in an efficient market prices are additive. They used the formula premium equals loss cost plus margin, and margin was computed as cost of capital times amount of capital, giving (Table 4.1 P1, P3a): \\[\nP = L + \\iota Q = \\nu L + \\delta a.\n\\tag{4.12}\\] It was deemed obvious that the cost of capital should vary by unit according to different levels of systematic risk. However, attempts to compute underwriting betas from accounting data failed or had standard errors that swamped differences by unit. In consequence, it became standard to assume that \\(\\iota\\) was constant across units. This story emerges in Myers and Cohn (1987), Cummins (2000), and Kozik (1994).\nAround the same time, Tasche (1999) pointed out that marginal capital is the “only definition for the risk contributions which is suitable for performance measurement.” Tasche’s analysis relies on a constant cost of capital to equate needs more capital with needs more margin. That relationship obviously fails if capital costs vary. As a result, by 2000, the ISA was to use a CCoC and marginal capital by unit to steer portfolios. This is called RAROC, return on risk adjusted capital: the return is constant but the capital varies with risk.\nAs we saw in Section 4.4, in the mid-1990s Wang and others introduced the idea of pricing by layer. It is easy to translate Equation 4.12 into the layer framework. Without loss of generality, we can assume the four adjustments of Section 4.3, so \\(a=\\max(X)\\). Then, \\[\n\\begin{aligned}\nP\n&= L + \\iota Q  \\\\\n&= \\nu\\, L + \\delta \\, a \\\\\n&= \\nu\\, \\mathsf E[X] + \\delta \\, \\max(X) \\\\\n&=  \\nu \\sum_{k=0}^{n-1} S_k\\Delta X_k + \\delta \\sum_{k=0}^{n-1} \\Delta X_k  \\\\\n&=  \\sum_{k=0}^{n-1} (\\nu S_k + \\delta) \\Delta X_k \\\\\n&=  \\sum_{k=0}^{n-1} g(S_k) \\Delta X_k\n\\end{aligned}\n\\] is exactly the premium associated with the CCoC distortion \\(g(s)=\\nu s + \\delta\\). Here an important transformation has occurred. ISA (reasonably) assumed a CCoC by unit. The layer view shows that constant unit costs mathematically imply a CCoC by layer, an implication that went unremarked at the time. The problem is that capital costs are manifestly not constant by layer. Numerous websites such as the Federal Reserve’s FRED and Bank of America’s ICE provide the latest information on credit yield spreads. Low-risk instruments such as government or AAA-rated bonds require lower yields than non-investment-grade or junk bonds. This is true even after corrections are made to the yield calculations to take default probability into account (Heynderickx et al. 2016).\n\n\n\n\n\n\nFigure 4.4: Assuming a constant cost of capital. © S. Mildenhall, 2024.\n\n\n\nAside from the factual inconvenience of making an empirically false assumption, the CCoC distortion causes other problems. It is numerically unstable because it puts a large weight on the worst outcome and underweights all others. For the same reason, it is very averse to tail risk but blithely ignores volatility risk around the mean. None of the four other distortions in Table 4.5 has these problems: all have a cost of capital that varies by layer and distribute their probability weight more evenly across events. CCoC is further unusual because it provides no diversification benefit for independent risks.\nExercise. Show that CCoC distortion pricing is additive for independent risks.\nSolution. \\(P=\\nu \\mathsf E[X] + \\delta\\max(X)\\) and expectation and maximum are additive for independent risks. \nTable 4.10 shows the calculation of \\(q_k\\), \\(\\mathsf E[X]\\), and the price of \\(X\\) using the \\(q\\,\\)s. It gives the same result as Table 4.4.\n\n\n\n\nTable 4.10: Pricing the portfolio with CCoC distorted probabilities \\(q\\).\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\nPortfolio \\(X_k\\)\nLayer size \\(\\Delta X_k\\)\nExceedance Pr \\(S_k\\)\n\\(pX\\)\nDistorted \\(q\\)\n\\(qX\\)\n\\(Z=q/p\\)\n\n\n\n\n0\n0\n0\n22\n1\n0\n0\n0\n\n\n\n1\n0.1\n22\n6\n0.9\n2.2\n0.087\n1.913\n0.870\n\n\n2\n0.1\n28\n8\n0.8\n2.800\n0.087\n2.435\n0.870\n\n\n3\n0.1\n36\n4\n0.7\n3.6\n0.087\n3.130\n0.870\n\n\n4\n0.4\n40\n15\n0.3\n16\n0.348\n13.913\n0.870\n\n\n5\n0.1\n55\n10\n0.2\n5.5\n0.087\n4.783\n0.870\n\n\n6\n0.1\n65\n35\n0.1\n6.5\n0.087\n5.652\n0.870\n\n\n7\n0.1\n100\n0\n0\n10\n0.217\n21.739\n2.174\n\n\nSum\n1\n\n100\n\n\n\n53.565\n\n\n\n\n\n\n\n\n\n\nLet us examine the allocations obtained by the CCoC assumption and compare the results obtained by using the Wang distortion.\nThe \\(Z\\) values associated with the Wang distortion are shown in Table 4.9; the CCoC values are in Table 4.10. The last three \\(Z\\) values are greater than one for Wang, whereas only the last one is greater than one for CCoC.\n\n\n\n\nTable 4.11: Industry Standard Approach to allocation.\n\n\n\n\n\n\n\n\n\nVariable\nUnit A \\(X^1_j\\)\nUnit B \\(X^2_j\\)\nUnit C \\(X^3_j\\)\nTotal \\(X_j\\)\n\n\n\n\n\\(L=\\mathsf E[X]\\)\n13.4\n18.3\n14.9\n46.6\n\n\n\\(a=\\mathsf{TVaR}\\)\n16\n20\n64\n100\n\n\n\\(Q=\\nu\\mathsf{XTVaR}\\)\n2.261\n1.478\n42.696\n46.435\n\n\n\\(P\\)\n13.739\n18.522\n21.304\n53.565\n\n\n\\(M=P-L\\)\n0.339\n0.222\n6.404\n6.965\n\n\nAllocation of \\(M\\)\n0.049\n0.032\n0.919\n1\n\n\n\\(\\iota=M/Q\\)\n0.150\n0.150\n0.150\n0.150\n\n\n\n\n\n\n\n\n\nFor InsCo, the results of the CCoC allocation are shown in Table 4.11. Unit C, because of its outsize presence in the worst event, gets 91.9% of the capital and margin allocated to it. This tends to be the outcome from practical applications of the standard approach: the thick-tail lines of business get nearly all the capital and margin allocated to them and the thin tail lines get a free pass. With the Wang distortion, instead of getting 8% of the margin, Units A and B get 15% of the margin.\nTo recap: It is often assumed that one can transition from allocating capital to allocating cost of capital (i.e., margin or premium) simply by multiplying capital by a single cost of capital rate. Indeed, it is basically that simple when dealing with the entire business portfolio in aggregate (because the total cost of capital is the weighted average cost times the amount). However, when disaggregating results into units this simple tactic founders for two reasons. First, as we saw above, purported capital allocation procedures based on marginal analysis of the capital risk measure aren’t really relevant to pricing. Second, it is likely that CCoC is not appropriate because there is no single cost of capital rate that applies to all layers of risk and different units consume a different mix of capital by layer. Each contributor to the business has its own risk profile and its own joint dependency with the aggregate risk profile—and therefore its own cost of capital.\nUnderstanding the flaws of the ISA highlights the improvements offered by layer pricing and allocation. First, by choice of distortion \\(g\\), we can match the cost of capital by layer to market observables. Second, since unit consumption of capital varies by layer, the calculation naturally produces different returns by unit in total, thus solving the problem from the 1990s originally tackled using underwriting betas. Moreover, the solution is consistent with the observations in Table 1.1. Thick-tailed, catastrophe exposed lines consume relatively more higher layer capital. Higher layer capital is less exposed to loss (like a higher rated bond) and earns a lower return. Thus, such lines will be charged with a lower cost of capital. They will run with lower leverage too, resulting in a low loss ratio. This is why catastrophe reinsurance works economically: it substitutes expensive equity for cheaper debt-like capital.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#sec-pricing-events",
    "href": "040_PricingAndAllocation.html#sec-pricing-events",
    "title": "4  Pricing and allocation",
    "section": "4.8 Pricing events: An explanation",
    "text": "4.8 Pricing events: An explanation\nWe saw in Equation 4.6 that premium can be computed as the expected value of the loss outcomes \\(X_k\\) with respect to altered probabilities \\(q_k\\). This seems to provide us with an interpretation of pricing events, rather than layers, but that interpretation comes with a big caveat.\nSo far, this is just algebra. We would like to interpret \\(Z_k\\) as the unique state price for event \\(k\\) and take this as the definitive statement of event pricing, backed by a no-arbitrage argument. Alas, that interpretation is not quite correct for reasons that involve bid-ask spreads and are beyond the scope of this monograph. They are explained more fully in chapter 10 of Mildenhall and Major (2022).\nHowever, it does provide a set of internally consistent allocations to events as part of the total. The portfolio premium \\(P\\) can be regarded as an expectation taken under the new, \\(q\\) probability \\(\\mathsf E_q[X]\\) or, equivalently, as a weighted expectation \\(\\mathsf E_p[ZX]\\) under the original distribution with \\(Z\\) as the weight. Using the \\(q\\) probabilities this way is also applying an SRM to \\(X\\), equivalent to how we did it previously through layer pricing. Using a distorted expectation to represent price is very much in the spirit of using risk neutral probabilities in modern finance; they are not always unique there, either. Thus, if we fix \\(X\\), then we can price any subunit \\(X^i\\) of \\(X\\) using \\(q_k\\) \\[\n\\text{allocation to\\ } X^i = \\sum_k X^i_k q_k\n\\] and never never run into logical or economic inconsistencies. This is exactly the same as the procedure for computing coTVaR and is the basis of the NA.\nIn general, Equation 4.7 can also be used to price any set of outcomes \\(Y_k\\) that are comonotonic with \\(X_k\\)—in the sense that it will give the same stand-alone price as the spectral measure. This follows because the ordering of events determined by \\(X_k\\) gives a nondecreasing ordering of events for \\(Y_k\\). (The converse may not be true. For example \\(Y_k=c\\) is constant is comonotonic with \\(X_k\\), but would admit any ordering of events as nondecreasing.) In our case, each layer is comonotonic with the total, so Equation 4.7 can be seen as an application. In general, the allocation will be less than or equal to the stand-alone price. There is a lower bound too, see chapter 10 of Mildenhall and Major (2022).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#summary",
    "href": "040_PricingAndAllocation.html#summary",
    "title": "4  Pricing and allocation",
    "section": "4.9 Summary",
    "text": "4.9 Summary\nSpectral pricing gives the price of the total (whole portfolio) risk in the market. In contrast, the NA gives an allocation to part of the total risk—and crucially depends on the total. The allocation equals the stand-alone spectral price only in the case that the risk is comonotonic with the total risk. You can use and interpret the NA as a risk-adjusted expected value, and never have a problem, provided you understand it in the context of a total \\(X\\). As an allocation, it has the beauty of being consistent with a marginal approach in the case that all the outcomes of \\(X\\) are distinct. In fact, it is consistent if and only if all the outcomes are distinct.\nSpectral prices are sub-additive, so insureds benefit from pooling together—providing a motivation for insurance companies to exist! The insurer has to figure out how to allocate that lower price fairly among its insureds. Of course, the insurer is happy with any price greater than equal to a fair allocation, particularly with the marginal interpretation of allocation. The marginal interpretation drives profit maximizing behavior in a standard sense.\nWe conceptualized the cash flows between policyholders and investors via the InsCo one-year model with claim payouts \\(X\\) and probabilities \\(p\\). We distinguished between capital and pricing risk measures. Portfolio funding was subject to the constraints \\(P+Q=a\\) and \\(P=L+M\\) with the investor return being \\(\\iota=M/Q\\), resulting in Equation 4.2: \\[\nP =  L + \\delta (a-L) = L + \\iota Q.\n\\]\nThese relationships were carried down to hold for each asset layer. Pricing was done via SRMs through distorted probabilities with \\(S\\) being replaced by \\(g(S)\\) and \\(p\\) being replaced by \\(q=\\Delta g(S)\\). This allows us to price any cash flow that is a function of specific value outcomes \\(X\\). In particular we can price the units of the portfolio: lines of business or even policies.\nWe saw that while understanding the marginal impact of increasing a unit’s exposure on the overall asset requirement (the capital measure) was meaningful for risk management, it had little relevance to pricing per se. We also saw that assuming a CCoC had troublesome implications about event weights \\(Z=q/p\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "040_PricingAndAllocation.html#algorithms",
    "href": "040_PricingAndAllocation.html#algorithms",
    "title": "4  Pricing and allocation",
    "section": "4.10 Algorithms",
    "text": "4.10 Algorithms\nThe next two algorithms are taken from Mildenhall and Major (2022). They are included here for completeness and ease of reference. As is often the case, they look more intimidating than they are—build the spreadsheet and you’ll see they are quite straightforward!\n\n4.10.1 Algorithm to evaluate an SRM on a discrete random variable\nAlgorithm Input: \\(X\\) is a discrete random variable, taking values \\(X_j\\ge 0\\), and \\(p_j=\\mathsf P(X=X_j)\\), \\(j=1,\\dots, n\\). \\(\\rho_g\\) is an SRM.\nFollow these steps to determine \\(\\rho_g(X)\\).\nAlgorithm Steps\n\nPad the input by adding a zero outcome \\(X_0=0\\) with probability 0.\nSort events by outcome \\(X_j\\) into ascending order.\nGroup by \\(X_j\\) and sum the corresponding \\(p_j\\). Relabel events \\(X_0 &lt; X_1 &lt; \\dots &lt; X_{n'}\\) and probabilities \\(p_0,\\dots, p_{n'}\\). All \\(X_j\\) are distinct.\nDecumulate probabilities to determine the survival function \\(S_j:=S(X_j)\\) using \\(S_0=1-p_0\\) and \\(S_j=S_{j-1}-p_j\\), \\(j&gt;0\\).\nDistort the survival function, computing \\(g(S_j)\\).\nDifference \\(g(S_j)\\) to compute risk-adjusted probabilities \\(\\Delta g(S_0)=1-g(S_0)\\), \\(\\Delta g(S_j)=g(S_{j-1})-g(S_j)\\), \\(j &gt; 0\\).\nSum-Product to compute \\(\\rho_g(X)= \\sum_j X_j\\,\\Delta g(S_j)\\).\n\nSteps (4) and (6) are inverse to one another. The transition \\(p\\to S\\to gS \\to \\Delta gS\\) from input probabilities to risk-adjusted probabilities is used in all SRM-related algorithms.\nThe algorithm computes the outcome-probability formula \\[\\begin{equation*}\n\\rho_g(X) = \\int_0^\\infty xg'(S(x))\\,dF(x) = \\sum_{j&gt;0} X_j \\Delta g(S_j).\n\\end{equation*}\\] When \\(X\\) is discrete the Steiltjes integral becomes a sum—the joy of using \\(dF\\). We can also use the survival function form with \\(\\Delta X_j=X_{j+1}-X_j\\) \\[\\begin{equation*}\n\\rho_g(X) = \\int_0^\\infty g(S(x))\\,dx = \\sum_{j\\ge 0} g(S_j) \\Delta X_j.\n\\end{equation*}\\]\n\n\n4.10.2 Algorithm to compute the linear NA for discrete random variables\nAlgorithm Inputs:\n\nThe outcome values \\((X^1_j,\\dots,X^m_j)\\), \\(j=1,\\dots, n\\), of a discrete \\(m\\)-dimensional multivariate loss random variable. Outcome \\(j\\) occurs with probability \\(p_j\\). \\(X_j=\\sum_i X^i_j\\) denotes the total loss for outcome \\(j\\).\nAn SRM \\(\\rho_g\\) associated with the distortion function \\(g\\).\n\nFollow these steps to determine \\(D^n\\rho_X(X^i)\\), the NA of \\(\\rho(X)\\) to unit \\(i\\).\nAlgorithm Steps\n\nPad the input by adding a zero outcome \\(X^1_0=\\cdots=X^m_0=X_0=0\\) with probability \\(p_0=0\\).\nSort events by total outcome \\(X_j\\) into ascending order.\nGroup by \\(X_j\\) and take \\(p\\)-weighted averages of the \\(X^i_j\\) within each \\(i\\) and \\(X_j=x\\) group. Sum the corresponding \\(p_j\\). Relabel events using \\(j=0,1,\\dots, n'\\) as \\(X^i_j\\) and probabilities \\(p_0,\\dots, p_{n'}\\).\nDecumulate probabilities to determine the survival function \\(S_j:=S(X_j)\\) using \\(S_0=1-p_0\\) and \\(S_j=S_{j-1}-p_j\\), \\(j&gt;0\\).\nDistort the survival function, computing \\(g(S_j)\\).\nDifference \\(g(S_j)\\) to compute risk-adjusted probabilities \\(\\Delta g(S_0)=1-g(S_0)\\) and \\(\\Delta g(S_j)=g(S_{j-1})-g(S_j)\\), \\(j &gt; 0\\).\nSum-Product to compute \\(\\rho_g(X)= \\sum_j X_j\\,\\Delta g(S_j)\\) and \\[\\begin{equation}\\label{eq:Dn-rho-xi}\nD^{n}\\rho_X(X_i) = \\mathsf E[X_iZ] = \\sum_j X^i_j\\,Z_j\\,p_j = \\sum_j X^i_j\\,\\frac{\\Delta g(S_j)}{p_j}\\,p_j = \\sum_j X^i_j\\,\\Delta g(S_j).\n\\end{equation}\\]\n\nComments.\n\nWhen the data are produced by a simulation model, \\(n\\) equals the number of events and \\(m\\) the number of units. With realistic data, \\(n\\) is in the range thousands to millions, and \\(m\\) ranges from a handful up to the hundreds for a full corporate model.\nStep (1) only results in a new outcome row when the smallest \\(X_j\\) observation is \\(&gt;0\\).\nThe averages in Step 3 are implemented as \\[\\begin{equation} \\label{eq:EXiX-discrete-simple}\n\\kappa^i(x) = \\mathsf E[X_i\\mid X=x] = \\frac{\\sum_{j:X_j = x}  p_j\\,X^i_j} {\\sum_{j:X_j = x}p_j}.\n\\end{equation}\\]\nAfter Step (3), the \\(X_j\\) are distinct, they are in ascending order, \\(X_0=0\\), and \\(p_j=\\mathsf P(X=X_j)\\).\nThe backward difference \\(\\Delta g(S_j)\\) computed in Step (6) replaces \\(g'(S)dF(x)\\) in various formulas.\n is an exact equation for a discrete distribution. Approximation occurs if it is applied to the empirical distribution of a discrete sample representing a different underlying distribution, possibly one with density.\n\n\n\n\n\n\n\nArtzner, Philippe, Freddy Delbaen, Jean-Marc Eber, and David Heath. 1999. “Coherent measures of risk.” Mathematical Finance 9 (3): 203–28. http://onlinelibrary.wiley.com/doi/10.1111/1467-9965.00068/abstract.\n\n\nBodoff, Neil M. 2007. “Capital Allocation by Percentile Layer.” Variance 3 (1): 13–30.\n\n\nBühlmann, Hans. 1985. “Premium Calculation from Top Down.” ASTIN Bulletin 15 (2): 89–101. https://doi.org/10.2143/ast.15.2.2015021.\n\n\nCaramagno, Nicholas, David Mamane, and Liam Neilson. 2021. An Introduction to IFRS 17 for P&C Actuaries. https://www.ifrs.org/issued-standards/list-of-standards/ifrs-17-insurance-contracts/.\n\n\nCummins, J. David. 1990. “Multi-Period Discounted Cash Flow Rate-making Models in Property-Liability Insurance.” Journal of Risk and Insurance 57 (1): 79–109.\n\n\nCummins, J. David. 2000. “Allocation of capital in the insurance industry.” Risk Management and Insurance Review, nos. October 1998: 7–28. http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6296.2000.tb00013.x/full.\n\n\nFairley, William B. 1979. “Investment Income and Profit Margins in Property-Liability Insurance: Theory and Empirical Results.” The Bell Journal of Economics 10 (1): 192. https://doi.org/10.2307/3003326.\n\n\nHeynderickx, W., J. Cariboni, W. Schoutens, and B. Smits. 2016. “The relationship between risk-neutral and actual default probabilities: the credit risk premium.” Applied Economics 48 (42): 4066–81. https://doi.org/10.1080/00036846.2016.1150953.\n\n\nKozik, Thomas J. 1994. “Underwriting betas-the shadows of ghosts.” Proceedings of the Casualty Actuarial Society 81 (154, 155): 303–29.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.\n\n\nMyers, Stewart C, and Richard A Cohn. 1987. “A discounted cash flow approach to property-liability insurance rate regulation.” In Fair Rate of Return in Property-Liability Insurance. Springer.\n\n\nTasche, Dirk. 1999. “Risk contributions and performance measurement.” Report of the Lehrstuhl Fur Mathematische Statistik, TU Munchen, 1–26.\n\n\nVenter, Gary G. 1991. “Premium Calculation Implications of Reinsurance Without Arbitrage.” ASTIN Bulletin 21 (02): 223–30. https://doi.org/10.2143/ast.21.2.2005365.\n\n\nVenter, Gary G., John A. Major, and Rodney E. Kreps. 2006. “Marginal Decomposition of Risk Measures.” ASTIN Bulletin 36 (2): 375–413. https://doi.org/10.2143/AST.36.2.2017927.\n\n\nWang, Shaun S. 1996. “Premium Calculation by Transforming the Layer Premium Density.” ASTIN Bulletin 26 (01): 71–92. https://doi.org/10.2143/AST.26.1.563234.\n\n\nWang, Shaun S. 2000. “A Class of Distortion Operators for Pricing Financial and Insurance Risks.” The Journal of Risk and Insurance 67 (1): 15–36. https://doi.org/10.2307/253675.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pricing and allocation</span>"
    ]
  },
  {
    "objectID": "050_Applications.html",
    "href": "050_Applications.html",
    "title": "5  Applications of pricing and allocation",
    "section": "",
    "text": "5.1 Performance assessment\nPerformance assessment, new business pricing, and reinsurance decision-making are all essentially the same exercise as price allocation. Mergers and acquisitions, and portfolio optimization, must be approached differently.\nTable 5.1 summarizes the events affecting InsCo’s three units of business as well as an available 35 excess of 65 stop loss reinsurance cover. This has ceded losses of zero in events 0 through 6 and 35 in event 7. It has an expected payout of 3.5 (loss on line 10%) and ceded premium of 6.5 (rate on line 18.6%). The expected net profit to the reinsurer is 3.0.\nIn the last three rows of the table we see the expected loss, the plan premium, and the required premium as calculated by a Wang transform calibrated to achieve a 15% return on capital for the (gross) portfolio.\nWhich units are performing at a satisfactory level?\nPerformance assessment is a multiattribute question, involving:\nand other qualities. However, a key factor is profitability. The first question is always: Is this unit meeting a profit benchmark?\nRecall we have a 15% return benchmark for the (gross) portfolio as a whole. Table 5.1 shows that additional premium of \\(53.565-52.2=1.365\\) is required to meet this threshold. This is true independent of the choice of distortion function—it is a direct consequence of the 15% return on capital requirement. Here we assume a calibrated Wang transform is used to allocate the portfolio requirement down to the units.\nTable 5.2 shows that of the three units, only Unit B has a plan premium that exceeds the allocated target premium, and there, only by 0.063. Unit A has a deficit of 0.209, and Unit C has most of the portfolio deficit at 1.219. Putting these shortfalls in perspective, Unit C still has the highest gap measured in loss ratio (4.45%), but Unit A needs to increase its plan margin by the greatest factor (41.8%)\nNote this analysis depends heavily on the fact that we have chosen the Wang transform to drive our SRM allocation. What if a different distortion function were used? And how do we decide which one to use? These questions are addressed in Chapter 8.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of pricing and allocation</span>"
    ]
  },
  {
    "objectID": "050_Applications.html#sec-Performance-Assessment",
    "href": "050_Applications.html#sec-Performance-Assessment",
    "title": "5  Applications of pricing and allocation",
    "section": "",
    "text": "recent and prospective growth\nexpense control\nemployee turnover\ncustomer retention\ncompetitive standing\nproduct and operational innovation\n\n\n\n\n\n\n\nTable 5.2: Plan premium relative to target required premium.\n\n\n\n\n\nMetric\nUnit A\nUnit B\nUnit C\nGross\n\n\n\n\nPlan Premium\n13.900\n18.700\n19.600\n52.200\n\n\n\\(\\rho_{Wang}\\)\n14.109\n18.637\n20.819\n53.565\n\n\n\\(\\mathsf E[X]\\)\n13.400\n18.300\n14.900\n46.600\n\n\nPlan margin\n0.500\n0.400\n4.700\n5.600\n\n\nExcess margin\n-0.209\n0.063\n-1.219\n-1.365\n\n\n% of plan\n-41.8%\n15.6%\n-25.9%\n-24.4%\n\n\nExcess LR\n-1.43%\n0.33%\n-4.45%\n-2.28%",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of pricing and allocation</span>"
    ]
  },
  {
    "objectID": "050_Applications.html#sec-Reinsurance-decision-making",
    "href": "050_Applications.html#sec-Reinsurance-decision-making",
    "title": "5  Applications of pricing and allocation",
    "section": "5.2 Reinsurance decision-making",
    "text": "5.2 Reinsurance decision-making\nReinsurance serves as loss protection and a source of loss-bearing capital. Table 5.1 shows a 35 excess of 65 portfolio stop loss contract. We assume this is aggregate cover, so questions of reinstatement do not apply. Per-occurrence cover would need to be analyzed to see its impact on aggregate results.\nShould this cover be purchased?\nReinsurance decisions must weigh several factors, including impact on capital requirements and underwriting constraints, counterparty credit risk, and price. First let us consider capital requirements.\nPreviously, we assumed a \\(\\mathsf{TVaR}_{0.99}\\) standard for required assets. For the gross portfolio, this led to \\(a=100\\), full collateral in this example. For the net portfolio, it would lead to \\(a=65\\) if full credit were to be allowed or if the same standard were applied to net losses.\nExercise. Apply the rate of return-driven pricing formula (Equation 4.2) to derive the required premium for the net portfolio based on a 15% required return. What might you conclude about the price of the reinsurance cover?\nSolution. \\[\nP = \\nu L + \\delta a = (0.86956)(43.1) + (0.13043)(65) = 45.956\n\\] where \\(L=43.1\\) is the net expected loss per Table 5.1, and \\(a=65\\) is the required asset conditional on having stop loss reinsurance. For comparison, recall from the end of Section 4.1 that \\(P(gross) = 0.86956\\cdot46.6+0.13043\\cdot100=53.565\\) (consistent with plugging in values from the Gross column of Table 5.1). This represents savings of 7.609, which is the maximum that InsCo should be willing to cede for the cover. Since the cover costs 6.5 (Table 5.1), we might conclude that the price of the cover is advantageously low and it should be purchased (unless a better deal were available). \nUnfortunately, this line of thinking relies on the familiar CCoC assumption. The risk profile has changed between the gross and net portfolio, so there is no basis for assuming that the same rate of return is required. An alternative approach is to use the loss protection interpretation and view reinsurance as a source of capital. This is elaborated upon in Mildenhall and Major (2022).\nApplying the same Wang transformation allocation to the reinsurance cash flows, Table 5.1 shows a target required premium of 6.087, which is 0.413 lower than the actual 6.5 ceded premium. We conclude the cover should not be purchased.\nExercise. Using the Wang allocation and assuming \\(a=65\\), compute the return on capital for the net portfolio. Explain why the result is in this direction.\nSolution. \\[\n\\iota = \\frac{M}{Q} = \\frac{P-L}{a-P} = \\frac{47.478-43.1}{65-47.478} = \\frac{4.378}{17.522} = 25\\%.\n\\] All investment tranches \\((Q_j^{Net},M_j^{Net})=(Q_j^{Gross},M_j^{Gross})\\), \\(j=1,\\dots,5\\) are identical, but the last tranche \\(j=6\\) has \\(\\Delta X_j^{Net}=0 &lt; \\Delta X_j^{Gross}\\). Relative to gross, the net WACC is therefore skewed to the lower tranches, which have higher exceedance probabilities and therefore higher required returns.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of pricing and allocation</span>"
    ]
  },
  {
    "objectID": "050_Applications.html#new-business-pricing",
    "href": "050_Applications.html#new-business-pricing",
    "title": "5  Applications of pricing and allocation",
    "section": "5.3 New business pricing",
    "text": "5.3 New business pricing\nWhile underwriting involves many factors other than pricing, such as exposure guidelines and physical and financial standards reflecting moral hazard, etc., pricing is a key task in the onboarding of new business.\nAt first glance, this seems equivalent to the performance assessment task: calculating a required premium as a distorted loss expectation. It would seem there is not much to add to this discussion. Typically, however, proposed new business is not run through the full capital model directly because that would be inefficient in terms of computing resources and timeliness. Rather, a simplified or surrogate version of the capital model is used to extract key loss statistics.\nLet’s say for example we have a (relatively) simple cat model through which the portfolio has been run—once, and the results stored—and through which a new business prospect has just been run. Non-cat losses are represented by an independent lognormal variable calibrated on exposure characteristics. The resulting scatter plot is shown in Figure 5.1.\n\n\n\n\n\n\nFigure 5.1: Simulated losses from a surrogate model applied to the gross portfolio and a new business prospect.\n\n\n\nThe problem is to translate these results into a form suitable for SRM allocation pricing. One could use the calibrated Wang transform to sort these results by the portfolio loss and calculate distorted probabilities, but we have no guarantee that the surrogate model accurately reproduces the distribution of portfolio losses that the capital model uses. The solution is to extract the conditional expected new business losses for each portfolio loss in Table 5.1. (See the discussion of \\(\\kappa^i(X_k)\\) in Section 4.3).\nThis extraction can be done via local regression (LOWESS, LOESS, kernel smoothing; Friedman et al. (2008)). Figure 5.2 shows the scatter plot of Figure 5.1 with an estimate of the conditional mean loss curve and the specific event points. This leads to the augmented loss (Table 5.3).\n\n\n\n\n\n\nFigure 5.2: Estimating conditional expected losses using locally weighted regression (LOWESS).\n\n\n\n\n\n\nTable 5.3: Events with new business conditional expected losses.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\n\\(q\\)\nGross\nNew Business\n\n\n\n\n0\n0.0\n0.0000\n0\n0.00000\n\n\n1\n0.1\n0.0522\n22\n0.02028\n\n\n2\n0.1\n0.0660\n28\n0.01965\n\n\n3\n0.1\n0.0748\n36\n0.01849\n\n\n4\n0.4\n0.3791\n40\n0.01855\n\n\n5\n0.1\n0.1190\n55\n0.02402\n\n\n6\n0.1\n0.1350\n65\n0.02910\n\n\n7\n0.1\n0.1739\n100\n0.06339\n\n\n\\(L=\\mathsf E[X]\\)\n\n\n46.600\n0.02491\n\n\n\\(P=\\rho_{\\mathsf{Wang}}(X)\\)\n\n\n53.565\n0.02858\n\n\n\\(M=P-L\\)\n\n\n6.965\n0.00367\n\n\n\n\n\n\nExercise. Why is it reasonable to treat a small new business addition as if it were a unit within the portfolio, using the SRM allocation to calculate a reservation price?\nSolution. Because it would have little or no impact on the sort order of portfolio losses. The distorted event probabilities \\(q\\) are functions of the distorted exceedance probabilities \\(g(S)\\), which are in turn functions of exceedance \\(S\\), which in turn is crucially dependent on the sort order of portfolio event losses. In the simple example, adding the new business losses \\(X^{n}\\) to portfolio losses \\(X\\) did not change the sort order of \\(X+X^{n}\\) compared to \\(X\\). In general, with a realistically fine-grained simulation, there may indeed be changes in the sort order, but they will be small and not be material to the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of pricing and allocation</span>"
    ]
  },
  {
    "objectID": "050_Applications.html#mergers-and-acquisitions",
    "href": "050_Applications.html#mergers-and-acquisitions",
    "title": "5  Applications of pricing and allocation",
    "section": "5.4 Mergers and acquisitions",
    "text": "5.4 Mergers and acquisitions\nWhat do you need to consider in a merger or acquisition? In general, there are quite a few considerations, such as strategic fit, financial performance of the target company, cultural compatibility, synergies and cost savings, regulatory compliance, tax implications, valuation for the target company, and risk management.\nWhile the finance department will no doubt have an opinion on valuation, the capital modeling team could also be tasked with an analysis of how the risk profile of the new portfolio fits with the existing portfolio. The same principles discussed previously still apply, but with a twist. Whereas it was reasonable to treat a new business policy as if it were already a (relatively small) unit within the portfolio, a sizeable acquisition requires a different approach.\nSay instead of the loss distribution portrayed in Figure 5.1, we had an acquisition opportunity with losses scaled up nearly 2,000 times larger on the \\(y\\)-axis at every point, having a mean loss of 48.35. We will focus on losses, with the acquired premiums to be compared to the required premium we will calculate.\nRather than the conditional mean loss of the acquired business (hereinafter \\(X^a\\)), we need the entire conditional distribution (or at least a representative sample). There are several technical approaches to this, such as quantile regression (Koenker and Hallock 2001) or kernel smoothing (Wand and Jones 1994). For our example, we will take what is possibly the simplest nontrivial approach.\n\nCalculate the conditional mean \\(\\bar{X^a} = \\mathsf E[X^a|X]\\) via LOWESS at every \\(X\\) point in the scaled-up equivalent to Figure 5.1, just as we did to obtain Figure 5.2.\nAt each such \\(X\\) point, compute the squared difference between \\(X^a\\) and \\(\\bar{X^a}\\).\nUse LOWESS again to compute the conditional mean square deviation, i.e. the variance, and its square root, the conditional standard deviation, \\(\\sigma_a\\).\nInspired by Gauss-Hermite quadrature (Kovvali 2022), we compute \\(\\bar{X^a}\\) plus or minus one standard deviation. This is depicted in Figure 5.3.\nDuplicate each row in Table 5.1, inserting a new column for loss \\(X^a\\) consisting of those \\(\\bar X^a \\pm \\sigma_a\\) points. Rescale each probability by half.\nInsert a new column for merger losses \\(X^m=X+X^a\\). Sort on \\(X^m\\).\nCompute \\(S\\), \\(g(S)\\), and \\(q\\) as before, based on the same Wang transform SRM. This is shown in Table 5.4.\n\n\n\n\n\n\n\nFigure 5.3: Representing the conditional loss distribution using locally weighted regression (LOWESS): mean plus or minus one standard deviation.\n\n\n\n\n\n\nTable 5.4: Events with acquired business Gauss-Hermite two-point sampled conditional losses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\n\\(X\\)\n\\(X^a\\)\n\\(X^m\\)\n\\(S\\)\n\\(g(S)\\)\n\\(q\\)\n\n\n\n\n0\n0\n0\n0\n0\n1\n1\n0\n\n\n1\n0.05\n22\n29.37\n51.37\n0.95\n0.9766\n0.0234\n\n\n2\n0.05\n28\n25.47\n53.47\n0.9\n0.9478\n0.0287\n\n\n3\n0.05\n36\n21.55\n57.55\n0.85\n0.9161\n0.0318\n\n\n4\n0.2\n40\n22.74\n62.74\n0.65\n0.7667\n0.1494\n\n\n5\n0.05\n22\n49.35\n71.35\n0.6\n0.7244\n0.0423\n\n\n6\n0.05\n28\n50.79\n78.79\n0.55\n0.6802\n0.0442\n\n\n7\n0.05\n55\n30.83\n85.83\n0.5\n0.6341\n0.0461\n\n\n8\n0.05\n36\n50.22\n86.22\n0.45\n0.5859\n0.0482\n\n\n9\n0.2\n40\n49.27\n89.27\n0.25\n0.3700\n0.2159\n\n\n10\n0.05\n65\n29.89\n94.89\n0.2\n0.3089\n0.0611\n\n\n11\n0.05\n55\n62.38\n117.38\n0.15\n0.2439\n0.0650\n\n\n12\n0.05\n100\n47.73\n147.73\n0.1\n0.1739\n0.0700\n\n\n13\n0.05\n65\n83.06\n148.06\n0.05\n0.0964\n0.0775\n\n\n14\n0.05\n100\n198.30\n298.30\n0\n0\n0.0964\n\n\n\\(L=\\mathsf E[X]\\)\n\n46.60\n48.35\n94.95\n\n\n\n\n\n\\(P=\\rho_{\\mathsf{Wang}}(X)\\)\n\n52.74\n59.09\n111.83\n\n\n\n\n\n\n\n\n\nThe required premium split between our original portfolio \\(X\\) (52.74) and the acquired \\(X^a\\) (59.09) is irrelevant to this analysis! Recall the original portfolio required premium was 53.57. What is important is the difference between the new merger portfolio premium 111.83 and the original premium of 53.57, i.e., 58.26. This is what InsCo would require for premium from the new business to secure enough margin to satisfy investors.\nExercise. Explain the capital situation. What increment of assets is needed with the newly acquired business? What capital? What is the total return on capital for the combined business? Return on incremental capital? Explain why this is not the 15% return originally specified.\nSolution. Using a 95% TVaR criterion, total assets of 298.30 are needed for the combined portfolio, which is an increase of 198.30 over the original 100. With required premium of 111.83, this corresponds to capital of 186.46 for the combined portfolio, an increase of 140.03 over the original 46.43 (\\(=100 - 53.57\\)) capital. Margin on the combined portfolio is 16.88, for a return of 9.06%. The margin has gone up by 9.92, corresponding to a return of 7.08% on incremental capital. More capital is backing more remote probability losses, requiring less margin and therefore lower returns on capital.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of pricing and allocation</span>"
    ]
  },
  {
    "objectID": "050_Applications.html#sec-portfolio-optimization",
    "href": "050_Applications.html#sec-portfolio-optimization",
    "title": "5  Applications of pricing and allocation",
    "section": "5.5 Portfolio optimization",
    "text": "5.5 Portfolio optimization\nIf this is the best of possible worlds,\nwhat then are the others?\n- Voltaire, “Candide”\n\n5.5.1 Theory\nPortfolio management is an inverse problem to pricing. Rather than taking the portfolio as given and determining appropriate pricing, the problem becomes to take the pricing as given and determine the best (or at least a better) composition of the portfolio. This task is typically complicated, however, by constraints such as volume or risk measures.\nThe textbook approach to optimization (e.g., Postek et al. (2024b), available online at Postek et al. (2024a)) starts with an objective function \\(f(x)\\) where \\(x\\) is typically a vector parameter. The problem is to find the value of \\(x\\) that maximizes (or minimizes) \\(f(x)\\). This is often complicated by the existence of multiple constraint functions \\(h_j(x)\\) requiring \\(h_j(x) \\le 0\\) for \\(j=1,\\dots,n\\) and \\(h_j(x) = 0\\) for \\(j=n+1,\\dots,m\\).\nWhat should be our objective function? It is tempting to think it should be margin, the difference between plan premium and expected loss. However, if our revelations in Chapter 4 taught us anything, it is that modifying the portfolio will modify the risk and that being properly rewarded for risk goes beyond expected loss. We need to compare the (rescaled) plan premium to the (rescaled) required premium from the pricing functional.\nWe found at the end of Section 4.1 that InsCo’s economic value added (EVA) is \\(-1.6\\). Our goal will be to make EVA as large (positive) as possible, or at the very least make it less negative.\nFor portfolio optimization, the particulars will likely look like:\n\nObjective function is economic value added.\nThe parameters are scale factors, one for each unit.\nThe primary constraint is to stay at the current capital.\nSecondary constraints are limits on growing or shrinking each unit.\n\nScale factors determine the volume of each unit. The current portfolio includes the random variable \\(X^i\\) representing the losses from unit \\(i\\). We now introduce the random variable \\(X^i(\\epsilon)\\) to represent its losses if the unit volume has changed by the factor \\(1+\\epsilon\\). The original portfolio loss variable is \\(X=\\sum_i X^i(0)\\) in this notation. If unit \\(i\\) were to grow, say, 5%, its loss random variable would be represented as \\(X^i(0.05)\\).\nLet the portfolio loss random variable be \\(X(\\epsilon^1,\\dots)=\\sum_i X^i(\\epsilon^i)\\) and denote the corresponding required assets by \\(a(\\epsilon^1,\\dots)\\). For any particular \\(\\epsilon\\) values, the capital model can compute these.\nFor the InsCo example, we will assume the constraints are \\(-0.5 \\le \\epsilon \\le 0.5\\).\nIf unit losses scaled linearly, as they do in stock portfolios, then we would have \\(X^i(\\epsilon)=(1+\\epsilon) X^i(0)\\). Unfortunately, this is usually not the case with insurance, although catastrophe cover in a limited geographic area comes close. However, a properly defined scale factor will give us scaling in planned premium.\nThere is a second source of nonlinearity here. As units grow or shrink, so do their losses in each event. Because the computation of the pricing SRM (and probably the capital measure as well) depends on the ordering of portfolio losses, that ordering may change, and so the measure will vary nonlinearly with the scale factors. For small changes in scale (i.e. \\(\\epsilon\\) close to 0), this reordering may not happen or may not make a material difference. Our approach will take this into account.\nThe primary capital constraint listed above needs elaboration. As the portfolio composition shifts, the asset requirement (specified by the capital risk measure \\(a(X)\\)) will change. So will the total plan premium. We interpret the constraint to mean that the portion of assets not supplied by premium, \\(Q=a-P\\) (Equation 1.1), be maintained at the current level. The constraint might have been simply not to exceed the current level, but we don’t want to “leave money on the table” in the sense of holding more capital than we are currently using to support risk. So the constraint is an equality. In some other application, there could be a target capital different than the current level. Our approach is easily adaptable to that.\nIn a real application, there might be other constraints. For example, we might not want total portfolio margin to go down. We will keep the example here relatively simple.\nIf EVA and required assets scaled linearly, then the above would be a linear programming (LP) problem that could be solved with readily-available linear programming codes. Locally, that is to say with \\(\\epsilon\\) close to zero, this should be a good approximation. However, we want to handle bigger steps correctly, so we need a nonlinear optimization strategy. The Nelder-Mead Simplex algorithm is generally applicable, but its performance may suffer in comparison to methods that use first derivatives such as the conjugate gradient algorithm by Polak and Ribiere and the quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno.  Other methods, such as the Newton conjugate gradient algorithm, use both first and second derivatives.\nIn Python, these and other methods are available in scipy.optimize. In R, the same holds for nloptr. Some of these implementations rely on FORTRAN codes written half a century ago!\nIf we treat losses as scaling linearly, then we can make use of first derivatives in our solution method. First derivatives are almost automatically provided when the objective—portfolio EVA—is evaluated. Second derivatives are impractical to compute here. If we do not wish to treat losses as scaling linearly, then we have a few options:\n\nUse a method like Nelder-Mead that does not require derivatives.\nCompute derivatives numerically by evaluating the objective at nearby points. This could be computationally inefficient.\nUse a proxy model—a simpler version of the complete model that approximates it—to provide derivatives. This is essentially what we are doing when we assume linear scaling, but a more accurate nonlinear proxy might be available.\n\nThe overall solution strategy here is stepwise hill climbing:\n\nIdentify promising directions for scale changes.\nSelect new scale parameters satisfying the capital and scale constraints.\nRerun the model at the new scales.\nRepeat until EVA improvement seems impossible.\n\nWith linear scaling, the important thing to remember is that allocated quantities (premium, assets, capital) are the same as marginal quantities. If a unit is scaled up by 1% (\\(\\epsilon=0.01\\)), say, then the allocated quantity will go up by 1%.\n\n\n5.5.2 InsCo optimization\nThe input data for InsCo is provided in Table 5.5.\n\n\n\nTable 5.5: InsCo marginal EVA and capital.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\nPlan premium \\(P_P\\)\n13.9000\n18.7000\n19.6000\n52.2000\n\n\nRequired prem \\(P_R\\)\n14.1092\n18.6375\n20.8186\n53.5652\n\n\nEVA \\(V=P_P-P_R\\)\n-0.2092\n0.0625\n-1.2186\n-1.3652\n\n\nRequired assets \\(a\\)\n16.0000\n20.0000\n64.0000\n100.0000\n\n\nPlan capital \\(Q_P=a-P_P\\)\n2.1000\n1.3000\n44.4000\n47.8000\n\n\nEVA/capital ratio \\(r=V/Q_P\\)\n-0.0996\n0.0481\n-0.0274\n\n\n\nMinimum scale \\(\\epsilon_-\\)\n-0.5\n-0.5\n-0.5\n\n\n\nMaximum scale \\(\\epsilon_+\\)\n0.5\n0.5\n0.5\n\n\n\n\n\n\n\nExercise. Assume that event 10 is always the one to produce the largest portfolio loss and therefore the capital measure. Compute an expression for \\(\\epsilon^3\\) (Unit C) in terms of \\(\\epsilon^1\\) (Unit A) and \\(\\epsilon^2\\) (Unit B) so that the capital equality constraint is maintained.\nSolution. Plan capital is the difference between assets and plan premium (Equation 1.1). Given the event 10 assumption, both scale linearly in \\(\\epsilon\\), so \\(Q\\) does as well. The requirement is \\[\n47.8 = \\sum_{i} Q_P^i(1+\\epsilon^i) = 2.1(1+\\epsilon^1) + 1.3 (1+\\epsilon^2) + 44.4(1+\\epsilon^3)\n\\] therefore \\[\n\\epsilon^3 = -\\frac{2.1}{44.4}\\epsilon^1 - \\frac{1.3}{44.4}\\epsilon^2 = -0.047297\\epsilon^1 - 0.029279\\epsilon^2.\n\\] \nA naive approach to optimization might take the linear approximation at face value and take the biggest and most advantageous change in scale parameters possible. Let’s see how this plays out with InsCo.\n\nThe most advantageous first move is to reduce Unit A by 50%. This increases EVA by \\((-0.2092) \\cdot (-0.5) = 0.1046\\). It also changes capital by \\((2.1) \\cdot (-0.5) = -1.05\\).\nNow we need to increase overall writings to use up all available capital and get it back to the target. Unit B has a positive EVA/capital ratio and Unit C a negative, so Unit B has the most advantage. We would have to increase Unit B’s scale by \\(1.05/1.3 = 0.808\\) to completely use the capital, but this is outside the constrained range. Scaling by the max allowed 50% adds \\(1.3 \\cdot 0.5 = 0.65\\) to the capital, bringing the net position to \\(-1.05 + 0.65 = -0.4\\). It also brings the EVA up by \\(0.0625 \\cdot 0.5 = 0.0313\\) to \\(0.1358\\).\nWe need a further adjustment, and the only candidate left is Unit C. Here we need a scale change of \\(0.4/44.4 = 0.0090\\), which is well within the allowed range. (The formula from the previous exercise tells us the same thing.) That brings the change in capital back to zero but changes EVA by \\((-1.2186) \\cdot 0.0090 = -0.0110\\) for a net EVA of \\(-1.2403\\) and an improvement of \\(0.1249\\). Adding these predicted changes to the original figures gives the results in Table 5.6.\n\n\n\n\nTable 5.6: InsCo linear predictions for EVA and capital.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\nScale change \\(\\epsilon^i\\)\n-0.5\n0.5\n0.009\n\n\n\nPlan premium \\(P_P\\)\n6.9500\n28.0500\n19.7766\n54.7766\n\n\nRequired prem \\(P_R\\)\n7.0546\n27.9562\n21.0061\n56.0169\n\n\nEVA \\(V=P_P-P_R\\)\n-0.1046\n0.0938\n-1.2295\n-1.2403\n\n\nRequired assets \\(a\\)\n8.0000\n30.0000\n64.5766\n102.5766\n\n\nPlan capital \\(Q_P=a-P_P\\)\n1.0500\n1.9500\n44.8000\n47.8000\n\n\n\n\n\n\nNow, had these moves been limited to, say 2%, we might be satisfied to stop here. In a real application, such a schedule of scale changes would be scrutinized for other business factors and questions of implementation feasibility. Some targets would probably be modified; for example, the Unit C scale change would no doubt be rounded to 1%.\nHowever, with 50% scale changes, we need to be wary of nonlinear effects. Let us rerun the model at the new scales.\nOur starting point is the original, 10-event loss distribution from Table 2.1. We multiply the losses in each column \\(i\\) by the scale factor \\(1+\\epsilon^i\\), retabulate the portfolio total loss, and then sort on the portfolio loss. We apply the same asset metric—essentially the maximum loss—and the same Wang transform for pricing. We do not compute a new Wang parameter to obtain a 15% portfolio return. Rather, we use the same parameter because this represents the investors’ risk appetite. With a new loss distribution and new asset requirement, we do not expect the same required return. The new loss table is set out in Table 5.7. The first column shows the original sorted event number \\(j\\), highlighting the reordering that has occurred with the change in volume by unit, see Table 2.1. Events 3, 4, 5, 7, and 8 all move and are therefore given a different weight by the Wang distortion. This extensive reshuffling accounts for the large (relative to EVA) change in required premium shown in Table 5.8.\n\n\n\nTable 5.7: InsCo losses after rescaling and re-sorting.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(j\\)\n\\(p\\)\nUnit A\nUnit B\nUnit C\nPortfolio\n\\(S\\)\n\\(g(S)\\)\n\\(q\\)\n\n\n\n\n\n0\n0\n0\n0\n0\n1.0\n1.000\n0.000\n\n\n1\n0.1\n7.5\n10.5\n0\n18.0000\n0.9\n0.931\n0.069\n\n\n2\n0.1\n7.5\n19.5\n0\n27.0000\n0.8\n0.851\n0.080\n\n\n7\n0.1\n7.5\n24.0\n9.0811\n40.5811\n0.7\n0.766\n0.086\n\n\n5\n0.1\n6.5\n30.0\n7.0631\n43.5631\n0.6\n0.675\n0.091\n\n\n3\n0.1\n2.5\n30.0\n11.0991\n43.5991\n0.5\n0.579\n0.096\n\n\n6\n0.1\n2.5\n40.5\n8.0721\n51.0721\n0.4\n0.479\n0.101\n\n\n8\n0.1\n13.0\n28.5\n10.0901\n51.5901\n0.3\n0.373\n0.106\n\n\n4\n0.1\n3.5\n49.5\n0\n53.0000\n0.2\n0.261\n0.112\n\n\n9\n0.1\n8.5\n12.0\n40.3604\n60.8604\n0.1\n0.140\n0.121\n\n\n10\n0.1\n8.0\n30.0\n64.5766\n102.5766\n0\n0.000\n0.140\n\n\n\n\n\n\n\n\n\nTable 5.8: InsCo model rerun, EVA and capital solutions.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\nScale change \\(\\epsilon^i\\)\n-0.5000\n0.5000\n0.0090\n\n\n\nPlan premium \\(P_P\\)\n6.9500\n28.0500\n19.7766\n54.7766\n\n\nRequired prem \\(P_R\\)\n6.8115\n28.6040\n20.8271\n56.2426\n\n\nEVA \\(V=P_P-P_R\\)\n0.1385\n-0.5540\n-1.0505\n-1.4660\n\n\nRequired assets \\(a\\)\n8.0000\n30.0000\n64.5766\n102.5766\n\n\nPlan capital \\(Q_P=a-P_P\\)\n1.0500\n1.9500\n44.8000\n47.8000\n\n\nEVA/capital ratio \\(r=V/Q_P\\)\n0.1319\n-0.2841\n-0.0234\n\n\n\n\n\n\n\nThe final EVA of \\(-1.466\\) is not only not as good as the predicted \\(-1.240\\); it is worse than the original \\(-1.365\\). Such is the curse of nonlinearity. Studying the derivatives here, it seems we need to go backwards, increasing the scale of Unit A and decreasing Unit B.\nIf we run the model with Unit B rescaled at many points throughout the \\([-0.5, 0.5]\\) range, we obtain the results shown in Figure 5.4. Nonlinearity is obvious. This is why most optimization algorithms take relatively small steps when updating parameters.\n\n\n\n\n\n\nFigure 5.4: EVA at –50% Unit A, various Unit B.\n\n\n\nApplying the Nelder-Mead Simplex algorithm, we obtain the surprising result that the scale constraints are not binding at the optimal solution. Unit A is to be reduced by almost, but not quite, 50%. Unit B is to be reduced by almost 21%. Details are in Table 5.9.\n\n\n\nTable 5.9: InsCo model rerun, EVA and capital solutions.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\nScale \\(\\epsilon^i\\)\n-0.478165\n-0.209790\n0.028758\n\n\n\nPlan premium \\(P_P\\)\n7.253507\n14.776927\n20.163665\n42.194098\n\n\nRequired prem \\(P_R\\)\n7.269164\n14.816395\n21.453658\n43.539217\n\n\nEVA \\(V=P_P-P_R\\)\n-0.015658\n-0.039468\n-1.289993\n-1.345119\n\n\nRequired assets \\(a\\)\n8.349360\n15.804200\n65.840538\n89.994098\n\n\nPlan capital \\(Q_P=a-P_P\\)\n1.095854\n1.027273\n45.676874\n47.800000\n\n\nEVA/capital ratio \\(r=V/Q_P\\)\n-0.014288\n-0.038420\n-0.028242\n-0.080950\n\n\n\n\n\n\nA grid search shows the situation in the neighborhood of the solution. See Figure 5.5. The optimal point has an EVA value of \\(-1.345\\). The innermost contour represents an EVA level set of \\(-1.350\\). Parameters inside this contour get at least 75% of the way from the original EVA to the optimal EVA.\n\n\n\n\n\n\nFigure 5.5: Contours of EVA at various Unit A, Unit B.\n\n\n\nThe solution suggests that Unit A and Unit B both need further reduction because of negative EVA. However, there is more going on here. Not only is there nonlinearity, there is discontinuity in first derivatives as evidenced by Figure 5.4. If we were to change the Unit B scale ever so slightly, from \\(-0.20979\\) to \\(-0.209795\\), we obtain dramatically different EVA allocations, Table 5.10.\n\n\n\nTable 5.10: InsCo model rerun, EVA and capital solutions.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\nScale \\(\\epsilon^i\\)\n-0.478165\n-0.209795\n0.028759\n-0.659201\n\n\nPlan premium \\(P_P\\)\n7.253507\n14.776834\n20.163668\n42.194008\n\n\nRequired prem \\(P_R\\)\n7.334042\n14.607535\n21.597550\n43.539127\n\n\nEVA \\(V=P_P-P_R\\)\n-0.080535\n0.169298\n-1.433882\n-1.345119\n\n\nRequired assets \\(a\\)\n8.349360\n15.804100\n65.840548\n89.994008\n\n\nPlan capital \\(Q_P=a-P_P\\)\n1.095854\n1.027267\n45.676880\n47.800000\n\n\nEVA/capital ratio \\(r=V/Q_P\\)\n-0.073491\n0.164805\n-0.031392\n0.059922\n\n\n\n\n\n\n\n\n\n5.5.3 Summary\nThe term “portfolio optimization” is quite ambitious. There are some questionable assumptions that have to be imposed on the problem to get to a solution. There are inherent nonlinearities and discontinuous derivatives. The uncertainties (see Section 8.1) in the inputs and the underlying model itself no doubt add breadth to the range of near-optimal solutions. One should approach the task with a healthy dose of humility.\nPerhaps the best that can be hoped for is to provide directions for portfolio improvement that are probably, approximately, correct.\n\n\n\n\n\n\nFriedman, J, T Hastie, and R Tibshirani. 2008. The elements of statistical learning. Springer.\n\n\nKoenker, Roger, and Kevin F Hallock. 2001. “Quantile Regression.” Journal of Economic Perspectives 15 (4): 143–56.\n\n\nKovvali, Narayan. 2022. Theory and Applications of Gaussian Quadrature Methods. Springer Nature.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.\n\n\nPostek, Krzysztof, Alessandro Zocca, Joaquim Gromicho, and Jeffrey Kantor. 2024a. “Companion Jupyter Book for “Hands-On Mathematical Optimization with Python’’.” GitHub.\n\n\nPostek, Krzysztof, Alessandro Zocca, Joaquim Gromicho, and Jeffrey Kantor. 2024b. Hands-On Mathematical Optimization with Python. Cambridge University Press.\n\n\nWand, Matt P, and M Chris Jones. 1994. Kernel Smoothing. CRC press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications of pricing and allocation</span>"
    ]
  },
  {
    "objectID": "060_SelectingSRM.html",
    "href": "060_SelectingSRM.html",
    "title": "6  Selecting and calibrating an SRM",
    "section": "",
    "text": "6.1 The envelope of possibilities and the five representatives\nAssume we have a can opener.\n- Unknown\nIn Chapter 1, we saw that the overall portfolio profitability target—whether stated as total premium, margin, or return on capital—is typically given as an input. In Chapter 4, we saw the crucial importance of the distortion function in determining how that profit requirement is distributed to portfolio units. This chapter discusses how to develop a suitable distortion function.\nTVaR is an SRM. The distortion function corresponding to \\(\\mathsf{TVaR}_p\\) goes linearly from \\(g(0)=0\\) to \\(g(1-p)=1\\) and then stays at 1 for \\(s&gt;1-p\\), see Figure 4.3. A weighted combination of SRMs with positive weights summing to one (i.e., a convex combination) is also an SRM. In particular, we call such a weighted sum of two TVaRs a bi-TVaR.\nExercise: Derive an explicit expression for the distortion function associated with the bi-TVaR \\(\\theta\\mathsf{TVaR}_{p_0}+(1-\\theta)\\mathsf{TVaR}_{p_1}\\).\nFor any portfolio (i.e., loss random variable) \\(X\\), there are two extreme values at which SRMs can price it. At one extreme, \\(\\mathsf{TVaR}_0(X)=\\mathsf E[X]\\) is the minimum possible price; at the other \\(\\mathsf{TVaR}_1(X)=\\max(X)\\) is the maximum.\nExercise: Prove that for any price \\(P\\) between \\(\\mathsf E[X]\\) and \\(\\max(X)\\), there are infinitely many SRMS \\(\\rho\\) with \\(\\rho(X)=P\\).\nSolution: Because \\(\\mathsf{TVaR}_p(X)\\) is continuous in \\(p\\), the intermediate value theorem implies that there is a \\(p^\\star\\) with \\(\\mathsf{TVaR}_{p^\\star}(X)=P\\). Similarly, because \\(\\rho_\\theta(X) \\equiv \\theta\\mathsf{TVaR}_{0}+(1-\\theta)\\mathsf{TVaR}_{1}\\) is continuous in \\(\\theta\\), there is a \\(\\theta^\\star\\) with \\(\\rho_{\\theta^\\star}(X)=P\\). The first is a TVaR and the second is a bi-TVaR, so they are distinct. Any convex combination of the two—and there are infinitely many of them—will also price \\(X\\) as \\(P\\).\nThere are more than that, however. The same logic applies to any bi-TVaR \\(\\theta\\mathsf{TVaR}_{p_0}+(1-\\theta)\\mathsf{TVaR}_{p_1}\\) with \\(p_0 &lt; p^\\star &lt; p_1\\); there is a \\(\\theta^\\star\\) pricing it at \\(P\\). There are infinitely many of those. Combinations of those also work. Combinations of those combinations also work. If we pass to the limit and construct infinite combinations, we get SRMs with smooth curves for distortion functions like the Wang transform.\nSo there are many, many SRMs \\(\\rho\\) that have \\(\\rho(X)=P\\). But applied to a particular unit (via NA), they are unlikely to all give the same price to the unit. It would be useful to be able to explore the limits of high and low unit prices consistent with the given portfolio price. Mildenhall and Major (2022) gives an in-depth theoretical explanation, with section 11.2 detailing the construction. Fortunately, the five representative distortions introduced in Section 4.6 do a good job of spanning the space of possibilities from body- (volatility-)centric to tail- (extreme risk-)centric and so in this monograph, we will consider only them.\nA warning, however: unless one distortion \\(g\\) dominates another \\(h\\) in the sense that \\(g(s) \\ge h(s)\\) for all \\(s\\) with strict inequality for some \\(s\\), then there are risks \\(X\\) and \\(Y\\) such that \\(\\rho_g(X)&gt;\\rho_g(Y)\\) and \\(\\rho_h(X)&lt;\\rho_h(Y)\\), i.e., \\(g\\) and \\(h\\) disagree about the relative price (riskiness) of \\(X\\) and \\(Y\\). That is, it is not generally possible to put distortion functions in an unambiguous order by implied price across all risks.\nExercise. Verify that CCoC in Table 4.7 is the bi-TVaR between \\(\\mathsf{TVaR}_1\\) and \\(\\mathsf{TVaR}_0\\) with \\(\\theta=0.87\\).\nSolution. Table 6.1 shows the \\(q\\) values corresponding to \\(\\mathsf{TVaR}_0=\\mathsf E[X]\\) and \\(\\mathsf{TVaR}_1=\\max(X)\\). The first consists of \\(q=p\\), and the second consists of \\(q=1\\) for the top event only, zero elsewhere.\nTable 6.1: \\(q\\) values for \\(\\mathsf{TVaR}_0=\\mathsf E[X]\\), \\(\\mathsf{TVaR}_1=\\max(X)\\) and the \\(\\theta=0.87\\) blend.\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(p\\)\n\\(S\\)\n\\(\\mathsf{TVaR}_0\\)\n\\(\\mathsf{TVaR}_1\\)\nBlend\n\n\n\n\n0\n0\n1\n0\n0\n0\n\n\n1\n0.1\n0.9\n0.1\n0\n0.087\n\n\n2\n0.1\n0.8\n0.1\n0\n0.087\n\n\n3\n0.1\n0.7\n0.1\n0\n0.087\n\n\n4\n0.4\n0.3\n0.4\n0\n0.348\n\n\n5\n0.1\n0.2\n0.1\n0\n0.087\n\n\n6\n0.1\n0.1\n0.1\n0\n0.087\n\n\n7\n0.1\n0\n0.1\n1\n0.217\nThe final column, a \\(1:0.15\\) blend of the previous two, matches the CCoC column in Table 4.7.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Selecting and calibrating an SRM</span>"
    ]
  },
  {
    "objectID": "060_SelectingSRM.html#required-premium-by-distortion",
    "href": "060_SelectingSRM.html#required-premium-by-distortion",
    "title": "6  Selecting and calibrating an SRM",
    "section": "6.2 Required premium by distortion",
    "text": "6.2 Required premium by distortion\nTable 6.2 shows required premiums and loss ratios for each unit, by distortion function. Remember, required here means required to meet the internally specified 15% return on capital hurdle. Also shown are plan premiums and loss ratios.\n\n\n\n\nTable 6.2: Example units with pricing by distortion.\n\n\n\n\n\n\n\n\n\n\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\nView\nPremium\nLoss ratio\nPremium\nLoss ratio\nPremium\nLoss ratio\nPremium\nLoss ratio\n\n\n\n\nccoc\n13.739\n97.5%\n18.522\n98.8%\n21.304\n69.9%\n53.565\n87.0%\n\n\ndual\n14.127\n94.9%\n19.117\n95.7%\n20.321\n73.3%\n53.565\n87.0%\n\n\nph\n14.060\n95.3%\n18.349\n99.7%\n21.156\n70.4%\n53.565\n87.0%\n\n\ntvar\n13.783\n97.2%\n20.412\n89.7%\n19.371\n76.9%\n53.565\n87.0%\n\n\nwang\n14.109\n95.0%\n18.637\n98.2%\n20.818\n71.6%\n53.565\n87.0%\n\n\nPlan\n13.9\n96.4%\n18.7\n97.9%\n19.6\n76.0%\n52.2\n89.3%\n\n\n\n\n\n\n\n\n\n\nRelative to model benchmarks, the entire portfolio is underpriced by \\(53.565-52.2=1.365\\). But which units need to do better, and by how much?\n\n\n\n\n\nTable 6.3: Range of implied loss ratios and cheapest distortion by unit.\n\n\n\n\n\n\n\n\n\nUnit\nccoc\nph\nwang\ndual\ntvar\nPlan\nCheapest\n\n\n\n\nUnit A\n97.5%\n95.3%\n95.0%\n94.9%\n97.2%\n96.4%\nccoc\n\n\nUnit B\n98.8%\n99.7%\n98.2%\n95.7%\n89.7%\n97.9%\nph\n\n\nUnit C\n69.9%\n70.4%\n71.6%\n73.3%\n76.9%\n76.0%\ntvar\n\n\n\n\n\n\n\n\n\nTable 6.3 shows the plan loss ratio, the range of target loss ratios generated by the five distortions, and which distortion suggests the lowest premium for each unit. A higher required loss ratio implies a lower margin and premium. Unit management may regard a lower premium as better because it makes it easier to sell policies in the marketplace. The table shows that for each unit there is at least one distortion whose pricing it can meet, because the plan loss ratio is no greater than the distortion indicated loss ratio. For example, Unit B’s management would be happy to target the PH-indicated 0.997 loss ratio instead of the current plan’s 0.979. However, there is no single distortion whose pricing all units can meet. How should the modeler proceed? A monograph with two authors can offer two paths.\nBefore discussing the two paths, it is worthwhile reflecting on how a market distortion emerges. Insurance is fundamentally risk sharing. The market consists of individuals with different endowments, owning different risks, and understood to have their own risk appetites. Each individual can cede risk into the pool and assume risk from it as a shareholder or mutual company owner. Typically there are more of the former than the latter. We assume appetites for insurable, diversifiable risk are given by distortion risk measures. The appetite for investment risk, with its systematic components, may be different. The efficient insurance market solution is for everyone to pool their risks together, for maximal diversification benefit, and then to layer out the total risk to the individual offering the cheapest price for it in each layer. This corresponds to looking at the minimum of the individual distortion functions. Figure 4.3 could correspond to a market of five individuals. In it, the TVaR individual assumes large risks (above about the 75th percentile, lower left) and CCoC the rest. TVaR is risk neutral in the tail, and equity financing is well known to be an effective solution for nicely diversifiable risk. This theoretical model is described in a famous paper Jouini et al. (2008). There are many steps between a simple model and reality, but it is a good picture to bear in mind for the rest of this chapter. Readers who work for brokers may be happy to learn the model implies an important role for them matching risk to its cheapest financing. Finally, notice the model implies the risk appetite that matters is that of investors, not management—despite management often talking about “their risk appetite.”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Selecting and calibrating an SRM</span>"
    ]
  },
  {
    "objectID": "060_SelectingSRM.html#zeroing-in-on-a-distortion-function",
    "href": "060_SelectingSRM.html#zeroing-in-on-a-distortion-function",
    "title": "6  Selecting and calibrating an SRM",
    "section": "6.3 Zeroing in on a distortion function",
    "text": "6.3 Zeroing in on a distortion function\nPath 1 holds there is a best distortion function that should be discoverable by the modeler. On this path, the modeler must avoid excessive naval gazing. Present the facts, being the range of indications from Table 6.2 with relevant commentary, but recognize the decision is for unit and corporate management to negotiate. As always, Table 6.2 hides many details and assumptions. What is the historical performance of each unit compared to prior plans? The growth prospects? The quality of the data and certainty in the simulations? Again, present facts and avoid selling against your model.\nIn an ideal world, we choose our distortion based on first principles and recommend each unit meets the target specified by that principle. A core principle of modern finance and accounting is to calibrate models to market observables where possible. Is the distortion consistent with cat bond pricing for extreme events? What about InsCo’s capital structure? A discussion with the finance department might be in order, to ask: What is the structure of the capital supporting the business and what are the costs of the various components of capital? For example, InsCo may have a Baa-rated corporate bond with initial annual default probability between 1.5% and 2% and a valuation equating to a risk-neutral default probability between 2.5% and 4%. These correspond to the exceedance probability \\(s\\) and distorted probability \\(g(s)\\), respectively, of a distortion function. Which calibrated distortions are consistent with these findings? A quick analysis reveals that the Dual and Wang distortions are the most likely candidates, with TVaR coming in a distant third place.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Selecting and calibrating an SRM</span>"
    ]
  },
  {
    "objectID": "060_SelectingSRM.html#embracing-the-range",
    "href": "060_SelectingSRM.html#embracing-the-range",
    "title": "6  Selecting and calibrating an SRM",
    "section": "6.4 Embracing the range",
    "text": "6.4 Embracing the range\nPath 2 holds that searching for a single best distortion is an unanswerable metaphysical quest, but it suggests there is much to learn from the range of indications. While no choice is correct, some are logically inconsistent and should be avoided.\nThe choice of a distortion corresponds to a statement of risk appetite. The ordering CCoC, PH, Wang, Dual, and TVaR we use to show the five representative distortions ranks them from most concerned about tail (extreme VaR) losses to most concerned about attritional (near plan) losses. The CCoC, driven by the maximum loss is obviously the most tail-centric. Conversely, TVaR is the most attritional loss averse. This ranking corresponds to the heights of the distortion functions plotted in Figure 4.3; remember that left on the horizontal axis is extreme losses. How does management talk about risk, for example, in quarterly analyst calls for stock companies? Often, their statements reveal they understand their investors are more concerned with attritional volatility. After a large catastrophe event, the market expects all players to have large losses, whereas a miss on attritional volatility suggests management is not pricing correctly and can cause disquiet among investors. Managements prodded by their investors to be more concerned with volatility typically find targets driven by TVaR or the Dual distortion to better align with their intuitions.\nThe range of implications across different distortions turns out to overlap with the range of the reinsurance pricing cycle, so these are not just academic considerations. For example, in Table 9.10, the model recommended maximum buy-price for reinsurance is 7.6 (a 46% loss ratio or higher) under CCoC but only 4.8 (73% loss ratio) under TVaR. Over a market cycle the loss ratio for catastrophe reinsurance can vary substantially, and whereas a CCoC view may recommend a buy in all markets, TVaR could be more circumspect. The authors have both worked with managements who have expressed exactly these concerns and who have disliked a CCoC-driven, industry-standard approach analysis as a consequence.\nKnowing the range of indications means you can determine whether a plan loss ratio is consistent with some risk appetite (it is within the range). For our simple model, Table 6.3 shows they all are, but that is not always the case. A unit with plan outside the range can be criticized because it is inconsistent with any risk appetite; it should be investigated accordingly. A plan far outside the range needs especial scrutiny.\n\n\n\n\n\n\nJouini, E., W. Schachermayer, and N. Touzi. 2008. “Optimal risk sharing for law invariant monetary utility functions.” Mathematical Finance 18 (2): 269–92. https://doi.org/10.1111/j.1467-9965.2007.00332.x.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Selecting and calibrating an SRM</span>"
    ]
  },
  {
    "objectID": "065_Evaluation.html",
    "href": "065_Evaluation.html",
    "title": "7  Evaluating models",
    "section": "",
    "text": "7.1 Evaluating your model\nThe trouble with most of us is that we would rather be ruined by praise than saved by criticism.\n- Norman Vincent Peale\nThis chapter encompasses three distinct tasks:\nThere are several phases to evaluating your own model.\nFirst, does it meet its design specifications in terms of features and functionality? You will refer back to the requirements document you prepared per Section 1.3. (You did prepare a formal document, didn’t you?) This serves as a checklist as you review the features of your model.\nSecond, like any software development project, you need to run a battery of tests to verify it is behaving—especially calculating—correctly. If you built the model from the ground up, you might have created tests concurrently with module development. Use those, and add some more.\nThird is validation: does the model produce realistic results? Whereas the first two phases apply to all the modules listed in Section 1.5, the third applies only to the Business Operations module. Validation might consist of expert judgment—but beware echo-chamber effects. Ideally, however, you have access to historical data that can be used to run the model as if it had existed in times past. Then, its outputs can be compared with what really did come to pass. Components of validation include:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluating models</span>"
    ]
  },
  {
    "objectID": "065_Evaluation.html#sec-eval-own-model",
    "href": "065_Evaluation.html#sec-eval-own-model",
    "title": "7  Evaluating models",
    "section": "",
    "text": "Best Estimates: With inputs corresponding to the plan, do central values (means or medians) of key variables (cash flows and derived accounting values) calibrate to the plan? Do the output distributions seem reasonable?\nVariability: Do basic statistics (coefficient of variation, skewness, quantiles, etc.) of outputs seem reasonable? Are you getting what you expect? Are they consistent with historical (your own or industry) experience?\nBack testing: If historical replays are possible, compute where the historical values lie in terms of percentiles of the output distributions, i.e., \\(p\\) values. The \\(p\\) values should reasonably be draws from uniform distributions. They should not cluster together at the low or high end—this indicates bias. They should not clump together in the middle—this indicates that historical variation is low compared to the model. Nor should they create a void in the middle—this indicates too much variation. Be careful not to overthink this, however. With only a handful of observations, only egregiously bad results can be considered statistically significant.\nRepeat: As the model gets used, repeat back testing regularly (quarterly or annually). Build a growing library of \\(p\\) values to demonstrate that model ranges are wide enough and symmetrical.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluating models</span>"
    ]
  },
  {
    "objectID": "065_Evaluation.html#sec-eval-3rd-party",
    "href": "065_Evaluation.html#sec-eval-3rd-party",
    "title": "7  Evaluating models",
    "section": "7.2 Evaluating third-party models",
    "text": "7.2 Evaluating third-party models\nThird-party models, if they include Business Operations simulation, should be evaluated—to the extent possible—in the manner described in Section 7.1.\nSome third-party models, like minimum capital requirements created and used by regulators and rating agencies, do not simulate business outcomes but rather proceed directly from business characteristics as input to required capital as output. You may want to use such a model in one of several ways:\n\nIt is your only model; all you want to know is required capital. Unfortunately, this may not give you a clear path to allocating cost of capital and required premiums. However, it is a start if you have no capital model at all. Apply the non-Business Operations evaluation steps outlined in Section 7.1.\nYou intend to run it in parallel with your own model and compare results. Evaluate as in the previous bullet.\nYou intend to integrate it with your own model. This can mean several things.\n\nIntegrated reporting but otherwise separate calculations can be treated as previously but with additional testing to ensure the reporting is working correctly.\nIf the third-party model is to function as the Capital Adequacy module, then careful testing and review is needed to determine whether its outputs will be consistent with the workings of the Business Operations and Pricing and Allocation modules. For example, if a change to the portfolio moves the 95% and 99% loss quantiles upwards but the required capital goes down, then you might rethink the wisdom of relying solely on the third-party model. If it has capital allocation functionality, is it implementing something like CCoC? Can it be modified to perform an NA, or can that part be bypassed in favor of a different Pricing and Allocation module?\n\n\nSome companies manage to (or are at least acutely aware of) their binding rating agency’s or regulator’s view of capital. You may be asked the point of an internal model, given this binding view. The answer should go back to the charter for the internal model—after all, it is not a surprise that an external model is binding. What additional value was anticipated from an internal model before you started building? Reconsider why you are building an internal model if you cannot answer this question ahead of time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluating models</span>"
    ]
  },
  {
    "objectID": "065_Evaluation.html#sec-eval-platform",
    "href": "065_Evaluation.html#sec-eval-platform",
    "title": "7  Evaluating models",
    "section": "7.3 Evaluating modeling platforms",
    "text": "7.3 Evaluating modeling platforms\nModel building platforms range from programming languages with integrated development environments to heavily graphical user interface-based wiring diagram applications. What they have in common is that they are not ready-to-run models. How they differ is the extent to which they offer prefabricated computational modules and the support they offer for translating specifications into a model. Some programming languages, like R or Python, have many open-source libraries available to the public for free.\nThe first question to be addressed is: can the platform be used to build the model you have specified in Section 1.3? The second, trickier question, is: how difficult will that be? Of course, budget limitations need also be taken into account. However, you may find a trade-off between direct expense to acquire the platform and labor expense to build the model with it.\nHere are some items to consider when evaluating a modeling platform:\n\nModeling\n\nWhat accounting bases are supported for financial computations?\nHow can loss payout patterns be modeled?\nHow can assets be modeled (e.g., sweep accounts with proportionate investment strategies)?\nHow can dependency be modeled (e.g., Iman-Conover shuffling, copulas, other)?\nWhat options are available for defining required capital?\nWhat options are available for defining and allocating required margins?\n\nParameters\n\nWhat parameters are explicitly variable and which are implicit and “hard-coded”?\nHow are parameters stored? Can they easily be changed from one run to another?\nIs there an explicit provision for uncertainty in parameterization? (See also Section 8.1.)\n\nInput and output\n\nWhat formats/databases are supported for data input? Report output? Simulation details?\nWhat are the report generation options?\nWhat tabular output options are there?\nWhat graphical output options are there?\n\nIntegration\n\nHow is integration with Excel handled?\nIs integration with a separate Economic Scenario Generator possible?\nHow is integration with commercial catastrophe models handled?\n\nRunning\n\nAre automatic multi-assumption model runs supported?\nIs there scripting? In what language?\n\nSystem issues\n\nHow are version control and model security handled?\nWhat are the hardware requirements (one PC, cluster, cloud)?\nIs there a front-end app or web interface?\nIs there an Application Programming Interface (API)?\n\nTrack record\n\nAre there models in production used for the London market, Solvency II, and Swiss Solvency Test?\nWhat is the typical size of a client model (lines, year, assets, etc.) and what is the corresponding simulation run time?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Evaluating models</span>"
    ]
  },
  {
    "objectID": "070_AdvancedTopics.html",
    "href": "070_AdvancedTopics.html",
    "title": "8  Advanced topics",
    "section": "",
    "text": "8.1 Dealing with uncertainty\nIn this world, nothing is certain except death and taxes.\n- Benjamin Franklin, letter to Jean-Baptiste Leroy\nThe purpose of simulation is to estimate certain quantities of interest or QoI. A QoI is typically a quantile or other function (e.g., mean, risk measure) of the sample outcomes that the simulation created. Since it is being estimated from a sample, it cannot be measured with perfect accuracy because of sampling error. The more simulated samples are created, the more accurate the estimate can be. However, there is another reason that the estimate will not be accurate.\nParameter uncertainty here is to be distinguished from process risk. For example, when you simulate the outcome from a random variable with a lognormal distribution with particular parameters, you are modeling process risk. When you aren’t sure what the parameter values should be, you are suffering from parameter uncertainty. The uncertainty around input parameters will propagate to uncertainty about the QoI. For example, it is well known that ignoring parameter uncertainty often systematically biases estimated quantiles downward. See Jewson et al. (2025) for an approach to adjust for this bias.\nParameter uncertainty arises from nonsampling error. There are many potential sources of this in a capital model. Data used to build the model could have erroneous entries, or it might not be of an appropriate vintage or refer to a population that is sufficiently relevant to the modeling task. In building the model, perhaps not all relevant factors have been taken into account. This is not an exhaustive list!\nIn addition, some parameters vary over time. A data snapshot taken to estimate parameters may be inappropriate to modeling the same phenomenon at a later date.\nParameter uncertainty will likely permeate your modeling efforts, especially if you are trying to make a model that can replicate historical experience. There are several ways to deal with uncertainty:\nFor a general treatment, see Smith (2024).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced topics</span>"
    ]
  },
  {
    "objectID": "070_AdvancedTopics.html#sec-Uncertainty",
    "href": "070_AdvancedTopics.html#sec-Uncertainty",
    "title": "8  Advanced topics",
    "section": "",
    "text": "Ignore it. Use a best estimate of parameter values and let it go at that. While this may be acceptable for early versions of the model, it is not recommended for the long term. At the very least you must disclose that there is some uncertainty as to the QoI values and what the business risks of that are. “The answer is \\(A\\), but there is some uncertainty.”\nReport it. Statistical analysis of data will usually give some indication of the range (e.g., variance) of parameter values consistent with a chosen model for the data. If you want to push your luck, that insight can be turned into a distributional model for the parameters. Rerun your model with carefully chosen exemplars or a sample from the putative distribution of those random parameters. Report the range or the resulting distribution of the QoI. “The answer is \\(A \\pm \\epsilon\\).”\nAbsorb it. Classical (frequentist) statistics has prediction intervals to accommodate uncertainty in the parameters; Bayesian theory provides a formal framework for treating parameter uncertainty as another species of process risk. Essentially, this means treating the random parameter as mixing the distribution of QoI, creating what Bayesians call a predictive distribution; see Bolstad and Curran (2016) or Geisser (2017). This is explained in an actuarial context in Kreps (1997); however, Major (1999) finds reasons to doubt it is always the right approach. Prediction intervals are bigger than classically estimated intervals. The QoI is likely to be sensitive to the upper tail, which moves up. “The answer is \\(A+\\delta\\).”\nIsolate it. Find a credible worst-case to offer as an example of what can go wrong. This is the approach covered in Hansen and Sargent (2008) and Major et al. (2015). Again, this is likely to increase a risk-sensitive QoI. “The answer is \\(A\\), but it could be as bad as \\(A+\\gamma\\).”",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced topics</span>"
    ]
  },
  {
    "objectID": "070_AdvancedTopics.html#sec-iman-conover",
    "href": "070_AdvancedTopics.html#sec-iman-conover",
    "title": "8  Advanced topics",
    "section": "8.2 The Iman-Conover method",
    "text": "8.2 The Iman-Conover method\nCorrelation is not causation, but you can cause some correlation.\nThe Iman-Conover method is an easy way to induce correlation between marginal distributions. Here is the idea. Given samples of \\(n\\) values from \\(r\\) known marginal distributions \\(X_1,\\dots,X_r\\) and a desired \\(r\\times r\\) (linear) correlation matrix \\(\\Sigma\\) between them, reorder the \\(i\\)th sample to have the same rank order as the \\(i\\)th column of a reference distribution of size \\(n\\times r\\) with linear correlation \\(\\Sigma\\). The reordered output will then have the same rank correlation as the reference, and since linear correlation and rank correlation are typically close, it will have close to the desired correlation structure. What makes the Iman-Conover method work so effectively is the existence of easy algorithms to determine samples from reference distributions with prescribed correlation structures.\nThe Iman-Conover method is very powerful, quick, and easy to use. However, it is not magic, and it is constrained by mathematical reality. For given pairs of distributions there is often an upper bound on the linear correlation between them, so if you ask for very high correlation you may be disappointed. You should go back and ask yourself why you think the linear correlation is so high.\nIman-Conover is usually used to induce a desired linear correlation. There other measures, such as rank correlation and Kendall’s tau, that could be used (Mildenhall 2005) and the user should reflect on which is most appropriate to their application. If your target is rank correlation, Iman-Conover will give you an exact match.\nThe aggregate documentation provides a detailed description of the algorithm, including discussion of using different reference distributions (called scores) in place of the normal, and a comparison with the normal copula method.\n\n8.2.1 Loss or loss ratio?\nAre you modeling correlation of loss ratios or correlation of losses? It is a surprising empirical fact that the former is often greater than the latter, leading to the conclusion that correlation in underwriting results is driven by correlation in premium more than loss, see Mildenhall (n.d.). These findings highlight a stark property-casualty split. Property tends to be loss-volatility driven, with loss correlation caused (and explained) by geographic proximity. Casualty tends to be premium-volatility driven, with loss ratio correlation caused by (account) underwriting and the underwriting cycle.\n\n\n8.2.2 Example 1: A simple example\nThis section presents a simple example applying Iman-Conover to realistic distributions, using the aggregate library. It shows how easy it is to use Iman-Conover in modern programming languages. Another example, with more the details, appears in the aggregate documentation. See Section 9.3 for instructions to set up your Python environment to replicate the code samples. The full source code to replicate the examples is available on the Monograph website.\nThe example works with \\(r=3\\) different marginals: Auto, general liability (GL), and Property. Losses are simulated using frequency-severity aggregate distributions with a lognormal severity and per occurrence limits. Frequency is a gamma-mixed Poisson (negative binomial) with varying CVs. See Mildenhall (2023) for a description of the aggregate DecL language, used to specify these portfolios.\nport ICExample\n    agg Auto      5000 loss  2000 xs 0 sev lognorm  30 cv 2 mixed gamma 0.2\n    agg GL        2000 loss  5000 xs 0 sev lognorm  50 cv 5 mixed gamma 0.3\n    agg Property  7000 loss 10000 xs 0 sev lognorm 100 cv 5 mixed gamma 0.1\nThis example uses only 1,000 simulations, but the software can easily handle 100,000 or more. The slowest part of the Iman-Conover algorithm is creating the scores. These are cached, so subsequent runs are very fast, even if the desired correlation matrix changes.\nTable 8.1 shows summary statistics and the first few samples of the underlying multivariate distribution, and Figure 8.1 shows histograms of each marginal and scatterplots. By construction, the units are approximately independent.\n\n\n\nTable 8.1: Set up and summary statistics of sample for Iman-Conover example using 1,000 samples.\n\n\n\n\n\n\n\n\n\n\n(a) Summary statistics.\n\n\n\n\n\nindex\nAuto\nGL\nProperty\n\n\n\n\ncount\n1,000\n1,000\n1,000\n\n\nmean\n5007.1\n2008.0\n7106.6\n\n\nstd\n1375.3\n1398.9\n3266.2\n\n\nmin\n1,588\n195\n1,244\n\n\n25%\n4048.2\n1,085\n4842.5\n\n\n50%\n4,922\n1,671\n6380.5\n\n\n75%\n5,898\n2,528\n8,573\n\n\nmax\n12,229\n15,100\n23,390\n\n\ncv\n0.275\n0.697\n0.460\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) First 10 samples.\n\n\n\n\n\nindex\nAuto\nGL\nProperty\n\n\n\n\n0\n6,158\n1,694\n4,362\n\n\n1\n5,714\n2,079\n6,258\n\n\n2\n5,165\n2,771\n3,298\n\n\n3\n4,377\n1,386\n5,860\n\n\n4\n5,839\n994\n11,687\n\n\n5\n4,380\n2,881\n7,925\n\n\n6\n4,504\n2,087\n8,936\n\n\n7\n7,699\n907\n4,746\n\n\n8\n3,015\n4,440\n15,814\n\n\n9\n4,167\n890\n12,251\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.1: Uncorrelated marginals input to IC method. Diagonal shows univariate histograms.\n\n\n\n\n\nIn addition to marginal distributions, Iman-Conover requires a target desired correlation matrix as input. This is produced by a separate analysis, and here we assume it is given by Table 8.2. Remember, a correlation matrix must be symmetric (easy to check) and positive definite (harder). Matrices computed from statistical analysis may fail to be positive definite if they are not computed from a multivariate sample. Iman-Conover fails if given a nonpositive definite matrix.\n\n\n\n\nTable 8.2: Target correlation matrix.\n\n\n\n\n\n\n\n\n\nindex\nAuto\nGL\nProperty\n\n\n\n\nAuto\n1\n-0.3\n0\n\n\nGL\n-0.3\n1\n0.8\n\n\nProperty\n0\n0.8\n1\n\n\n\n\n\n\n\n\n\nThe Iman-Conover method is implemented in aggregate by the function iman_conover, so the transformation is simply\ndf2 = iman_conover(df, desired)\nwhere desired is the desired correlation matrix. Table 8.3 shows the achieved linear and rank correlation, and the absolute and relative errors. Finally, Figure 8.2 provides a scatter plot of the shuffled inputs, making the correlation evident. It is possible to obtain a correlated sample from a Portfolio object by passing in a correlation matrix: the code below extracts a sample and automatically applies Iman-Conover.\ndf = port.sample(n, desired_correlation=desired, keep_total=False)\n\n\n\nTable 8.3: Achieved correlation matrix and output errors with Iman-Conover method.\n\n\n\n\n\n\n\n\n\n\n(a) Linear correlation\n\n\n\n\n\nindex\nAuto\nGL\nProperty\n\n\n\n\nAuto\n1\n-0.265\n-0.001\n\n\nGL\n-0.265\n1\n0.770\n\n\nProperty\n-0.001\n0.770\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Rank correlation\n\n\n\n\n\nindex\nAuto\nGL\nProperty\n\n\n\n\nAuto\n1\n-0.282\n0.009\n\n\nGL\n-0.282\n1\n0.778\n\n\nProperty\n0.009\n0.778\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Absolute error\n\n\n\n\n\nindex\nAuto\nGL\nProperty\n\n\n\n\nAuto\n0\n0.035\n0.001\n\n\nGL\n0.035\n0\n0.030\n\n\nProperty\n0.001\n0.030\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Relative error\n\n\n\n\n\nindex\nAuto\nGL\nProperty\n\n\n\n\nAuto\n0\n-0.117\n-inf\n\n\nGL\n-0.117\n0\n-0.037\n\n\nProperty\n-inf\n-0.037\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Correlated marginals output by the Iman-Conover method. Diagonal shows unchanged univariate histograms, but the scatterplots clearly show the introduction of correlation.\n\n\n\n\n\n\n\n8.2.3 Different reference distributions\nThe Iman-Conover algorithm relies on a sample from a multivariate reference distribution called scores. A straightforward method to compute a reference is to use the Choleski decomposition method applied to certain independent scores (samples with mean zero and standard deviation one). The standard algorithm uses standard normal scores. However, nothing prevents the use of other distributions for the scores, provided they are suitably normalized to have mean zero and standard deviation one. Several other families of multivariate distributions could be used, such as elliptically contoured distributions (which include the normal and \\(t\\) as a special cases) and the multivariate Laplace distribution. These are all easy to simulate.\nThe aggregate implementation provides \\(t\\)-distribution scores. It includes an argument dof. By default dof=0 resulting in a normal reference. A dof&gt;0 is interpreted as degrees of freedom, and the reference becomes a \\(t\\)-distribution. Figure 8.3 reruns Section 8.2.2 using a \\(t\\) reference with 2 degrees of freedom, a very extreme choice. It results in more tail correlation, evident in the pinched scatterplots. Be warned, however, that this modeling flexibility definitely falls into the “just because you can, doesn’t mean you should” category!\n\n\n\n\n\n\n\n\nFigure 8.3: Correlated marginals using a \\(t\\)-distribution with 2 degrees of freedom.\n\n\n\n\n\n\n\n8.2.4 Copulas as reference distributions\nThe reader may be wondering about using a copula as a reference score. Alas, most copulas are only bivariate. Multivariate copulas, aside from those related to elliptical or Laplace distributions already mentioned, are Archimedian, which means they have a single parameter and cannot be calibrated to a desired correlation matrix.\nSee the aggregate documentation and Mildenhall (2005) for more details.\n\n\n8.2.5 Iman-Conover compared with the normal copula method\nThe normal copula method is described in Wang (1998). Given a set of risks \\((X_1,\\dots,X_k)\\) with marginal cumulative distribution functions \\(F_i\\) and Kendall’s tau \\(\\tau_{ij}=\\tau(X_i,X_j)\\) or rank correlation coefficients \\(r(X_i,X_j)\\), it works as follows. Assume that \\((Z_1,\\dots,Z_k)\\) have a multivariate normal joint probability density function given by \\[\nf(z_1,\\dots,z_k)=\\frac{1}{\\sqrt{(2\\pi)^k|\\Sigma|}}\\exp(-\\mathsf{z}'\\Sigma^{-1}\\mathsf{z}/2),\n\\] \\(\\mathsf{z}=(z_1,\\dots,z_k)\\), with correlation coefficients \\(\\Sigma_{ij}=\\rho_{ij}=\\rho(Z_i,Z_j)\\). Let \\(H(z_1,\\dots,z_k)\\) be their joint cumulative distribution function. Then \\[\nC(u_1,\\dots,u_k)=H(\\Phi^{-1}(u_1),\\dots,\\Phi^{-1}(u_k))\n\\] defines a multivariate uniform cumulative distribution function called the normal copula. Using \\(H\\), the set of variables \\[\nX_1=F_1^{-1}(\\Phi(Z_1)),\\dots,X_k=F_1^{-1}(\\Phi(Z_k))\n\\tag{8.1}\\] have a joint cumulative function \\[\nF_{X_1,\\dots,X_k}(x_1,\\dots,x_k)=H(\\Phi^{-1}(F_x(u_1)),\\dots,\n\\Phi^{-1}(F_k(u_k))\n\\] with marginal cumulative distribution functions \\(F_1,\\dots,F_k\\). The multivariate variables \\((X_1,\\dots,X_k)\\) have Kendall’s tau \\[\n\\tau(X_i,X_j)=\\tau(Z_i,Z_j)=\\frac{2}{\\pi}\\arcsin(\\rho_{ij})\n\\] and Spearman’s rank correlation coefficients \\[\n\\text{rkCorr}(X_i,X_j)=\\text{rkCorr}(Z_i,Z_j)=\\frac{6}{\\pi}\\arcsin(\\rho_{ij}/2)\n\\] so \\(\\rho\\) can be adjusted to achieve the desired correlation.\nIn the normal copula method, we simulate from \\(H\\) and then invert using Equation 8.1. In the Iman-Conover method with normal scores we produce a sample from \\(H\\) such that \\(\\Phi(z_i)\\) are equally spaced between zero and one and then, rather than invert the distribution functions, we make the \\(j\\)th order statistic from the input sample correspond to \\(\\Phi(z)=j/(n+1)\\) where the input has \\(n\\) observations. Because the \\(j\\)th order statistic of a sample of \\(n\\) observations from a distribution \\(F\\) approximates \\(F^{-1}(j/(n+1))\\), we see the normal copula and Iman-Conover methods are doing essentially the same thing.\nWhile the normal copula method and the Iman-Conover method are confusingly similar, there are some important differences to bear in mind. Comparing and contrasting the two methods should help clarify how the two algorithms are different.\n\nWang (1998) shows the normal copula method corresponds to the Iman-Conover method when the latter is computed using normal scores and the Choleski trick.\nThe Iman-Conover method works on a given sample of marginal distributions. The normal copula method generates the sample by inverting the distribution function of each marginal as part of the simulation process.\nThrough the use of scores, the Iman-Conover method relies on a sample of normal variables. The normal copula method could use a similar method, or it could sample randomly from the base normals. Conversely, a sample could be used in the Iman-Conover method.\nOnly the Iman-Conover method has an adjustment to ensure that the reference multivariate distribution has exactly the required correlation structure.\nIman-Conover method samples have rank correlation exactly equal to a sample from a reference distribution with the correct linear correlation. Normal copula samples have approximately correct linear and rank correlations.\nAn Iman-Conover method sample must be taken in its entirety to be used correctly. The number of output points is fixed by the number of input points, and the sample is computed in its entirety in one step. Some Iman-Conover tools produce output, which is in a particular order. Thus, if you sample the \\(n\\)th observation from multiple simulations, or take the first \\(n\\) samples, you will not get a random sample from the desired distribution. However, if you select random rows from multiple simulations (or, equivalently, if you randomly permute the rows’ output prior to selecting the \\(n\\)th) then you will obtain the desired random sample. It is important to be aware of these issues before using canned software routines.\nThe normal copula method produces simulations one at a time, and at each iteration the resulting sample is a sample from the required multivariate distribution. That is, output from the algorithm can be partitioned and used in pieces.\n\nIn summary, remember that these differences can have material, practical consequences, and it is important not to misuse Iman-Conover method samples.\n\n\n8.2.6 Block Iman-Conover (BIC)\nThe Iman-Conover method can lead you into a state of sin: the sin of thinking you can reliably estimate a \\(100 \\times 100\\) correlation matrix (or larger). You can’t; don’t try—it’s all noise. Look at the distribution of eigenvalues to see; start by computing the SVD and compare your distribution with that of a random matrix. However, you might be able to say something about the correlation of chunks of your book. Generally (going back to premium is riskier than loss), lines that are underwritten together have correlated loss ratios, and properties that are nearby have correlated losses. The Block Iman-Conover (BIC) method is a way to use this information without fabricating large correlation matrices.\n\n8.2.6.1 Description of BIC\nBIC is best explained through an example. You underwrite auto liability, GL, and property in four regions: North, East, South, and West. Based on reliable studies, you have a sense of the loss (ratio) correlation between each line overall (a \\(3 \\times 3\\) matrix, with three parameters), and, separately, between the total regional results (a \\(4 \\times 4\\) matrix, with six parameters). The BIC produces a multivariate sample across all 12 line and region units that incorporates what you know, but using just these nine parameters rather than requiring 66 parameters (for a \\(12 \\times 12\\) matrix). Here’s how it works. Pick a number of simulations \\(n\\), then:\n\nProduce four \\(n \\times 3\\) matrices with results by line for each region, \\(R_1\\), \\(R_2\\), \\(R_3\\), \\(R_4\\).\nApply Iman-Conover to each \\(R_i\\) separately using your \\(3\\times 3\\) between-line correlation matrix, to produce \\(\\tilde R_i\\).\nSum across simulations in each \\(\\tilde R_i\\) to get total regional losses and assemble these into a \\(n\\times 4\\) matrix \\(T\\).\nApply Iman-Conover to \\(T\\) using your \\(4\\times 4\\) between-region correlation matrix, and capture the ranking.\nReorder each \\(\\tilde R_i\\) by-row according to the ranking in Step 4 to obtain \\(\\tilde{\\tilde R_i}\\).\nAssemble \\(\\tilde{\\tilde R_i}\\) together into a \\(n\\times 12\\) multivariate sample \\(X\\).\n\nThe result of this algorithm has the correct by-line correlation within each region—because in step 5 rows are moved, which preserves their within-group correlation—and the correct correlation of total by region, from step 4.\nThe aggregate package implements BIC as block_iman_conover. It takes arguments:\n\nunit_losses: a list of samples, in our example the four blocks \\(R_i\\) of losses by line within a region.\nintra_unit_corrs: a list of correlation matrices within (intra) each region.\ninter_unit_corr: the between (inter) unit correlation matrix.\n\nThe regions can have different numbers of lines, and the intra-correlation matrices can vary across regions. In the example below, we just use four regions with three lines, and we assume the intra-region by-line correlations are all the same.\n\n\n\n8.2.7 Example 2: Multistate, multiline correlation\nExample 2 implements the case described in Section 8.2.6.1: four regions each with three lines, and reliably estimated correlation matrices between lines and between regions. The next block shows the specification of the regional loss distributions in DecL.\nport North\n    agg Auto_North  4000 loss  2000 xs 0 sev lognorm  30 cv 2 mixed gamma .2\n    agg GL_North    2000 loss  5000 xs 0 sev lognorm  50 cv 4 mixed gamma .3\n    agg Prop_North  2000 loss 10000 xs 0 sev lognorm 100 cv 5 mixed gamma .1\n\nport East\n    agg Auto_East     3000 loss  2000 xs 0 sev lognorm  30 cv 2 mixed gamma .2\n    agg GL_East       1000 loss  5000 xs 0 sev lognorm  50 cv 4 mixed gamma .3\n    agg Property_East 3000 loss 10000 xs 0 sev lognorm 100 cv 5 mixed gamma .1\n\nport West\n    agg Auto_West     5000 loss  2000 xs 0 sev lognorm  30 cv 2 mixed gamma .2\n    agg GL_West       2000 loss  5000 xs 0 sev lognorm  50 cv 4 mixed gamma .3\n    agg Property_West 2000 loss 10000 xs 0 sev lognorm 100 cv 5 mixed gamma .1\n\nport South\n    agg Auto_South   5000 loss  2000 xs 0 sev lognorm  30 cv 2 mixed gamma .2\n    agg GL_South     2000 loss  5000 xs 0 sev lognorm  50 cv 4 mixed gamma .3\n    agg Prop_South   7000 loss 10000 xs 0 sev lognorm 100 cv 5 mixed gamma .1\nThe losses by line are simulated using frequency-severity aggregate distributions with a lognormal severity and per occurrence limits. Frequency is a gamma-mixed Poisson (negative binomial) with varying CVs. See Mildenhall (2023) for a description of the aggregate DecL language, used to specify these portfolios. The South region portfolio was previously used to illustrate the standard Iman-Conover method in Section 8.2.2.\nTable 8.4 shows the expected losses, modeling error, CV, and 99th percentile of total loss by region. Model error means the difference between the output of the Fast Fourier Transform algorithm and the requested input mean losses. As you can see, the two are almost identical. The CVs vary reflecting volume and mix of business between regions.\n\n\n\n\nTable 8.4: Expected losses, CV, and 99th percentile loss by region.\n\n\n\n\n\n\n\n\n\nRegion\nExpected loss\nModel Error\nCV\n99th percentile\n\n\n\n\nNorth\n8000.0\n-0.000\n0.298\n16,237\n\n\nEast\n7,000\n-0.000\n0.347\n15,920\n\n\nSouth\n14,000\n-0.000\n0.266\n25,737\n\n\nWest\n9,000\n-0.000\n0.276\n17,331\n\n\n\n\n\n\n\n\n\nNext, we sample \\(n=1000\\) losses from each region with the intraregion correlation. In this case, all the intraregion by-line correlations are the same, but the method allows them to vary.\nThe BIC is then run using:\nans = block_iman_conover(region_losses, intra_region_correl, inter_region_correl, True)\nThe three panels of Table 8.5 (respectively, Table 8.6) show the target and achieved inter-region (respectively, intraregion) correlation and the error (difference). The achieved results are very close to those desired.\n\n\n\nTable 8.5: Target and achieved inter-region correlations.\n\n\n\n\n\n\n\n\n\n\n(a) Target inter-region correlation\n\n\n\n\n\nindex\nNorth\nEast\nSouth\nWest\n\n\n\n\nNorth\n1\n0.7\n0.2\n0\n\n\nEast\n0.7\n1\n0.4\n0.2\n\n\nSouth\n0.2\n0.4\n1\n0.8\n\n\nWest\n0\n0.2\n0.8\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Achieved inter-region correlation\n\n\n\n\n\nindex\nNorth\nEast\nSouth\nWest\n\n\n\n\nNorth\n1\n0.685\n0.164\n-0.003\n\n\nEast\n0.685\n1\n0.370\n0.196\n\n\nSouth\n0.164\n0.370\n1\n0.801\n\n\nWest\n-0.003\n0.196\n0.801\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Difference\n\n\n\n\n\nindex\nNorth\nEast\nSouth\nWest\n\n\n\n\nNorth\n0\n0.015\n0.036\n0.003\n\n\nEast\n0.015\n0\n0.030\n0.004\n\n\nSouth\n0.036\n0.030\n0\n-0.001\n\n\nWest\n0.003\n0.004\n-0.001\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.6: Achieved (a-d) and target (e) intra-region correlation.\n\n\n\n\n\n\n\n\n\n\n(a) North\n\n\n\n\n\nindex\nN-AL\nN-GL\nN-Pr\n\n\n\n\nN-AL\n1\n0.567\n0.177\n\n\nN-GL\n0.567\n1\n0.323\n\n\nN-Pr\n0.177\n0.323\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) East\n\n\n\n\n\nindex\nE-AL\nE-GL\nE-Pr\n\n\n\n\nE-AL\n1\n0.535\n0.185\n\n\nE-GL\n0.535\n1\n0.318\n\n\nE-Pr\n0.185\n0.318\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) South\n\n\n\n\n\nindex\nS-AL\nS-GL\nS-Pr\n\n\n\n\nS-AL\n1\n0.566\n0.189\n\n\nS-GL\n0.566\n1\n0.355\n\n\nS-Pr\n0.189\n0.355\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) West\n\n\n\n\n\nindex\nW-AL\nW-GL\nW-Pr\n\n\n\n\nW-AL\n1\n0.570\n0.183\n\n\nW-GL\n0.570\n1\n0.331\n\n\nW-Pr\n0.183\n0.331\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Target\n\n\n\n\n\nindex\nW-AL\nW-GL\nW-Pr\n\n\n\n\nW-AL\n1\n0.6\n0.2\n\n\nW-GL\n0.6\n1\n0.4\n\n\nW-Pr\n0.2\n0.4\n1\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.7 shows the overall correlation matrix at the line-region level (12 x 12 matrix), and Figure 8.4 shows an image plot of the same matrix. The underlying block structure is clear from the image. \n\n\n\n\nTable 8.7: Achieved correlation across lines and regions.\n\n\n\n\n\n\n\n\n\nindex\nN-AL\nN-GL\nN-Pr\nE-AL\nE-GL\nE-Pr\nS-AL\nS-GL\nS-Pr\nW-AL\nW-GL\nW-Pr\n\n\n\n\nN-AL\n1\n0.567\n0.177\n0.376\n0.337\n0.355\n0.086\n0.101\n0.102\n0.033\n-0.012\n0.000\n\n\nN-GL\n0.567\n1\n0.323\n0.354\n0.385\n0.433\n0.122\n0.088\n0.157\n0.053\n0.020\n0.018\n\n\nN-Pr\n0.177\n0.323\n1\n0.277\n0.335\n0.490\n0.055\n0.022\n0.088\n-0.031\n-0.034\n-0.025\n\n\nE-AL\n0.376\n0.354\n0.277\n1\n0.535\n0.185\n0.170\n0.156\n0.204\n0.128\n0.081\n0.044\n\n\nE-GL\n0.337\n0.385\n0.335\n0.535\n1\n0.318\n0.168\n0.220\n0.242\n0.138\n0.095\n0.066\n\n\nE-Pr\n0.355\n0.433\n0.490\n0.185\n0.318\n1\n0.148\n0.218\n0.267\n0.116\n0.113\n0.158\n\n\nS-AL\n0.086\n0.122\n0.055\n0.170\n0.168\n0.148\n1\n0.566\n0.189\n0.384\n0.368\n0.348\n\n\nS-GL\n0.101\n0.088\n0.022\n0.156\n0.220\n0.218\n0.566\n1\n0.355\n0.450\n0.461\n0.388\n\n\nS-Pr\n0.102\n0.157\n0.088\n0.204\n0.242\n0.267\n0.189\n0.355\n1\n0.514\n0.569\n0.490\n\n\nW-AL\n0.033\n0.053\n-0.031\n0.128\n0.138\n0.116\n0.384\n0.450\n0.514\n1\n0.570\n0.183\n\n\nW-GL\n-0.012\n0.020\n-0.034\n0.081\n0.095\n0.113\n0.368\n0.461\n0.569\n0.570\n1\n0.331\n\n\nW-Pr\n0.000\n0.018\n-0.025\n0.044\n0.066\n0.158\n0.348\n0.388\n0.490\n0.183\n0.331\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Image plot of achieved intra-region correlation. The block structure is clearly evident.\n\n\n\n\n\n\n\n8.2.8 Theoretical derivation of the Iman-Conover method\nSuppose that \\(\\mathsf{M}\\) is an \\(n\\) element sample from an \\(r\\) dimensional multivariate distribution, so \\(\\mathsf{M}\\) is an \\(n\\times r\\) matrix. Assume that the columns of \\(\\mathsf{M}\\) are uncorrelated, have mean zero, and have a standard deviation of one. Let \\(\\mathsf{M}'\\) denote the transpose of \\(\\mathsf{M}\\). These assumptions imply that the correlation matrix of the sample \\(\\mathsf{M}\\) can be computed as \\(n^{-1}\\mathsf{M}'\\mathsf{M}\\), and because the columns are independent, \\(n^{-1}\\mathsf{M}'\\mathsf{M}=\\mathsf{id}\\). (There is no need to scale the covariance matrix by the row and column standard deviations because they are all one. In general \\(n^{-1}\\mathsf{M}'\\mathsf{M}\\) is the covariance matrix of \\(\\mathsf{M}\\).)\nLet \\(\\mathsf{S}\\) be a correlation matrix, i.e., \\(\\mathsf{S}\\) is a positive semidefinite symmetric matrix with ones on the diagonal and all elements \\(\\le 1\\) in absolute value. In order to rule out linearly dependent variables, assume \\(\\mathsf{S}\\) is positive definite. These assumptions ensure \\(\\mathsf{S}\\) has a Choleski decomposition \\[\n\\mathsf{S}=\\mathsf{C}'\\mathsf{C}\n\\] for some upper triangular matrix \\(\\mathsf{C}\\), see Golub and Van Loan (2013) or Press et al. (1992). Set \\(\\mathsf{T}=\\mathsf{M}\\mathsf{C}\\). The columns of \\(\\mathsf{T}\\) still have mean zero, because they are linear combinations of the columns of \\(\\mathsf{M}\\), which have zero mean by assumption. It is less obvious, but still true, that the columns of \\(\\mathsf{T}\\) still have standard deviation one. To see why, remember that the covariance matrix of \\(\\mathsf{T}\\) is \\[\nn^{-1}\\mathsf{T}'\\mathsf{T}=n^{-1}\\mathsf{C}'\\mathsf{M}'\\mathsf{M}\\mathsf{C}=\\mathsf{C}'\\mathsf{C}=\\mathsf{S},\n\\tag{8.2}\\] since \\(n^{-1}\\mathsf{M}'\\mathsf{M}=\\mathsf{id}\\) is the identity by assumption. Now \\(\\mathsf{S}\\) also is actually the correlation matrix because the diagonal is scaled to standard deviation one, so the covariance and correlation matrices coincide. The process of converting \\(\\mathsf{M}\\), which is easy to simulate, into \\(\\mathsf{T}\\), which has the desired correlation structure \\(\\mathsf{S}\\), is the theoretical basis of the Iman-Conover method.\nIt is important to note that estimates of correlation matrices, depending on how they are constructed, need not have the mathematical properties of a correlation matrix. Therefore, when trying to use an estimate of a correlation matrix in an algorithm, such as the Iman-Conover, which actually requires a proper correlation matrix as input, it may be necessary to check the input matrix does have the correct mathematical properties.\nNext, we discuss how to make \\(n\\times r\\) matrices \\(\\mathsf{M}\\), with independent, mean zero columns. The basic idea is to take \\(n\\) numbers \\(a_1,\\dots,a_n\\) with \\(\\sum_i a_i=0\\) and \\(n^{-1}\\sum_i a_i^2=1\\), use them to form one \\(n\\times 1\\) column of \\(\\mathsf{M}\\), and then to copy it \\(r\\) times. Finally, randomly permute the entries in each column to make them independent as columns of random variables. Iman and Conover call these \\(a_i\\) scores. They discuss several possible definitions for scores, including scaled versions of \\(a_i=i\\) (ranks) and \\(a_i\\) uniformly distributed. They note that the shape of the output multivariate distribution depends on the scores. All of the examples in their paper use normal scores.\nGiven that the scores will be based on normal random variables, we can either simulate \\(n\\) random standard normal variables and then shift and rescale to ensure mean zero and standard deviation one, or we can use a systematic sample from the standard normal, \\(a_i=\\Phi^{-1}(i/(n+1))\\). By construction, the systematic sample has mean zero, which is an advantage. Also, by symmetry, using the systematic sample halves the number of calls to \\(\\Phi^{-1}\\). For these two reasons we prefer it in the algorithm below. \nThe correlation matrix of \\(\\mathsf{M}\\), constructed by randomly permuting the scores in each column, will only be approximately equal to \\(\\mathsf{id}\\) because of random simulation error. In order to correct for the slight error which could be introduced, Iman and Conover use another adjustment in their algorithm. Let \\(\\mathsf{EE}=n^{-1}\\mathsf{M}'\\mathsf{M}\\) be the actual correlation matrix of \\(\\mathsf{M}\\), and let \\(\\mathsf{EE}=\\mathsf{F}'\\mathsf{F}\\) be the Choleski decomposition of \\(\\mathsf{EE}\\), and define \\(\\mathsf{T}=\\mathsf{M}\\mathsf{F}^{-1}\\mathsf{C}\\). The columns of \\(\\mathsf{T}\\) have mean zero, and the covariance matrix of \\(\\mathsf{T}\\) is \\[\n\\begin{aligned}\nn^{-1}\\mathsf{T}'\\mathsf{T} &= n^{-1}\\mathsf{C}'\\mathsf{F}'^{-1}\\mathsf{M}'\\mathsf{M}\\mathsf{F}^{-1}\\mathsf{C} \\notag  \\\\\n&= \\mathsf{C}'\\mathsf{F}'^{-1}\\mathsf{EE}\\mathsf{F}^{-1}\\mathsf{C} \\notag   \\\\\n&= \\mathsf{C}'\\mathsf{F}'^{-1}\\mathsf{F}'\\mathsf{F}\\mathsf{F}^{-1}\\mathsf{C} \\notag  \\\\\n&= \\mathsf{C}' \\mathsf{C} \\notag  \\\\\n&= \\mathsf{S}\n\\end{aligned}\n\\] and hence \\(\\mathsf{T}\\) has correlation matrix exactly equal to \\(\\mathsf{S}\\), as desired. If \\(\\mathsf{EE}\\) is singular, then the column shuffle needs to be repeated.\nNow that the reference distribution \\(\\mathsf{T}\\) with exact correlation structure \\(\\mathsf{S}\\) is in hand, all that remains to complete the Iman-Conover method is to reorder each column of the input distribution \\(\\mathsf{X}\\) to have the same rank order as the corresponding column of \\(\\mathsf{T}\\).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced topics</span>"
    ]
  },
  {
    "objectID": "070_AdvancedTopics.html#sec-Allocating-Capital",
    "href": "070_AdvancedTopics.html#sec-Allocating-Capital",
    "title": "8  Advanced topics",
    "section": "8.3 Allocating capital, if you must",
    "text": "8.3 Allocating capital, if you must\nWithin each layer, we can allocate its expected losses and premiums to the various portfolio units. Having done so, capital allocation to units follows.\nAdmonition. We can’t emphasize strongly enough that you should not allocate capital. Different layers of capital do not have the same cost (think: equity, junk debt, government bonds), therefore you do not know the appropriate target return on your allocated capital. Without that, you can’t say if a projected return is good or bad. Allocate margin, not capital!\nWarnings about smoking don’t work either. We know your CFO, senior management, and investors discuss allocating underwriting capacity in the language of allocating capital, and we previously admonished you to speak the language of your customers. This section describes two methods you can use to translate allocated margins into allocated capital.\nThe first method is obvious: simply assume all capital earns the weighted average cost of capital and allocate capital by dividing allocated margin by the weighted average cost of capital. This will result in an additive allocation of capital. This has the advantage of being simple to implement, but the disadvantage is that it obscures—nay, denies!—the fact that different segments of capital earn different returns.\nThe second method has a more theoretically sound basis. It applies the principles discussed in Chapter 4 to obtain an allocation of capital \\(Q\\) and unit-specific rates of return \\(\\iota^i\\) consistent with the SRM approach.\nLayer \\(k\\) is \\(\\Delta X_k\\) dollars wide but is triggered by any portfolio loss \\(X &gt; X_k\\). We propose that the expectation of that loss be allocated to units by equal priority, that is, in proportion to the expected losses of the units in those triggering events. We may define the layer-\\(k\\) expected loss share to unit \\(i\\) as \\[\n\\alpha_k^i := \\mathsf E_p\\left[\\frac{X^i}{X}\\mid X &gt; X_k\\right].\n\\] In particular, this means \\[\n\\alpha_k^i S_k = \\mathsf E_p\\left[\\frac{X^i}{X}1_{\\mid X &gt; X_k}\\right] = \\sum_{j&gt;k}\\frac{X_j^i}{X_j}p_j.\n\\tag{8.3}\\] The \\(\\alpha^i\\) sum to one across the units because (conditional) expectations are linear. The expected loss to layer \\(k\\), \\(S_k\\Delta X_k\\), can thus be partitioned into \\(\\alpha_k^i S_k\\Delta X_k\\) across the various units. The total portfolio loss, \\(\\mathsf E[X]\\), is allocated to unit \\(i\\) as \\[\n\\mathsf E[X^i] = \\sum_j \\alpha_j^i S_j\\Delta X_j.\n\\tag{8.4}\\]\nExercise. Prove Equation 8.4, given Equation 8.3.\nSolution. \\[\n\\begin{aligned}\n\\sum_j \\alpha_j^i S_j\\Delta X_j &= \\sum_{k=0}^{n-1} \\left(\\sum_{j=k+1}^{n}\\frac{X_j^i}{X_j} p_j  \\right)(X_{k+1}-X_k) \\\\\n&=\\sum_{j=1}^n \\frac{X_j^i}{X_j} p_j\\sum_{k=0}^{j-1} (X_{k+1}-X_{k}) \\\\\n&=\\sum_{j=1}^n \\frac{X_j^i}{X_j} p_j X_j \\\\\n&=\\sum_{j=1}^n X_j^i p_j \\\\\n&= \\mathsf E[X^i]\n\\end{aligned}\n\\] noting \\(X_0=X_0^i=0\\). \nWhat about premium? We propose that the premium associated with layer \\(k\\) be allocated to units in proportion to the distorted expected losses of the units in those triggering events. This leads to the definition of the distorted expected loss share \\[\n\\beta_k^i := \\mathsf E_q\\left[\\frac{X^i}{X}\\mid X &gt; X_k\\right]\n\\] In particular, this means \\[\n\\beta_k^i g(S_k) = \\sum_{j&gt;k}\\frac{X_j^i}{X_j}q_j = \\sum_{j&gt;k}\\frac{X_j^i Z_j}{X_j}p_j.\n\\] where, recall, \\(Z_j=q_j/p_j\\) is the event weight.\nThe premium to layer \\(k\\), given by \\(P_k = g(S_k)\\Delta X_k\\), is partitioned to the units as \\(\\beta_k^i g(S_k)\\Delta X_k\\). The total premium is therefore allocated to the units as \\[\n\\mathsf E[X^i Z(X)] = \\sum_j \\beta_j^i g(S_j)\\Delta X_j\n\\] being the total premium to unit \\(i\\).\nExercise. Prove the previous equality.\nSolution. The same steps as the preceding exercise. \nSince margin is the difference between premium and expected loss (Equation 4.1), we immediately have the allocation of unit \\(i\\)’s margin \\[\nM^i = \\sum_j (\\beta_j^i g(S_j) - \\alpha_j^i S_j)\\Delta X_j.\n\\tag{8.5}\\]\nExercise. Use Equation 4.10 with Equation 4.8 and Equation 4.9 to derive a simple equation for \\(M^i\\) and then prove it is the equivalent of Equation 8.5. \nRecall that the layer \\(k\\) cost of capital is \\[\n\\iota_k := \\frac{M_k}{Q_k} = \\frac{P_k - S_k\\Delta X_k}{\\Delta X_k-P_k} = \\frac{g(S_k) - S_k}{1- g(S_k)}.\n\\] Here, we lean on the assumption of law invariance (see Section 4.4). We claim that for a law invariant pricing approach, the layer cost of capital must be the same for all units. Law invariance implies the risk measure is only concerned with the attachment probability of the layer and not with the cause of loss within the layer. If \\(\\iota\\) within a layer varied by unit, then the allocation could not be law invariant. This crucial observation underlies the following logic.\nBecause cost of capital equals margin over capital, and seeing as both margin and cost of capital are known by layer and by unit, with the latter constant across units, we can compute unit capital by layer \\(Q_k^i\\) via\n\\[\\begin{equation}\n\\iota_k: = \\frac{M_k}{Q_k} = \\frac{M_k^i}{Q_k^i}\\implies Q_k^i = \\frac{M_k^i}{\\iota_k}.\n\\end{equation}\\] Substituting the details gives the following definition for the NA of capital in layer \\(k\\) to unit \\(i\\): \\[\nQ_k^i = \\frac{\\beta_k^i g(S_k) -  \\alpha_k^i S_k}{g(S_k)- S_k} (1-g(S_k))\\Delta X_k.\n\\tag{8.6}\\]\nIn words, taking Equation 8.6 and dividing numerator and denominator by capital in the layer shows \\[\nQ_k^i = \\frac{\\text{Margin for unit in layer}}{\\text{Total margin in layer}}\\times \\text{Capital in layer} = \\frac{\\text{Margin for unit in layer}}{\\text{Layer cost of capital}}.\n\\] The NA is the same as method 1 (divide by weighted average cost of capital) but applied separately for each layer of capital!\nExercise. Derive Equation 8.6. \nSince \\(1-g(S_k)\\) is the proportion of of layer \\(k\\)’s limit devoted to capital, this says the proportion of capital allocated to unit \\(i\\) is given by the nicely symmetric expression \\[\n\\frac{\\beta_k^i g(S_k) -  \\alpha_k^i S_k}{g(S_k)- S_k}\n\\tag{8.7}\\] To determine total capital by unit, we sum across layers \\[\nQ^i := \\sum_j Q_j^i.\n\\] Finally, we can determine the average cost of capital for unit \\(i\\) \\[\\begin{equation}\n\\iota^i = \\frac{M^i}{Q^i}.\n\\end{equation}\\]\nThis is illustrated for our example in Table 8.8, Table 8.9, and Table 8.10. In the last table, the columns are calculated as \\(M^i=\\beta^i g(S)-\\alpha^i S\\), \\(\\iota=(g-S)/(1-g)\\), \\(Q^i=M^i/\\iota\\), and the low row \\(M\\) (respectively, \\(Q\\)) show sum products with \\(\\Delta X\\). \n\n\n\n\nTable 8.8: Calculation of loss shares by layer. The row \\(L\\) shows sum products with \\(\\Delta X\\).\n\n\n\n\n\n\n\n\n\n\\(k\\)\nUnit A \\(X^1\\)\nUnit B \\(X^2\\)\nUnit C \\(X^3\\)\nTotal \\(X\\)\n\\(p\\)\n\\(S\\)\n\\(\\Delta X\\)\n\\(\\alpha^1 S\\)\n\\(\\alpha^2 S\\)\n\\(\\alpha^3 S\\)\n\n\n\n\n0\n0\n0\n0\n0\n0\n1\n22\n0.325\n0.441\n0.234\n\n\n1\n15\n7\n0\n22\n0.1\n0.9\n6\n0.257\n0.409\n0.234\n\n\n2\n15\n13\n0\n28\n0.1\n0.8\n8\n0.203\n0.362\n0.234\n\n\n3\n5\n20\n11\n36\n0.1\n0.7\n4\n0.189\n0.307\n0.204\n\n\n4\n10\n24\n6\n40\n0.4\n0.3\n15\n0.089\n0.067\n0.144\n\n\n5\n26\n19\n10\n55\n0.1\n0.2\n10\n0.042\n0.032\n0.126\n\n\n6\n17\n8\n40\n65\n0.1\n0.1\n35\n0.016\n0.020\n0.064\n\n\n7\n16\n20\n64\n100\n0.1\n0\n0\n0\n0\n0\n\n\n\\(L=\\mathsf E_p\\)\n13.4\n18.3\n14.9\n46.6\n\n\n\n13.4\n18.3\n14.9\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.9: Calculation of premium shares by layer. The row \\(P\\) shows sum products with \\(\\Delta X\\).\n\n\n\n\n\n\n\n\n\n\\(k\\)\nUnit A \\(X^1\\)\nUnit B \\(X^2\\)\nUnit C \\(X^3\\)\nTotal \\(X\\)\n\\(q\\)\n\\(gS\\)\n\\(\\Delta X\\)\n\\(\\beta^1 S\\)\n\\(\\beta^2 S\\)\n\\(\\beta^3 S\\)\n\n\n\n\n0\n0\n0\n0\n0\n0\n1\n22\n0.295\n0.409\n0.296\n\n\n1\n15\n7\n0\n22\n0.052\n0.948\n6\n0.260\n0.392\n0.296\n\n\n2\n15\n13\n0\n28\n0.066\n0.882\n8\n0.225\n0.362\n0.296\n\n\n3\n5\n20\n11\n36\n0.075\n0.807\n4\n0.214\n0.320\n0.273\n\n\n4\n10\n24\n6\n40\n0.379\n0.428\n15\n0.119\n0.093\n0.216\n\n\n5\n26\n19\n10\n55\n0.119\n0.309\n10\n0.063\n0.051\n0.194\n\n\n6\n17\n8\n40\n65\n0.135\n0.174\n35\n0.028\n0.035\n0.111\n\n\n7\n16\n20\n64\n100\n0.174\n0\n0\n0\n0\n0\n\n\n\\(L=\\mathsf E_p\\)\n13.4\n18.3\n14.9\n46.6\n\n\n\n\n\n\n\n\n\\(P=\\mathsf E_p\\)\n14.109\n18.637\n20.819\n53.565\n\n\n\n14.109\n18.637\n20.819\n\n\n\\(M=P-L\\)\n0.709\n0.337\n5.919\n6.965\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8.10: Allocation of capital.\n\n\n\n\n\n\n\n\n\n\\(k\\)\nTotal \\(X\\)\n\\(\\Delta X\\)\n\\(M^1\\)\n\\(M^2\\)\n\\(M^3\\)\n\\(Q^1\\)\n\\(Q^2\\)\n\\(Q^3\\)\n\\(\\iota\\)\n\\(\\delta\\)\n\n\n\n\n0\n0\n22\n-0.030\n-0.032\n0.061\n0\n0\n0\ninf\n1\n\n\n1\n22\n6\n0.003\n-0.017\n0.061\n0.003\n-0.018\n0.067\n0.917\n0.478\n\n\n2\n28\n8\n0.021\n-0.001\n0.061\n0.031\n-0.001\n0.089\n0.693\n0.409\n\n\n3\n36\n4\n0.025\n0.013\n0.069\n0.045\n0.024\n0.125\n0.555\n0.357\n\n\n4\n40\n15\n0.030\n0.026\n0.072\n0.134\n0.115\n0.323\n0.224\n0.183\n\n\n5\n55\n10\n0.021\n0.019\n0.069\n0.133\n0.121\n0.437\n0.158\n0.136\n\n\n6\n65\n35\n0.012\n0.015\n0.047\n0.132\n0.165\n0.529\n0.089\n0.082\n\n\n7\n100\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\\(\\mathsf E_q\\)\n\n\n0.709\n0.337\n5.919\n8.411\n8.691\n29.333\n\n\n\n\n\\(\\iota\\)\n\n\n\n\n\n0.084\n0.039\n0.202\n0.150\n\n\n\n\n\n\n\n\n\n\nObserve that \\((\\sum_i \\iota^i Q^i)/Q\\) is 15%, the weighted cost of capital over all units is 15%, as assumed.\nNotice that the first layer \\(k=0\\) has undefined cost of capital \\(\\iota\\); since \\(S_0=1\\), we have \\(g(S_0)=1\\), \\(P_0=\\Delta X_0\\), and \\(M_0=Q_0=0\\). Nonetheless, we have nonzero unit margins \\(M_0^i\\) summing to zero.\nThe smallest portfolio loss equals 22, meaning the first layer is entirely funded by loss payments and requires no capital in aggregate. However, there are cross-subsidies between the units within the layer. Were the three units to purchase insurance from an insurance company with only $22 of assets, it would benefit the most volatile line, because it would capture an outsized proportion of the available assets in the case it had a large loss. The margins reflect these cross-subsidies. However because the layer is fully funded by losses, there is no capital and hence no aggregate margin, which is why the allocated margins sum to zero.\nAlthough the cost of capital within each layer is the same for all units, the margin, the proportion of limit that is capital, and the proportion of that capital attributable to each unit, all vary across layers. Therefore, average cost of capital generally varies by unit. Table 8.11 shows the difference in results between the natural Wang allocation and the CCoC approaches done previously. (The natural CCoC is equivalent to the industry standard approach.) The story is complex. While both noncatastrophe lines receive more margin and more capital under the Wang allocation, it is not proportional; their rates of return are not only lower than the 15% benchmark, but they are markedly different.\n\n\n\n\nTable 8.11: Comparison of Wang SRM and CCoC natural capital allocations.\n\n\n\n\n\n\n\n\n\nDistortion\nMetric\nUnit A\nUnit B\nUnit C\nPortfolio\n\n\n\n\nWang, 0.343\nCapital \\(Q\\)\n8.411\n8.691\n29.333\n46.435\n\n\n\nMargin \\(M\\)\n0.709\n0.337\n5.919\n6.965\n\n\n\nReturn \\(\\iota\\)\n8.4%\n3.9%\n20.2%\n15.0%\n\n\nCCoC, 0.150\nCapital \\(Q\\)\n4.874\n5.288\n36.273\n46.435\n\n\n\nMargin \\(M\\)\n0.339\n0.222\n6.404\n6.965\n\n\n\nReturn \\(\\iota\\)\n7.0%\n4.2%\n17.7%\n15.0%\n\n\n\n\n\n\n\n\n\nFor a typical distortion other than CCoC, in line with Table 1.1, we find:\n\nLower layers of assets, below the expected losses, have a high cost of capital but, offsetting this, they are mostly funded by premium and require little supporting investor capital, resulting in high leverage.\nHigher layers of assets have a lower cost of capital but higher capital content: they are funded by capital to a greater degree.\nLow volatility units by definition tend to have losses close to their expected value, regardless of the value of total losses, and so consume relatively more of the expensive, lower layer capital and a smaller proportion of cheaper, higher asset layers. Offsetting this, there is more capital in higher layers than lower ones.\nHigh volatility units tend to have a larger proportion of total losses when the portfolio total loss is large, and so they consume a greater proportion of cheaper, higher layer capital.\n\nWe see all of these phenomena in the example.\nWith price, quantity, and proportion all variable the overall effect of the NA of capital is hard to predict. Since price, amount, and use are all correlated, it is never adequate to assume that the unit average cost of capital is the portfolio average cost of capital. This is the fundamental flaw in the industry standard approach, which uses the same cost of capital for all capital layers (see Section 4.7). The SRM-based method outlined here produces varying cost of capital by unit in a defensible manner—given a choice of distortion function. How to make that choice was discussed in Chapter 6.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced topics</span>"
    ]
  },
  {
    "objectID": "070_AdvancedTopics.html#comparions-with-bodoffs-layer-capital-allocation",
    "href": "070_AdvancedTopics.html#comparions-with-bodoffs-layer-capital-allocation",
    "title": "8  Advanced topics",
    "section": "8.4 Comparions with Bodoff’s layer capital allocation",
    "text": "8.4 Comparions with Bodoff’s layer capital allocation\nReaders may have guessed that the NA is closely related to Bodoff’s layer capital allocation (Bodoff 2007). This section will explain how they are correct and show the connections, similarities, and differences between the two approaches. Bodoff’s approach uses \\(\\alpha^i\\), which describes how loss in each layer is shared between the units, and applies it to all assets in the layer. Since \\(\\sum_i \\alpha^1 = 1\\) we get a decomposition \\[\na = \\int_0^a 1\\,dx = \\sum_i \\int_0^a \\alpha^i(x)\\,dx.\n\\] Thus, \\(a^i=\\int_0^a \\alpha^i(x)\\,dx\\) is a reasonable allocation of assets. However, to determine a premium \\(a^i\\) needs to be split into amounts \\(P^i + Q^i\\) funded by policyholder premium and investor capital. For example, we could fall back to CCoC, resulting in a layer premium \\[\n(\\alpha^i(x) S(x) + \\delta \\alpha^i(x) (1 - S(x)))  + \\nu \\delta \\alpha^i(x) (1 - S(x))\n\\] where the first parenthetic term is expected loss (Equation 8.3) plus a margin \\(\\delta\\) times the unfunded liability (Table 4.1, P3), and the second term is investor capital (Table 4.1, Q). This reproduces distortion CCoC pricing. Picking other distortions, which allow for the unfunded liability to be split differently than losses, produces a range of other answers. Bodoff’s paper is very insightful, but it falls short by not fully describing a premium allocation: further assumptions are still needed to convert allocated assets into allocated premiums. Table 8.12 shows Bodoff’s asset allocation in the last four right-hand columns.\n\n\n\n\nTable 8.12: Bodoff’s asset allocations, \\(\\alpha^i\\Delta X\\).\n\n\n\n\n\n\n\n\n\n\\(k\\)\nTotal \\(X\\)\n\\(p\\)\n\\(S\\)\n\\(\\Delta X\\)\n\\(\\alpha^1\\Delta X\\)\n\\(\\alpha^2\\Delta X\\)\n\\(\\alpha^3\\Delta X\\)\n\\(\\sum\\alpha^i\\Delta X\\)\n\n\n\n\n0\n0\n0\n1\n22\n7.152\n9.694\n5.154\n22\n\n\n1\n22\n0.1\n0.9\n6\n1.713\n2.726\n1.562\n6\n\n\n2\n28\n0.1\n0.8\n8\n2.033\n3.624\n2.343\n8\n\n\n3\n36\n0.1\n0.7\n4\n1.082\n1.753\n1.164\n4\n\n\n4\n40\n0.4\n0.3\n15\n4.471\n3.343\n7.186\n15.000\n\n\n5\n55\n0.1\n0.2\n10\n2.108\n1.615\n6.277\n10.000\n\n\n6\n65\n0.1\n0.1\n35\n5.600\n7.000\n22.400\n35.000\n\n\n7\n100\n0.1\n0\n0\n0\n0\n0\n0\n\n\nTotal\n46.6\n\n\n\n24.159\n29.756\n46.086\n100.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBodoff, Neil M. 2007. “Capital Allocation by Percentile Layer.” Variance 3 (1): 13–30.\n\n\nBolstad, William M, and James M Curran. 2016. Introduction to Bayesian Statistics. John Wiley & Sons.\n\n\nGeisser, Seymour. 2017. Predictive Inference. Chapman; Hall/CRC.\n\n\nGolub, Gene H, and Charles F Van Loan. 2013. Matrix Computations. Johns Hopkins University Press.\n\n\nHansen, Lars Peter, and Thomas J Sargent. 2008. Robustness. https://doi.org/10.1007/978-1-4471-5562-1_7.\n\n\nJewson, Stephen, Trevor Sweeting, and Lynne Jewson. 2025. “Reducing reliability bias in assessments of extreme weather risk using calibrating priors.” Advances in Statistical Climatology, Meteorology and Oceanography 11 (1): 1–22. https://doi.org/10.5194/ascmo-11-1-2025.\n\n\nKreps, Rodney E. 1997. Parameter Uncertainty in (Log)normal Distributions. https://www.casact.org/sites/default/files/database/proceed_proceed97_97553.pdf.\n\n\nMajor, John A. 1999. “Taking Uncertainty Into Account: Bias Issues Arising from Parameter Uncertainty in Risk Models.” CAS Forum Summer (153-196).\n\n\nMajor, John A., Ruodu Wang, and Micah G. Woolstenhulme. 2015. “The Most Dangerous Model: A Natural Benchmark for Assessing Model Risk.” Society of Actuaries Monograph: Enterprise Risk Management Symposium, 1–45.\n\n\nMildenhall, Stephen J. 2023. “aggregate: Frequency-Severity Distributions Made Easy.” Actuarial Review 50(3): 31–34. https://ar.casact.org/aggregate-frequency-severity-distributions-made-easy/.\n\n\nMildenhall, Stephen J. n.d. When is Premium Riskier Than Loss?\n\n\nMildenhall, Stephen J. 2005. “Correlation and Aggregate Loss Distributions With An Emphasis On The Iman-Conover Method.” Casualty Actuarial Society Forum Winter. https://www.casact.org/sites/default/files/database/forum_06wforum_06w107.pdf.\n\n\nPress, William H., Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C. Cambridge University Press.\n\n\nSmith, Ralph C. 2024. Uncertainty Quantification: Theory, Implementation, and Applications. SIAM.\n\n\nWang, Shaun S. 1998. “Aggregation of correlated risk portfolios: models and algorithms.” Proceedings of the Casualty Actuarial Society, 848–939.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced topics</span>"
    ]
  },
  {
    "objectID": "080_Calculations.html",
    "href": "080_Calculations.html",
    "title": "9  Calculations with Aggregate",
    "section": "",
    "text": "9.1 Aggregate and Python\nExample calculations using Python and R.\nThis chapter reproduces many of the exhibits from the main text using Python and the aggregate package. It also describes how to extend to more realistic examples. Throughout, we have included only terse subsets of code so the reader can focus on the results and the output. Instructions for obtaining the full code are provided below.\naggregate is a Python package that can be used to build approximations to compound (aggregate) probability distributions quickly and accurately, and to solve insurance, risk management, and actuarial problems using realistic models that reflect underlying frequency and severity. It delivers the speed and accuracy of parametric distributions to situations that usually require simulation, making it as easy to work with an aggregate (compound) probability distribution as the lognormal. aggregate includes an expressive language called DecL to describe aggregate distributions.\nThe aggregate package is available on PyPi, the source code is on GitHub at https://github.com/mynl/aggregate, and there is extensive documentation. The Aggregate class and DecL lanaguage are described in Mildenhall (2024). There is also an extensive series of videos introducing various capabilities available on YouTube.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Calculations with `Aggregate`</span>"
    ]
  },
  {
    "objectID": "080_Calculations.html#aggregate-and-r",
    "href": "080_Calculations.html#aggregate-and-r",
    "title": "9  Calculations with Aggregate",
    "section": "9.2 Aggregate and R",
    "text": "9.2 Aggregate and R\nPython packages can be used in R via the reticulate library. Python and R code can also be mixed in RMarkdown (now Quarto) files. This short video explains how to use aggregate from R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Calculations with `Aggregate`</span>"
    ]
  },
  {
    "objectID": "080_Calculations.html#sec-setup-code",
    "href": "080_Calculations.html#sec-setup-code",
    "title": "9  Calculations with Aggregate",
    "section": "9.3 Reproducing the code examples",
    "text": "9.3 Reproducing the code examples\nTo reproduce the Python code examples, you must set up your environment and then install aggregate using\npip install aggregate\nThe formatting of the examples relies on greater_tables, which can be installed in the same way. The installation process will also install several standard packages, such as numpy and pandas, that it depends on. If you get an error message about other missing packages, you can install them with pip. All the code in the monograph runs on aggregate version 0.24.0 and should run on any later version. If you have an older version installed, you can updated it with pip install -U aggregate.\nOnce aggregate is installed, start by importing basic libraries. The next code block shows the relevant aggregate function. The class GT from greater_tables is used to format DataFrames consistently.\nfrom aggregate import (\n    Portfolio,                  # creates multi-unit portfolios\n    make_ceder_netter,          # models reinsurance\n    Distortion                  # creates a Distortion function\n)\nfrom greater_tables import GT   # consistent table format\nOnline resources. Complete code samples can be extracted from the online version of the monograph, available at https://cmm.mynl.com/. Each code block has a small copy icon in the upper right-hand corner. The code blocks on each page can all be shown or hidden using the &lt;/&gt;Code control at the top of the page. To download the original qmd file, use the View Source option under the same control. Alternatively, the entire Quarto (previously RMarkdown) source can be downloaded from GitHub at https://github.com/mynl/CapitalModeling (CAS TO INSERT OWN LINK).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Calculations with `Aggregate`</span>"
    ]
  },
  {
    "objectID": "080_Calculations.html#sec-sde",
    "href": "080_Calculations.html#sec-sde",
    "title": "9  Calculations with Aggregate",
    "section": "9.4 Reproducing the discrete example",
    "text": "9.4 Reproducing the discrete example\n\n9.4.1 Input data and setup\nThe InsCo example running through this monograph is based on the 10 events shown in Table 9.1, which reproduces Table 2.1, and adds the mean and CV. For reference, here is the code to create the underlying dataframe.\n\n\nCode\nloss_sample = pd.DataFrame(\n    {\n        'A': [ 5,  7, 15, 15, 13,  5, 15, 26, 17, 16],\n        'B': [20, 33, 13,  7, 20, 27, 16, 19,  8, 20],\n        'C': [11,  0,  0,  0,  7,  8,  9, 10, 40, 64],\n    }\n)\nloss_sample['total'] = loss_sample.sum(axis=1)\nloss_sample['p_total'] = 1/10\nprint(loss_sample)\n\n\n    A   B   C  total  p_total\n0   5  20  11     36      0.1\n1   7  33   0     40      0.1\n2  15  13   0     28      0.1\n3  15   7   0     22      0.1\n4  13  20   7     40      0.1\n5   5  27   8     40      0.1\n6  15  16   9     40      0.1\n7  26  19  10     55      0.1\n8  17   8  40     65      0.1\n9  16  20  64    100      0.1\n\n\n\n\n\n\nTable 9.1: The 10 equally likely simulations underlying the basic examples.\n\n\n\n\n\n\n\n\n\nindex\nA\nB\nC\nTotal\n\n\n\n\n0\n5\n20\n11\n72\n\n\n1\n7\n33\n0\n80\n\n\n2\n15\n13\n0\n56\n\n\n3\n15\n7\n0\n44\n\n\n4\n13\n20\n7\n80\n\n\n5\n5\n27\n8\n80\n\n\n6\n15\n16\n9\n80\n\n\n7\n26\n19\n10\n110\n\n\n8\n17\n8\n40\n130\n\n\n9\n16\n20\n64\n200\n\n\nEX\n13.4\n18.3\n14.9\n93.2\n\n\nCV\n0.453\n0.412\n1.324\n0.455\n\n\n\n\n\n\n\n\n\nNow we create an aggregate Portfolio class instance based on the loss_sample simulation output. The next block shows how to do this, using the Portfolio.create_from_sample method. TMA1 is a label, and loss_sample is the dataframe created previously. The other arguments specify working with unit-sized buckets bs=1 appropriate for our integer losses and to use \\(256=2^8\\) buckets, log2=8.\n\n\nCode\nwport = Portfolio.create_from_sample('TMA1', loss_sample, bs=1, log2=8)\n\n\nThe next line of code displays Table 9.2, which shows summary output from the object wport by accessing its describe attribute. The summary includes the mean, CV, and skewness. The model and estimated (Est) columns are identical because we specified the distribution directly; no simulation or Fast Fourier Transform-based numerical methods are employed.\n\n\nCode\nfGT(wport.describe.drop(columns=['Err E[X]', 'Err CV(X)']))\n\n\n\n\nTable 9.2: Summary statistics for the base example.\n\n\n\n\n\n\n\n\n\nunit\nX\nE[X]\nEst E[X]\nCV(X)\nEst CV(X)\nSkew(X)\nEst Skew(X)\n\n\n\n\nA\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n13.4\n13.4\n0.453\n0.453\n0.281\n0.281\n\n\n\nAgg\n13.4\n13.4\n0.453\n0.453\n0.281\n0.281\n\n\nB\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n18.300\n18.300\n0.412\n0.412\n0.269\n0.269\n\n\n\nAgg\n18.300\n18.300\n0.412\n0.412\n0.269\n0.269\n\n\nC\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n14.9\n14.900\n1.324\n1.324\n1.603\n1.603\n\n\n\nAgg\n14.9\n14.900\n1.324\n1.324\n1.603\n1.603\n\n\ntotal\nFreq\n3\n\n0\n\n\n\n\n\n\nSev\n15.533\n15.533\n0.827\n\n1.884\n\n\n\n\nAgg\n46.6\n46.600\n0.471\n0.471\n1.177\n1.177\n\n\n\n\n\n\n\n\n\nNext, we need to calibrate distortion functions to achieve the desired pricing. The example uses full capitalization throughout (assets equal the maximum loss), which is equivalent to a 100% percentile level (see Table 9.5) and assumes the cost of capital is 15%. The code runs the calibration by calling wport.calibrate_distortions with arguments for the capital percentile level Ps and cost of capital COCs. The arguments are passed as lists because the routine can be used to solve for several percentile levels and costs simultaneously. Table 9.3 shows the resulting parameters, which match those in Table 4.5.\n\n\nCode\n# calibrate to 15% return at p=100% capital standard\nwport.calibrate_distortions(Ps=[1], COCs=[.15]);\n\n\n\n\n\n\nTable 9.3: Distortion functions calibrated to 15% return on full capital.\n\n\n\n\n\n\n\n\n\nmethod\nP\nparam\nerror\n\n\n\n\nccoc\n53.565\n0.150\n0\n\n\nph\n53.565\n0.720\n0.000\n\n\nwang\n53.565\n0.343\n0.000\n\n\ndual\n53.565\n1.595\n-0.000\n\n\ntvar\n53.565\n0.271\n0.000\n\n\n\n\n\n\n\n\n\nFigure 9.1 shows the resulting distortion \\(g\\) functions (top) and the probability adjustment \\(Z=q/p\\) (bottom). The red star indicates a probability mass at \\(p=1\\).\n\n\n\n\n\n\n\n\nFigure 9.1: Distortion functions (top row) and distortion probability adjustment functions (bottom row).\n\n\n\n\n\nFor technical reasons (discussed in Section 4.3 as well as chapter 14.1.5 in Mildenhall and Major (2022), see especially Figure 14.2), we must summarize the 10 events by distinct totals. Routine code produces Table 9.4, reproducing Table 4.2. \n\n\n\n\nTable 9.4: Assumptions for portfolio, summarized by distinct totals.\n\n\n\n\n\n\n\n\n\nindex\nA\nB\nC\np\nTotal\n\n\n\n\n0\n15\n7\n0\n0.1\n22\n\n\n1\n15\n13\n0\n0.1\n28\n\n\n2\n5\n20\n11\n0.1\n36\n\n\n3\n10\n24.000\n6.000\n0.4\n40\n\n\n4\n26\n19\n10\n0.1\n55\n\n\n5\n17\n8\n40\n0.1\n65\n\n\n6\n16\n20\n64\n0.1\n100\n\n\nEX\n13.4\n18.3\n14.9\n1\n46.6\n\n\nPlan\n13.9\n18.7\n19.6\n1\n52.2\n\n\n\n\n\n\n\n\n\nThe examples describe a VaR- or TVaR-based capital standard (see Table 3.1), but for this example it works out to the the same as a fully capitalized 100% standard, as shown by the values in Table 9.5. The wport methods q and tvar compute quantiles and TVaRs for each unit and the total.\n\n\n\n\nTable 9.5: Quantiles (VaR) and TVaR at different probability levels.\n\n\n\n\n\n\n\n\n\np\nQuantile\nTVaR\n\n\n\n\n0.8\n55\n82.500\n\n\n0.850\n65\n88.333\n\n\n0.9\n65\n100\n\n\n1\n100\n100\n\n\n\n\n\n\n\n\n\n\n\n9.4.2 Premiums by distortion\nTable 4.11 shows premium assuming a CCoC distortion with a 15% return applied using a 100% capital standard. The premium P row shows what Mildenhall and Major (2022) calls the linear NA premium. Table 9.6 reproduces this exhibit, confirming an equal 15% returns across all allocated capital. The code illustrates the power of aggregate. The first line creates a CCoC distortion using the Distortion class; note the argument is risk discount \\(\\delta\\). The second line uses the price method to compute the linear allocation, which it returns in a structure with various diagnostic information, including Table 9.6. \n\n\nCode\nccoc = Distortion.ccoc(.15 / 1.15)\npricing_info = wport.price(1, ccoc, allocation='linear')\n\n\n\n\n\n\nTable 9.6: Industry standard approach pricing using the CCoC distortion.\n\n\n\n\n\n\n\n\n\nindex\nA\nB\nC\nTotal\n\n\n\n\nL\n13.4\n18.3\n14.900\n46.6\n\n\na\n16.000\n20.000\n64.000\n100\n\n\nQ\n2.261\n1.478\n42.696\n46.435\n\n\nP\n13.739\n18.522\n21.304\n53.565\n\n\nM\n0.339\n0.222\n6.404\n6.965\n\n\nCOC\n0.150\n0.150\n0.150\n0.150\n\n\n\n\n\n\n\n\n\nTable 4.3, Table 4.4 and Table 4.10 compute \\(\\rho(X)\\) for \\(\\rho\\) the CCoC SRM, using the \\(\\rho(X)=\\int g(S(x))\\,dx\\) (second table) and \\(\\rho(X)=\\int xg'(S(x))f(x)\\,dx\\) (third) representations. The numbers needed for these calculations are shown in Table 9.7, where \\(q=g'(S(x))f(x)\\). These are extracted from the dataframe wport.density_df, which contains the probability mass, density, and survival functions for the portfolio, among other facts. The last row carries out the calculations and confirms the two methods give the same result, the total under \\(gS\\) using the former representation and under \\(q\\) using the latter representation.\n\n\n\n\nTable 9.7: Industry standard approach pricing: raw ingredients and computed means.\n\n\n\n\n\n\n\n\n\nloss\np_total\nF\nS\ngS\nq\nΔX\n\n\n\n\n22\n0.1\n0.1\n0.9\n0.913\n0.087\n6\n\n\n28\n0.1\n0.2\n0.8\n0.826\n0.087\n8\n\n\n36\n0.1\n0.3\n0.7\n0.739\n0.087\n4\n\n\n40\n0.4\n0.7\n0.3\n0.391\n0.348\n15\n\n\n55\n0.1\n0.8\n0.2\n0.304\n0.087\n10\n\n\n65\n0.1\n0.9\n0.1\n0.217\n0.087\n35\n\n\n100\n0.1\n1\n0\n0\n0.217\n0\n\n\n\\(\\rho(X)\\)\n\n\n\n53.565\n53.565\n\n\n\n\n\n\n\n\n\n\n\nTable 9.8 shows the results for the other standard distortions. The calculations match those of the Wang in Table 9.8 (c). All of the calibrated distortions are carried in the dictionary wport.dists. These tables also include the NA of capital (a process we do not recommend, but that is described in Section 8.3). The Q row matches the calculation of allocated capital shown in Table 8.10. \n\n\n\nTable 9.8: Pricing by distortion by unit.\n\n\n\n\n\n\n\n\n\n\n(a) CCoC\n\n\n\n\n\nstatistic\nA\nB\nC\nTotal\n\n\n\n\nL\n13.400\n18.300\n14.900\n46.600\n\n\na\n18.613\n23.810\n57.577\n100.0\n\n\nQ\n4.874\n5.288\n36.273\n46.435\n\n\nP\n13.739\n18.522\n21.304\n53.565\n\n\nM\n0.339\n0.222\n6.404\n6.965\n\n\nCOC\n0.070\n0.042\n0.177\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PH\n\n\n\n\n\nstatistic\nA\nB\nC\nTotal\n\n\n\n\nL\n13.400\n18.300\n14.900\n46.600\n\n\na\n21.133\n24.914\n53.952\n100.000\n\n\nQ\n7.074\n6.565\n32.796\n46.435\n\n\nP\n14.060\n18.349\n21.156\n53.565\n\n\nM\n0.660\n0.049\n6.256\n6.965\n\n\nCOC\n0.093\n0.008\n0.191\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Wang\n\n\n\n\n\nstatistic\nA\nB\nC\nTotal\n\n\n\n\nL\n13.400\n18.300\n14.900\n46.600\n\n\na\n22.520\n27.328\n50.152\n100.000\n\n\nQ\n8.411\n8.691\n29.333\n46.435\n\n\nP\n14.109\n18.637\n20.819\n53.565\n\n\nM\n0.709\n0.337\n5.919\n6.965\n\n\nCOC\n0.084\n0.039\n0.202\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Dual\n\n\n\n\n\nstatistic\nA\nB\nC\nTotal\n\n\n\n\nL\n13.400\n18.300\n14.900\n46.600\n\n\na\n22.999\n28.260\n48.741\n100.0\n\n\nQ\n8.873\n9.143\n28.419\n46.435\n\n\nP\n14.127\n19.117\n20.322\n53.565\n\n\nM\n0.727\n0.817\n5.422\n6.965\n\n\nCOC\n0.082\n0.089\n0.191\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) TVaR\n\n\n\n\n\nstatistic\nA\nB\nC\nTotal\n\n\n\n\nL\n13.400\n18.300\n14.900\n46.600\n\n\na\n22.817\n29.659\n47.525\n100.000\n\n\nQ\n9.034\n9.247\n28.154\n46.435\n\n\nP\n13.783\n20.412\n19.371\n53.565\n\n\nM\n0.383\n2.112\n4.471\n6.965\n\n\nCOC\n0.042\n0.228\n0.159\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.4.3 Reinsurance analysis\nChapter 5 analyzes a possible 35 xs 65 aggregate stop loss reinsurance contract. The setup to analyze this contract, using a separate Portfolio object, is shown next. The first two lines apply reinsurance; [1, 35, 65] specifies 100% of the 35 xs 65 layer, and c and n are functions mapping gross to ceded and net. The last line creates a new Portfolio object from the net and ceded aggregate losses.\n\n\nCode\nloss_sample_re = loss_sample.copy()\nc, n = make_ceder_netter([(1, 35, 65)])\nloss_sample_re['Net'] = n(loss_sample_re.total)\nloss_sample_re['Ceded'] = c(loss_sample_re.total)\nloss_sample_re = loss_sample_re[['Net', 'Ceded', 'p_total']]\n\n# build Portfolio object with Net, Ceded units\nwport_re = Portfolio.create_from_sample('WGS3',\n            loss_sample_re[['Net', 'Ceded', 'p_total']], bs=1, log2=8)\n\n\nTable 9.9 shows the standard summary statistics.  It should be compared to Table 5.1. Table 9.10 shows the allocated pricing to the reinsurance and net across the five standard distortions, compare with the last row of Table 5.1 and discussion in Section 5.2. \n\n\n\n\nTable 9.9: Summary statistics created by the reinsurance Portfolio object.\n\n\n\n\n\n\n\n\n\nunit\nX\nE[X]\nEst E[X]\nCV(X)\nEst CV(X)\nSkew(X)\nEst Skew(X)\n\n\n\n\nNet\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n43.1\n43.100\n0.317\n0.317\n0.369\n0.369\n\n\n\nAgg\n43.1\n43.100\n0.317\n0.317\n0.369\n0.369\n\n\nCeded\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n3.5\n3.500\n3\n3.000\n2.667\n2.667\n\n\n\nAgg\n3.5\n3.500\n3\n3.000\n2.667\n2.667\n\n\ntotal\nFreq\n2\n\n0\n\n\n\n\n\n\nSev\n23.3\n23.300\n0.998\n\n0.340\n\n\n\n\nAgg\n46.6\n46.600\n0.370\n0.370\n0.788\n0.788\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.10: Pricing by distortion for ceded, net, and total (gross).\n\n\n\n\n\n\n\n\n\n\n(a) CCoC\n\n\n\n\n\nstatistic\nCeded\nNet\nTotal\n\n\n\n\nL\n3.500\n43.100\n46.600\n\n\na\n30.013\n69.987\n100.0\n\n\nQ\n22.405\n24.030\n46.435\n\n\nP\n7.609\n45.957\n53.565\n\n\nM\n4.109\n2.857\n6.965\n\n\nCOC\n0.183\n0.119\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PH\n\n\n\n\n\nstatistic\nCeded\nNet\nTotal\n\n\n\n\nL\n3.500\n43.100\n46.600\n\n\na\n23.383\n76.617\n100.000\n\n\nQ\n16.721\n29.714\n46.435\n\n\nP\n6.662\n46.903\n53.565\n\n\nM\n3.162\n3.803\n6.965\n\n\nCOC\n0.189\n0.128\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Wang\n\n\n\n\n\nstatistic\nCeded\nNet\nTotal\n\n\n\n\nL\n3.500\n43.100\n46.600\n\n\na\n20.237\n79.763\n100.000\n\n\nQ\n14.150\n32.284\n46.435\n\n\nP\n6.087\n47.478\n53.565\n\n\nM\n2.587\n4.378\n6.965\n\n\nCOC\n0.183\n0.136\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Dual\n\n\n\n\n\nstatistic\nCeded\nNet\nTotal\n\n\n\n\nL\n3.500\n43.100\n46.600\n\n\na\n18.539\n81.461\n100.0\n\n\nQ\n13.125\n33.310\n46.435\n\n\nP\n5.415\n48.151\n53.565\n\n\nM\n1.915\n5.051\n6.965\n\n\nCOC\n0.146\n0.152\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) TVaR\n\n\n\n\n\nstatistic\nCeded\nNet\nTotal\n\n\n\n\nL\n3.500\n43.100\n46.600\n\n\na\n17.679\n82.321\n100.000\n\n\nQ\n12.876\n33.559\n46.435\n\n\nP\n4.803\n48.762\n53.565\n\n\nM\n1.303\n5.662\n6.965\n\n\nCOC\n0.101\n0.169\n0.150",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Calculations with `Aggregate`</span>"
    ]
  },
  {
    "objectID": "080_Calculations.html#sec-realistic",
    "href": "080_Calculations.html#sec-realistic",
    "title": "9  Calculations with Aggregate",
    "section": "9.5 A more realistic example",
    "text": "9.5 A more realistic example\nIn this section, we create a series of exhibits analogous to those in Section 9.4 for an example with more realistic assumptions. It is included to show how aggregate can be used to solve real-world problems, hopefully motivating you to explore it further. The analysis steps are:\n\nCreate realistic by-unit frequency and severity distributions using Fast Fourier Transforms with independent units.\nSample the unit distributions with correlation induced by Iman-Conover, Section 8.2.\nBuild a Portfolio from the correlated sample.\nCalibrate distortions and compute unit pricing for each distortion.\nApply by-unit, per-occurrence reinsurance and examine pricing impact.\n\nThe aggregate DecL programming language makes it easy to specify frequency-severity compound distributions. In the code chunk below, the four lines inside the build statement are the DecL program. The triple quotes are Python shorthand for entering a multiline string. The first program line, beginning agg Auto, creates a distribution with an expected loss of 5000 (think: losses in 000s), severity from the 5000 xs 0 layer of a lognormal variable with a mean of 50 and a CV of 2, and gamma mixed-Poisson frequency with a mixing parameter (CV) of 0.2. The other two lines are analogous. The build statement runs the DecL program and creates a Portfolio object with relevant compound distributions by unit. It also sums the unit distributions as though they were independent—we will introduce some correlation later.\n\n\nCode\nfrom aggregate import build\nport = build(\"\"\"\nport Units\n    agg Auto 5000 loss  5000 xs 0 sev lognorm 50 cv  2 mixed gamma 0.2\n    agg GL   3000 loss  2500 xs 0 sev lognorm 75 cv  3 mixed gamma 0.3\n    agg WC   7000 loss 25000 xs 0 sev lognorm  5 cv 10 mixed gamma 0.25\n\"\"\")\n\n\nTable 9.11 shows the by-unit frequency and severity statistics and the total statistics assuming the units are independent. The deviation between the Fast Fourier Transform-generated compound distributions and the requested specifications are negligible. Figure 9.2 plots the unit and total densities on a nominal and log scale. The effect of WC driving the tail, via thicker severity and higher occurrence limit, is clear on the log plot. \n\n\n\n\nTable 9.11: Unit frequency, severity and compound assumptions, and portfolio total, showing requested and model achieved and key statistics.\n\n\n\n\n\n\n\n\n\nunit\nX\nE[X]\nEst E[X]\nCV(X)\nEst CV(X)\nSkew(X)\nEst Skew(X)\n\n\n\n\nAuto\nFreq\n100.0\n0\n0.224\n0\n0.402\n0\n\n\n\nSev\n49.982\n49.982\n1.973\n1.973\n10.678\n10.678\n\n\n\nAgg\n5,000\n5000.0\n0.298\n0.298\n0.699\n0.699\n\n\nGL\nFreq\n41.008\n0\n0.338\n0\n0.604\n0\n\n\n\nSev\n73.156\n73.156\n2.377\n2.378\n7.385\n7.385\n\n\n\nAgg\n3,000\n3000.0\n0.502\n0.502\n1.024\n1.024\n\n\nWC\nFreq\n1401.1\n0\n0.251\n0\n0.500\n0\n\n\n\nSev\n4.996\n4.945\n9.103\n9.198\n147.9\n147.9\n\n\n\nAgg\n7,000\n7000.0\n0.350\n0.350\n1.776\n1.775\n\n\ntotal\nFreq\n1542.1\n0\n0.229\n0\n0.496\n0\n\n\n\nSev\n9.727\n9.681\n6.123\n0\n68.886\n0\n\n\n\nAgg\n15,000\n15000.0\n0.216\n0.216\n0.939\n0.938\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: By-unit loss densities (left) and logdensities (right).\n\n\n\n\n\nNext, we sample from the unit distributions and shuffle using Iman-Conover to achieve the desired correlation shown in Table 9.12. This matrix comes from a separate analysis. The revised statistics (note higher total CV), quantiles, and achieved correlation are shown in Table 9.13 and Table 9.14. \n\n\n\n\nTable 9.12: Desired correlation matrix, as input to Iman-Conover.\n\n\n\n\n\n\n\n\n\nindex\nAuto\nGL\nWC\n\n\n\n\nAuto\n1\n0.5\n0.4\n\n\nGL\n0.5\n1\n0.1\n\n\nWC\n0.4\n0.1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.13: Key statistics from sample with Iman-Conover induced correlation.\n\n\n\n\n\n\n\n\n\nindex\nAuto\nGL\nWC\nTotal\n\n\n\n\ncount\n1,000\n1,000\n1,000\n1,000\n\n\nmean\n5071.0\n3020.2\n6970.5\n15061.7\n\n\nstd\n1508.4\n1533.0\n2354.5\n3985.2\n\n\nmin\n1,434\n269\n3,381\n6,707\n\n\n25%\n3,963\n1903.8\n5,311\n12193.2\n\n\n50%\n4894.5\n2,705\n6,427\n14617.5\n\n\n75%\n6000.2\n3826.2\n8012.8\n17382.2\n\n\nmax\n10,877\n10,321\n23,271\n39,588\n\n\nCV\n0.297\n0.508\n0.338\n0.265\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.14: Achived between-unit correlation.\n\n\n\n\n\n\n\n\n\nindex\nAuto\nGL\nWC\nTotal\n\n\n\n\nAuto\n1\n0.475\n0.390\n0.792\n\n\nGL\n0.482\n1\n0.103\n0.626\n\n\nWC\n0.386\n0.097\n1\n0.778\n\n\ntotal\n0.798\n0.631\n0.732\n1\n\n\n\n\n\n\n\n\n\nTable 9.15 shows the output of using the correlated sample to build a Portfolio object. This uses the samples directly, proxying the aggregate loss as a compound with degenerate frequency distribution identically equal to one and severity equal to the desired distribution. Hence the frequency rows show expectation one with zero CV.\n\n\n\n\nTable 9.15: Portfolio statistics reflecting the correlation achieved by the Iman-Conover sample.\n\n\n\n\n\n\n\n\n\nunit\nX\nE[X]\nEst E[X]\nCV(X)\nEst CV(X)\nSkew(X)\nEst Skew(X)\n\n\n\n\nAuto\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n5071.0\n5071.0\n0.297\n0.297\n0.540\n0.540\n\n\n\nAgg\n5071.0\n5071.0\n0.297\n0.297\n0.540\n0.540\n\n\nGL\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n3020.2\n3020.2\n0.507\n0.507\n0.974\n0.974\n\n\n\nAgg\n3020.2\n3020.2\n0.507\n0.507\n0.974\n0.974\n\n\nWC\nFreq\n1\n\n0\n\n\n\n\n\n\nSev\n6970.5\n6970.5\n0.338\n0.338\n1.620\n1.620\n\n\n\nAgg\n6970.5\n6970.5\n0.338\n0.338\n1.620\n1.620\n\n\ntotal\nFreq\n3\n\n0\n\n\n\n\n\n\nSev\n5020.6\n5020.6\n0.487\n\n1.013\n\n\n\n\nAgg\n15061.7\n15061.7\n0.212\n0.212\n0.818\n0.818\n\n\n\n\n\n\n\n\n\nTable 9.16 shows the parameters for distortions calibrated to achieve an overall 15% return at a 99.5% capital standard. Figure 9.3 plots the distortions and probability adjustment functions, compare this with Figure 9.1. These distortions are slightly more risk averse (higher parameters except PH, driving higher indicated prices).\n\n\n\n\nTable 9.16: Distortion parameters for to target total return at the regulatory capital standard.\n\n\n\n\n\n\n\n\n\nmethod\nP\nparam\nerror\n\n\n\n\nccoc\n16721.7\n0.150\n0\n\n\nph\n16721.7\n0.662\n-0.000\n\n\nwang\n16721.7\n0.420\n0.000\n\n\ndual\n16721.7\n1.706\n-0.000\n\n\ntvar\n16721.7\n0.282\n0.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3: Distortion functions and distortion probability adjustment functions.\n\n\n\n\n\nTable 9.17 compares the indicated pricing by unit and distortion. It is computed using exactly the same method described for the simple discrete example. The calculations are easy enough that they can be performed in a spreadsheet, but that is not recommended unless you are very patient. Table 9.18 shows just the implied loss ratios. The total loss ratios are all the same because the distortions are all calibrated to the same overall total premium. Since WC has the heaviest tail, it gets the lowest target loss ratio under CCoC, which is the most tail-risk averse. In fact, CCoC is so tail-risk averse that GL (which offers some diversification because of the correlation structure) gets a target over 100%. This is typical CCoC behavior—cat is really expensive; everything else is cheaper. The reason is clear when you remember CCoC pricing is just a weighted average of the mean and the max. The other distortions are more reasonable. TVaR and dual are quite similar and focus more on volatility risk. Since all three lines are similarly volatile, it is no surprise their target loss ratios are also similar under this view.\nThis analysis—and indeed the whole monograph—has ignored several important elements: expenses, taxes, and the time value of money. Expenses are relatively easy to incorporate: acquisition expenses are typically amortized over the policy term, administration expenses span the policy term and claim payout period, and claim expenses can be bundled with losses. Taxes, however, are a Byzantine labyrinth requiring careful modeling at the level of legal entity and country. Their treatment is up to each insurer, and it’s hard to offer even vague general guidance. Finally, time value of money brings real conceptual and computational difficulty (as opposed to the man-made complexity of taxes). It is the quantum gravity of insurance pricing. For some early thoughts, see the forthcoming monograph Pricing Multi-Period Insurance Risk by your intrepid second author. \n\n\n\nTable 9.17: Pricing by distortion.\n\n\n\n\n\n\n\n\n\n\n(a) CCoC\n\n\n\n\n\nstatistic\nAuto\nGL\nWC\nTotal\n\n\n\n\nL\n5065.4\n3016.4\n6957.8\n15039.6\n\n\na\n6968.9\n5138.5\n15828.6\n27936.0\n\n\nQ\n1722.6\n1855.2\n7636.4\n11214.3\n\n\nP\n5246.3\n3283.3\n8192.2\n16721.7\n\n\nM\n180.8\n266.9\n1234.4\n1682.1\n\n\nCOC\n0.105\n0.144\n0.162\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) PH\n\n\n\n\n\nstatistic\nAuto\nGL\nWC\nTotal\n\n\n\n\nL\n5065.4\n3016.4\n6957.8\n15039.6\n\n\na\n8655.6\n5610.8\n13669.6\n27936.0\n\n\nQ\n3141.2\n2225.1\n5848.0\n11214.3\n\n\nP\n5514.4\n3385.7\n7821.6\n16721.7\n\n\nM\n449.0\n369.3\n863.8\n1682.1\n\n\nCOC\n0.143\n0.166\n0.148\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Wang\n\n\n\n\n\nstatistic\nAuto\nGL\nWC\nTotal\n\n\n\n\nL\n5065.4\n3016.4\n6957.8\n15039.6\n\n\na\n8887.7\n5637.7\n13410.6\n27936.0\n\n\nQ\n3323.9\n2218.2\n5672.2\n11214.3\n\n\nP\n5563.9\n3419.5\n7738.4\n16721.7\n\n\nM\n498.4\n403.1\n780.6\n1682.1\n\n\nCOC\n0.150\n0.182\n0.138\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Dual\n\n\n\n\n\nstatistic\nAuto\nGL\nWC\nTotal\n\n\n\n\nL\n5065.4\n3016.4\n6957.8\n15039.6\n\n\na\n8987.6\n5679.8\n13268.6\n27936.0\n\n\nQ\n3389.0\n2235.0\n5590.3\n11214.3\n\n\nP\n5598.5\n3444.8\n7678.3\n16721.7\n\n\nM\n533.1\n428.5\n720.6\n1682.1\n\n\nCOC\n0.157\n0.192\n0.129\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) TVaR\n\n\n\n\n\nstatistic\nAuto\nGL\nWC\nTotal\n\n\n\n\nL\n5065.4\n3016.4\n6957.8\n15039.6\n\n\na\n9017.0\n5680.6\n13238.3\n27936.0\n\n\nQ\n3397.0\n2232.8\n5584.5\n11214.3\n\n\nP\n5620.0\n3447.9\n7653.9\n16721.7\n\n\nM\n554.6\n431.5\n696.1\n1682.1\n\n\nCOC\n0.163\n0.193\n0.125\n0.150\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9.18: Implied loss ratio by unit by pricing by distortion (extract from previous tables).\n\n\n\n\n\n\n\n\n\nMethod\nAuto\nGL\nWC\nTotal\n\n\n\n\nDist ccoc\n96.6%\n91.9%\n84.9%\n0.899\n\n\nDist ph\n91.9%\n89.1%\n89.0%\n0.899\n\n\nDist wang\n91.0%\n88.2%\n89.9%\n0.899\n\n\nDist dual\n90.5%\n87.6%\n90.6%\n0.899\n\n\nDist tvar\n90.1%\n87.5%\n90.9%\n0.899\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMildenhall, Stephen J. 2024. “Aggregate: fast, accurate, and flexible approximation of compound probability distributions.” Annals of Actuarial Science, 1–40. https://doi.org/10.1017/S1748499524000216.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance Risk: Theory and Practice. John Wiley & Sons, Inc.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Calculations with `Aggregate`</span>"
    ]
  },
  {
    "objectID": "090_EndMatter.html",
    "href": "090_EndMatter.html",
    "title": "10  Appendices",
    "section": "",
    "text": "10.1 Table of symbols",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "090_EndMatter.html#table-of-symbols",
    "href": "090_EndMatter.html#table-of-symbols",
    "title": "10  Appendices",
    "section": "",
    "text": "Table 10.1: Symbols used in this monograph.\n\n\n\n\n\n\n\n\n\n\nSymbol\nInterpretation\nReference\n\n\n\n\n\\(\\alpha_k^i\\)\nunit \\(i\\) share of layer \\(k\\) expected loss\nSection 8.3\n\n\n\\(\\beta_k^i\\)\nunit \\(i\\) share of layer \\(k\\) premium\nSection 8.3\n\n\n\\(\\delta\\)\nrate of discount, \\(\\delta=\\iota/(1+\\iota)\\)\nSection 4.1\n\n\n\\(\\Delta X_k\\)\nlayer \\(k\\) limit, \\(\\Delta X_k=X_{k+1}-X_k\\)\nSection 4.4\n\n\n\\(\\epsilon\\)\nscaling parameter\nSection 3.2, Section 5.5\n\n\n\\(\\epsilon_-\\)\nminimum scale\nSection 5.5.2\n\n\n\\(\\epsilon_+\\)\nmaximum scale\nSection 5.5.2\n\n\n\\(\\iota\\)\nexpected return, cost of capital, \\(\\iota = M/Q\\)\nSection 4.1\n\n\n\\(\\iota^i\\)\nunit \\(i\\) cost of capital\nSection 8.3\n\n\n\\(\\iota_k\\)\nlayer \\(k\\) cost of capital\nSection 8.3\n\n\n\\(\\kappa^i(x)\\)\nunit \\(i\\) conditional expected loss, \\(\\kappa^i(x) := \\mathsf E[X^i|X=x]\\)\nSection 4.3\n\n\n\\(\\nabla^i\\phi(X)\\)\npartial derivative with respect to scaling\nSection 3.2\n\n\n\\(\\nu\\)\ndiscount factor, \\(\\nu=1/(1+\\iota)\\)\nSection 4.1\n\n\n\\(\\Phi(z)\\)\nstandard Gaussian cumulative distribution function\nSection 4.6\n\n\n\\(\\rho()\\)\npricing risk measure\nSection 1.6\n\n\n\\(\\theta\\)\nweight parameter for bi-TVaR\nSection 6.1\n\n\n\\(a\\)\nassets\nSection 1.6\n\n\n\\(a()\\)\ncapital risk measure\nSection 1.6\n\n\n\\(a^i\\)\nallocation of assets to unit \\(i\\) (industry standard)\nSection 4.7\n\n\n\\(g(s)\\)\ndistortion function\nSection 4.4\n\n\n\\(i\\)\nindex to portfolio unit\nSection 2.2\n\n\n\\(j\\)\nevent index, \\(j=1, \\dots, n\\)\nSection 2.2\n\n\n\\(k\\)\nlayer index, \\(k=0,\\dots,n-1\\)\nSection 4.4\n\n\n\\(L\\)\nexpected loss\nSection 4.1\n\n\n\\(L^i\\)\nunit \\(i\\) expected loss\nSection 4.7, Section 4.5\n\n\n\\(M\\)\nmargin, \\(P=L+M\\)\nEquation 4.1, Section 4.1\n\n\n\\(M^i\\)\nunit \\(i\\) margin \\(M^i = P^i - L^i\\)\nSection 4.5\n\n\n\\(m\\)\nnumber of units\nSection 2.2\n\n\n\\(n\\)\nnumber of events\nSection 2.2\n\n\n\\(p_j\\)\nevent probability \\(p_j=\\Pr(X=X_j)\\)\nSection 2.2\n\n\n\\(P\\)\npremium \\(P=\\rho(X)\\)\nSection 1.6\n\n\n\\(P_k\\)\nlayer \\(k\\) premium, \\(P_k = g(S_k)\\Delta X_k\\)\nSection 4.4\n\n\n\\(P_P\\)\nplan premium\nSection 5.5.2\n\n\n\\(P_R\\)\nrequired premium\nSection 5.5.2\n\n\n\\(P^i\\)\nunit \\(i\\) premium\nSection 4.7, Section 4.5\n\n\n\\(q_k\\)\ndistorted probability, \\(q_k = g(S_{k-1}) - g(S_{k})\\)\nSection 4.4\n\n\n\\(Q\\)\ncapital \\(Q=a-P\\)\nSection 1.6\n\n\n\\(Q^i\\)\nunit \\(i\\) capital\nSection 8.3\n\n\n\\(Q_k^i\\)\nunit \\(i\\) capital in layer \\(k\\)\nSection 8.3\n\n\n\\(Q_P\\)\nplan capital, \\(Q_P=a-P_P\\)\nSection 5.5.2\n\n\n\\(r\\)\nEVA/capital ratio, \\(r=V/Q_P\\)\nSection 5.5.2\n\n\n\\(S_k\\)\nlayer \\(k\\) attachment probability, \\(S_k=Pr(X &gt; X_k)\\)\nSection 4.4\n\n\n\\(\\mathsf{TVaR}_{0.99}\\)\nInsCo example capital standard\nSection 3.2\n\n\n\\(V\\)\nEVA, economic value added, \\(V=P_P-P_R\\)\nSection 5.5.2\n\n\n\\(\\mathsf{VaR}_{0.85}\\)\nexample alternative risk measure\nSection 3.2\n\n\n\\(X\\)\nportfolio liability random variable\nSection 1.6\n\n\n\\(X_0\\)\nphantom portfolio loss \\(X_0=0\\) with probability \\(p_0=0\\)\nSection 4.3\n\n\n\\(X_j^i\\)\nloss to unit \\(i\\) in event \\(j\\)\nSection 2.2\n\n\n\\(X_j\\)\nportfolio loss in event \\(j\\)\nSection 2.2\n\n\n\\(X_k\\)\nlayer payout threshold (attachment)\nSection 4.4\n\n\n\\(X\\wedge a\\)\nportfolio payout to policyholders\nSection 1.6\n\n\n\\(X^i(a)\\)\nunit \\(i\\) capped losses\nSection 4.3\n\n\n\\(X^i\\)\nloss to unit \\(i\\) (random variable)\nSection 2.2\n\n\n\\(Z_k\\)\nevent weight, likelihood ratio, \\(Z_k=q_k/p_k\\)\nSection 4.4",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "090_EndMatter.html#sec-ASOPs",
    "href": "090_EndMatter.html#sec-ASOPs",
    "title": "10  Appendices",
    "section": "10.2 Actuarial Standards of Practice",
    "text": "10.2 Actuarial Standards of Practice\n“The Actuarial Standards Board (ASB) sets standards for appropriate actuarial practice in the United States through the development and promulgation of Actuarial Standards of Practice (ASOPs). These ASOPs describe the procedures an actuary should follow when performing actuarial services and identify what the actuary should disclose when communicating the results of those services.” ASOPs can be found at https://www.actuarialstandardsboard.org/standards-of-practice/\n\n\n\nTable 10.2: ASOPs that are relevant to capital modeling.\n\n\n\n\n\n\n\n\n\n\nNumber\nTitle\nReference\n\n\n\n\n7\nAnalysis of Life, Health, or Property/Casualty Insurer Cash Flows\nChapter 2\n\n\n23\nData Quality\nChapter 2\n\n\n30\nTreatment of Profit and Contingency Provisions and the Cost of Capital in Property/Casualty Insurance Ratemaking\nChapter 4\n\n\n46\nRisk Evaluation in Enterprise Risk Management\nChapter 2, Chapter 3\n\n\n47\nRisk Treatment in Enterprise Risk Management\nSection 1.3, Section 3.2\n\n\n55\nCapital Adequacy Assessment\nChapter 3\n\n\n56\nModeling\nChapter 2\n\n\n58\nEnterprise Risk Management\nChapter 2, Chapter 3\n\n\n\n\n\n\nThe ASB repealed ASOP Nos. 46 and 47 in December 2024 and replaced them with ASOP No. 58, Enterprise Risk Management, to reflect the developments since 2012, to better reflect today’s ERM practices and terminology, and to align with ASOP No. 55.\n\n\n\nTable 10.3: ASOPs that are potentially relevant to capital modeling.\n\n\n\n\n\n\n\n\n\nNumber\nTitle\n\n\n\n\n12\nRisk Classification (for All Practice Areas)\n\n\n13\nTrending Procedures in Property/Casualty Insurance\n\n\n19\nAppraisals of Casualty, Health, and Life Insurance Businesses\n\n\n20\nDiscounting of Property/Casualty Claim Estimates\n\n\n25\nCredibility Procedures\n\n\n29\nExpense Provisions for Prospective Property/Casualty Risk Transfer and Risk Retention\n\n\n36\nStatements of Actuarial Opinion Regarding Property/Casualty Loss, Loss Adjustment Expense, or Other Reserves\n\n\n38\nCatastrophe Modeling (for All Practice Areas)\n\n\n39\nTreatment of Catastrophe Losses in Property/Casualty Insurance Ratemaking\n\n\n41\nActuarial Communications\n\n\n43\nProperty/Casualty Unpaid Claim Estimates\n\n\n53\nEstimating Future Costs for Prospective Property/Casualty Risk Transfer and Risk Retention",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "090_EndMatter.html#glossary",
    "href": "090_EndMatter.html#glossary",
    "title": "10  Appendices",
    "section": "10.3 Glossary",
    "text": "10.3 Glossary\n\n\n\n\n\n\n\n\nTerm\nDescription\nReference\n\n\n\n\nAccounting\nmodule that produces financial statements\nSection 1.5\n\n\nAllocation\nmodule that distributes portfolio premium to units\nSection 1.5\n\n\nBCAR\nBest’s Capital Adequacy Ratio\nSection 1.5\n\n\nBi-TVaR\nconvex combination of TVaRs\nChapter 6\n\n\nBusiness Operations\nmodule that deals with loss experience\nSection 1.5\n\n\nCapital\nowner-provided funds\nSection 1.1\n\n\nCapital Adequacy\nmodule dealing with risk and capital sufficiency\nSection 1.5\n\n\nCapital risk measure\nrule relating liabilities to required assets\nSection 1.6\n\n\nCapital structure\nmix of liabilities funding total assets\nSection 1.5, Section 2.5\n\n\nCharter\nformal document setting out the rationale\nSection 1.2\n\n\nCo-TVaR\nconditional TVaR, Natural Allocation of TVaR\nSection 3.2\n\n\nco-XTVaR\nConditional XTVaR\nSection 4.7\n\n\nComonotonic\nRandom variables \\(X\\) and \\(Y\\) are comonotonic if they are nondecreasing functions of a third r.v.\nChapter 4\n\n\nComonotonic additive\nProperty of a risk measure \\(\\rho\\): If \\(X\\) and \\(Y\\) are comonotonic, then \\(\\rho(X+Y)=\\rho(X)+\\rho(Y)\\)\nChapter 4\n\n\nConstant cost of capital (CCoC)\nassumption that all layers require the same return\nSection 4.4\n\n\nDistortion function\ntranslates attachment probability to rate on line\nSection 4.4\n\n\nDistorted expectation\nexpectation calculated with distorted probabilities \\(q\\)\nSection 4.5, Section 4.8\n\n\nDistorted expected loss share\na unit’s share of a layer’s distorted expected loss, \\(\\beta_k^i\\)\nSection 8.3\n\n\nDistorted probability\nafter applying a distortion function, \\(q\\) replaces \\(p\\)\nSection 4.4\n\n\nEconomic Scenario Generator\nmodule to generate socioeconomic outcomes\nSection 1.5, Section 2.4\n\n\nEconomic value added (EVA)\nProfits beyond what is necessary for investors\nSection 4.1\n\n\nEqual Priority\nRule for distributing funds among claimants\nSection 4.3\n\n\nExpected loss share\na unit’s share of a layer expected loss, \\(\\alpha_k^i\\)\nSection 8.3\n\n\nFunding equation\nstates that assets are the sum of premiums and investor capital, \\(a=P+Q\\)\nEquation 1.1, Section 1.6\n\n\nGradient\nderivative with respect to several variables\nSection 3.2\n\n\nInsCo\nhypothetical insurer used as a simple example\nSection 1.6\n\n\nInsureds\ncustomers of InsCo, who pay premiums\nSection 1.6\n\n\nInvestors\nowners of InsCo, who supply capital\nSection 1.6\n\n\nLaw invariance\nrisk measure depends only on distribution\nSection 4.4\n\n\nLayer\nsegment of assets between two levels of portfolio loss\nSection 4.4\n\n\nLayer funding equation\nstates that layer assets are the sum of premiums and investor capital, \\(\\Delta X_k=P_k+Q_k\\)\nEquation 4.4, Section 4.4\n\n\nLikelihood ratio\nratio of distorted to original probabilities\nSection 4.8\n\n\nMargin\ndifference between premium and expected loss, \\(M=P-L\\)\nEquation 4.1, Section 4.1\n\n\nMarginal approach\nexamine the effect of a small change; look at first derivative\nSection 3.2\n\n\nModel\nsimplified representation of reality, usually mathematical\nSection 1.1, Section 1.5\n\n\nNatural Allocation (NA)\ndecomposition of portfolio premium into distorted expectation of unit losses\nSection 4.5\n\n\nNatural Allocation of capital\nallocation of capital consistent with allocation of premium, \\(Q_k^i\\)\nSection 8.3\n\n\nNormal copula\nmultivariate uniform distribution based on the normal\nSection 8.2.5\n\n\nObjectivity\nsynonym for law invariance\nSection 4.4\n\n\nParameter uncertainty\nlack of knowledge about parameters of a process, sometimes modeled as a distribution\nSection 8.1\n\n\nPricing\nmodule assigns technical premium to the portfolio \nSection 1.5\n\n\nPricing & Allocation\nmodule that assigns technical premiums to units\nSection 1.5\n\n\nPricing risk measure\nrule that relates liabilities to technical premiums\nSection 1.6\n\n\nPro forma\nhypothetical financial statement\nSection 1.5 , Section 2.7\n\n\nProxy model\nsimpler model of another model\nSection 5.5\n\n\nProbabilistic database\nstored sample of random outcomes\nSection 2.2\n\n\nProcess risk\nrandomness in outcomes from a well defined and parameterized stochastic process\nSection 8.1\n\n\nReinsurance\nmodule that deals with risk transfer\nSection 1.5\n\n\nRisk appetite\ncompensation/price limit for taking on risk\nSection 1.5\n\n\nRisk tolerance\nlimit of risk the firm is willing to take on\nSection 1.5\n\n\nRVaR\nRange Value at Risk, window version of VaR\nSection 3.2\n\n\nQuantity of Interest\ntarget metric, statistic, etc., of a simulation exercise\nSection 8.1\n\n\nSpectral risk measure (SRM)\nrisk measure determined by a distortion function\nSection 4.4\n\n\nState price\nA contract that pays $1 in a particular state of the world\nSection 4.8\n\n\nTranche\nsynonym for layer\nSection 4.4\n\n\nUnit\nportion of portfolio, e.g., line of business\nSection 1.5\n\n\nValue at Risk (VaR)\nrisk measure, quantile of a distribution\nSection 1.5\n\n\nTail Value at Risk (TVaR)\naverage of losses in a specified tail probability; also CVaR, TCE\nChapter 3\n\n\nExcess TVaR (XTVaR)\nTVaR minus expected losses\nSection 4.7",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Aas, Kjersti, Claudia Czado, Arnoldo Frigessi, and Henrik Bakken. 2009.\n“Pair-copula constructions of multiple\ndependence.” Insurance: Mathematics and Economics\n44 (2): 182–98. https://doi.org/10.1016/j.insmatheco.2007.02.001.\n\n\nActuarial Standards Board. 2007. “Property/Casualty Unpaid\nClaim Estimates.” Actuarial Standard of Practice No.\n43.\n\n\nActuarial Standards Board. 2011. “Treatment\nof Profit and Contingency Provisions and the Cost of Capital in Property\n/ Casualty Insurance Ratemaking.” Actuarial Standard\nof Practice No. 30.\n\n\nActuarial Standards Board. 2019. “Modeling.”\nActuarial Standard of Practice No. 56, no. 56.\n\n\nArtzner, Philippe, Freddy Delbaen, Jean-Marc Eber, and David Heath.\n1999. “Coherent measures of\nrisk.” Mathematical Finance 9 (3): 203–28. http://onlinelibrary.wiley.com/doi/10.1111/1467-9965.00068/abstract.\n\n\nBlanchard III, Ralph S. 2008. Basic Insurance Accounting –\nSelected Topics. July.\n\n\nBodoff, Neil M. 2007. “Capital Allocation by\nPercentile Layer.” Variance 3 (1): 13–30.\n\n\nBolstad, William M, and James M Curran. 2016. Introduction to\nBayesian Statistics. John Wiley & Sons.\n\n\nBrehm, Paul J, Spencer M Gluck, Kreps Rodney E, et al. 2007.\n“Enterprise Risk Analysis for Property & Liability Insurance\nCompanies.” Guy Carpenter & Company, LLC.\n\n\nBühlmann, Hans. 1985. “Premium Calculation\nfrom Top Down.” ASTIN Bulletin 15 (2): 89–101. https://doi.org/10.2143/ast.15.2.2015021.\n\n\nCaramagno, Nicholas, David Mamane, and Liam Neilson. 2021. An Introduction to IFRS 17 for P&C\nActuaries. https://www.ifrs.org/issued-standards/list-of-standards/ifrs-17-insurance-contracts/.\n\n\nCasualty Actuarial Society. 2022. Complete Text\nReferences for Exam 7, Syllabus of Basic Education. https://www.casact.org/sites/default/files/2021-03/7_individual_textref.pdf.\n\n\nChan, Cynthia et al. 2024. Insurance Rating Criteria. Fitch\nRatings.\n\n\nCherny, Alexander, and Dilip Madan. 2009. “New measures for performance evaluation.”\nReview of Financial Studies 22 (7): 2571–606. https://doi.org/10.1093/rfs/hhn081.\n\n\nConning. 2020. “A User’s Guide to Economic Scenario Generation in\nProperty/Casualty Insurance.” CAS Research Papers. https://www.casact.org/sites/default/files/2021-02/economic-scenario-generation-conning1020.pdf.\n\n\nCont, Rama, Romain Deguest, and Giacomo Scandolo. 2010. “Robustness and sensitivity analysis of risk measurement\nprocedures.” Quantitative Finance 10 (6):\n593–606. https://doi.org/10.1080/14697681003685597.\n\n\nCummins, J. David. 1990. “Multi-Period\nDiscounted Cash Flow Rate-making Models in Property-Liability\nInsurance.” Journal of Risk and Insurance 57 (1):\n79–109.\n\n\nCummins, J. David. 2000. “Allocation of\ncapital in the insurance industry.” Risk Management\nand Insurance Review, nos. October 1998: 7–28. http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6296.2000.tb00013.x/full.\n\n\nDaykin, Chris D, Teivo Pentikainen, and Martti Pesonen. 1993.\nPractical Risk Theory for Actuaries. Chapman; Hall/CRC.\n\n\nDelbaen, Freddy. 2000. “Coherent Risk\nMeasusres.” Blätter Der DGVFM 10\n(24(4)): 733–39. https://doi.org/10.1007/BF02808276.\n\n\nEIOPA. 2009. Directive 138/2009/EC (Solvency II\nDirective).\n\n\nEIOPA. 2015. Delegated Regulation (EU) 2015/35.\n\n\nEling, Martin, and David Holzmüller. 2008. “An Overview and Comparison of Risk-Based Capital\nStandards.” Journal of Insurance Regulation 26\n(4): 31–60.\n\n\nEmbrechts, Paul. 2010. “Risk Aggregation\npresentation.” Copula Theory and Its\nApplications, 111–26. https://doi.org/10.1007/978-3-642-12465-5.\n\n\nEmma, Charles C et al. 2000. Dynamic Financial Models of\nProperty-Casualty Insurers. Casualty Actuarial Society. https://www.casact.org/sites/default/files/database/forum_00wforum_00wf317.pdf.\n\n\nFairley, William B. 1979. “Investment Income\nand Profit Margins in Property-Liability Insurance: Theory and Empirical\nResults.” The Bell Journal of Economics 10 (1):\n192. https://doi.org/10.2307/3003326.\n\n\nFloreani, Alberto. 2011. “Risk margin\nestimation through the cost of capital approach: Some conceptual\nissues.” Geneva Papers on Risk and Insurance: Issues\nand Practice 36 (2): 226–53. https://doi.org/10.1057/gpp.2011.2.\n\n\nFriedland, Jacqueline. 2010. “Estimating Unpaid Claims Using Basic\nTechniques.” https://www.casact.org/sites/default/files/database/studynotes_friedland_estimating.pdf.\n\n\nFriedman, J, T Hastie, and R Tibshirani. 2008. The elements of statistical learning.\nSpringer.\n\n\nGabaix, Xavier, and David I Laibson. 2008. The Seven Properties of\nGood Models.\n\n\nGeisser, Seymour. 2017. Predictive Inference. Chapman;\nHall/CRC.\n\n\nGolub, Gene H, and Charles F Van Loan. 2013. Matrix\nComputations. Johns Hopkins University Press.\n\n\nHansen, Lars Peter, and Thomas J Sargent. 2008.\nRobustness. https://doi.org/10.1007/978-1-4471-5562-1_7.\n\n\nHeynderickx, W., J. Cariboni, W. Schoutens, and B. Smits. 2016.\n“The relationship between risk-neutral and\nactual default probabilities: the credit risk premium.”\nApplied Economics 48 (42): 4066–81. https://doi.org/10.1080/00036846.2016.1150953.\n\n\nIAA Risk Margin Working Group. 2009. Measurement of Liabilities for Insurance Contracts:\nCurrent Estimates and Risk Margins. International Actuarial\nAssocation.\n\n\nIAIS. 2008. Standard on the Use of Internal\nModels for Regulatory Capital Purposes. no. July.\n\n\nJakobsen, Mathilde, Timothy Prince, Brad Herman, and Mahesh Mistry.\n2020. Understanding Universal BCAR.\n\n\nJewson, Stephen, Trevor Sweeting, and Lynne Jewson. 2025. “Reducing reliability bias in assessments of extreme\nweather risk using calibrating priors.” Advances in\nStatistical Climatology, Meteorology and Oceanography 11 (1): 1–22.\nhttps://doi.org/10.5194/ascmo-11-1-2025.\n\n\nJouini, E., W. Schachermayer, and N. Touzi. 2008. “Optimal risk sharing for law invariant monetary utility\nfunctions.” Mathematical Finance 18 (2): 269–92.\nhttps://doi.org/10.1111/j.1467-9965.2007.00332.x.\n\n\nKarakuyu, Ali, Mark Button, et al. 2023. Criteria | Insurance |\nGeneral: Insurer Risk-Based Capital Adequacy–Methodology and\nAssumptions.\n\n\nKlugman, Stuart A, Harry H Panjer, and Gordon E Willmot. 2019. Loss\nModels: From Data to Decisions. Vol. 715. John Wiley & Sons.\n\n\nKoca, Stephen et al. 2023. Statements of Actuarial Opinion on\nProperty and Casualty Loss Reserves. American Academy of Actuaries.\n\n\nKoenker, Roger, and Kevin F Hallock. 2001. “Quantile\nRegression.” Journal of Economic Perspectives 15 (4):\n143–56.\n\n\nKovvali, Narayan. 2022. Theory and Applications of Gaussian\nQuadrature Methods. Springer Nature.\n\n\nKozik, Thomas J. 1994. “Underwriting\nbetas-the shadows of ghosts.” Proceedings of the\nCasualty Actuarial Society 81 (154, 155): 303–29.\n\n\nKreps, Rodney E. 1997. Parameter Uncertainty in (Log)normal\nDistributions. https://www.casact.org/sites/default/files/database/proceed_proceed97_97553.pdf.\n\n\nMajor, John A. 1999. “Taking Uncertainty Into\nAccount: Bias Issues Arising from Parameter Uncertainty in Risk\nModels.” CAS Forum Summer (153-196).\n\n\nMajor, John A., Ruodu Wang, and Micah G. Woolstenhulme. 2015.\n“The Most Dangerous Model: A Natural\nBenchmark for Assessing Model Risk.” Society of\nActuaries Monograph: Enterprise Risk Management Symposium, 1–45.\n\n\nMcClenahan, Charles L. 1999. “Insurance\nProfitability.” Actuarial Considerations Regarding\nRisk and Return, 113–24.\n\n\nMcNeil, Alexander J., Paul Embrechts, and Rudiger Frey. 2005. Quantitative Risk Management: Concepts, Techniques, and\nTools. Princeton University Press. https://doi.org/10.1198/jasa.2006.s156.\n\n\nMeyers, Glenn G. 2019. Stochastic Loss Reserving Using\nBayesian MCMC Models. CAS Monograph Series.\n\n\nMildenhall, Stephen J. 2004. “A Note on the\nMyers and Read Capital Allocation Formula.” North\nAmerican Actuarial Journal 8 (2): 32–44. http://library.soa.org/library-pdf/naaj0402_3.pdf.\n\n\nMildenhall, Stephen J. 2017. “In Praise of\nValue at Risk.” Actuarial Review 44(6): 48–50. https://ar.casact.org/in-praise-of-value-at-risk/.\n\n\nMildenhall, Stephen J. 2023. “aggregate:\nFrequency-Severity Distributions Made Easy.” Actuarial\nReview 50(3): 31–34. https://ar.casact.org/aggregate-frequency-severity-distributions-made-easy/.\n\n\nMildenhall, Stephen J. 2024. “Aggregate:\nfast, accurate, and flexible approximation of compound probability\ndistributions.” Annals of Actuarial Science,\n1–40. https://doi.org/10.1017/S1748499524000216.\n\n\nMildenhall, Stephen J. n.d.-a. Aggregate Software\nDocumentation.\n\n\nMildenhall, Stephen J. n.d.-b. When is Premium\nRiskier Than Loss?\n\n\nMildenhall, Stephen J. 2005. “Correlation and\nAggregate Loss Distributions With An Emphasis On The Iman-Conover\nMethod.” Casualty Actuarial Society Forum Winter.\nhttps://www.casact.org/sites/default/files/database/forum_06wforum_06w107.pdf.\n\n\nMildenhall, Stephen J, and John A Major. 2022. Pricing Insurance\nRisk: Theory and Practice. John Wiley & Sons, Inc.\n\n\nMyers, Stewart C, and Richard A Cohn. 1987. “A discounted cash flow approach to property-liability\ninsurance rate regulation.” In Fair Rate of Return in\nProperty-Liability Insurance. Springer.\n\n\nNAIC. 2024. Risk-Based Capital.\n\n\nOdomirok, Kathleen C, Liam M Mcfarlane, Gareth L Kennedy, and Justin J\nBrenden. 2020. Casualty Actuarial Society\nFinancial Reporting through the Lens of a Property/Casualty\nActuary.\n\n\nPostek, Krzysztof, Alessandro Zocca, Joaquim Gromicho, and Jeffrey\nKantor. 2024a. “Companion Jupyter Book for “Hands-On Mathematical Optimization with\nPython’’.” GitHub.\n\n\nPostek, Krzysztof, Alessandro Zocca, Joaquim Gromicho, and Jeffrey\nKantor. 2024b. Hands-On Mathematical\nOptimization with Python. Cambridge University Press.\n\n\nPress, William H., Saul A. Teukolsky, William T. Vetterling, and Brian\nP. Flannery. 1992. Numerical Recipes in\nC. Cambridge University Press.\n\n\nReitano, Robert R. 1997. “Two paradigms for\nthe market value of liabilities.” North American\nActuarial Journal 1 (4): 104–22. https://doi.org/10.1080/10920277.1997.10595657.\n\n\nRobbin, Ira. 1992. The Underwriting Profit\nProvision.\n\n\nSmith, Ralph C. 2024. Uncertainty Quantification: Theory,\nImplementation, and Applications. SIAM.\n\n\nSociety of Actuaries. 2024. Economic Scenario Generators|SOA.\nhttps://www.soa.org/resources/tables-calcs-tools/research-scenario/.\n\n\nSzkoda, Susan T et al. 1995. CAS Dynamic Financial Analysis\nProperty/Casualty Insurance Companies Handbook. Casualty Actuarial\nSociety. https://www.casact.org/sites/default/files/database/forum_96wforum_96wf001.pdf.\n\n\nTasche, Dirk. 1999. “Risk contributions and\nperformance measurement.” Report of the Lehrstuhl Fur\nMathematische Statistik, TU Munchen, 1–26.\n\n\nVenter, Gary G. 1991. “Premium Calculation\nImplications of Reinsurance Without Arbitrage.” ASTIN\nBulletin 21 (02): 223–30. https://doi.org/10.2143/ast.21.2.2005365.\n\n\nVenter, Gary G., John A. Major, and Rodney E. Kreps. 2006. “Marginal Decomposition of Risk Measures.”\nASTIN Bulletin 36 (2): 375–413. https://doi.org/10.2143/AST.36.2.2017927.\n\n\nWand, Matt P, and M Chris Jones. 1994. Kernel Smoothing. CRC\npress.\n\n\nWang, Shaun S. 1996. “Premium Calculation by\nTransforming the Layer Premium Density.” ASTIN\nBulletin 26 (01): 71–92. https://doi.org/10.2143/AST.26.1.563234.\n\n\nWang, Shaun S. 1998. “Aggregation of\ncorrelated risk portfolios: models and algorithms.”\nProceedings of the Casualty Actuarial Society, 848–939.\n\n\nWang, Shaun S. 2000. “A Class of Distortion\nOperators for Pricing Financial and Insurance Risks.”\nThe Journal of Risk and Insurance 67 (1): 15–36. https://doi.org/10.2307/253675.\n\n\nWuthrich, Mario V., and Michael Merz. 2015. Stochastic Claims Reserving Manual: Advances in Dynamic\nModeling.",
    "crumbs": [
      "Bibliography"
    ]
  }
]